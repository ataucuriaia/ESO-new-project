{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ataucuriaia/ESO-new-project/blob/main/ESO_new_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4cjw3fHznrp",
        "outputId": "e26aa617-c9d1-43af-bf85-ae27205331f9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  9%|▊         | 42/492 [02:13<16:40,  2.22s/it]"
          ]
        }
      ],
      "source": [
        "# ================================\n",
        "# ESO Web Scraping + Enrichment Pipeline (v1.0)\n",
        "# Enterprise Studio — Complete Pipeline\n",
        "# \n",
        "# PHASES:\n",
        "#   0) Setup & Data Load + Health Check\n",
        "#   1) URL Normalization & Homepage Scraping\n",
        "#   2) Homepage Signals Extraction\n",
        "#   3) Org Categorization Module (Phase 1 - Objective A)\n",
        "#   4) Support Page Crawling + People Extraction (Phase 2 - Objective B)\n",
        "#   5) Exports + Diagnostics\n",
        "#\n",
        "# INPUTS: CSV with columns \"Org Name\" and \"Website URL\"\n",
        "# OUTPUTS: \n",
        "#   - Organization_Database_enriched_v1_0.csv (org-level with categorization)\n",
        "#   - People_Extracted_v1_0.csv (people-level with expertise tags)\n",
        "# ================================\n",
        "\n",
        "# --- 1) Install + imports ---\n",
        "# Handle pip install for both Colab and local environments\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "try:\n",
        "    import pandas as pd\n",
        "    import requests\n",
        "    from bs4 import BeautifulSoup\n",
        "    from tqdm import tqdm\n",
        "except ImportError:\n",
        "    print(\"Installing required packages...\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"-q\", \"install\", \n",
        "                          \"beautifulsoup4\", \"lxml\", \"tqdm\", \"requests\"])\n",
        "    import pandas as pd\n",
        "    import requests\n",
        "    from bs4 import BeautifulSoup\n",
        "    from tqdm import tqdm\n",
        "\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "import os\n",
        "from urllib.parse import urlparse, urljoin\n",
        "\n",
        "# --- 2) Load your existing database ---\n",
        "INPUT_PATH = \"Organization Database 1f24e34e337d8027b500d2a10b1ceaa7.csv\"\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(INPUT_PATH)\n",
        "except FileNotFoundError:\n",
        "    raise FileNotFoundError(\n",
        "        f\"CSV file not found: {INPUT_PATH}\\n\"\n",
        "        f\"Please ensure the file is in the current directory: {os.getcwd()}\"\n",
        "    )\n",
        "\n",
        "print(f\"Loaded CSV: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
        "print(f\"Columns: {list(df.columns)}\")\n",
        "\n",
        "# Basic checks (matches your structure)\n",
        "REQUIRED_COLS = [\"Org Name\", \"Website URL\"]\n",
        "missing = [c for c in REQUIRED_COLS if c not in df.columns]\n",
        "if missing:\n",
        "    raise ValueError(\n",
        "        f\"Missing required columns: {missing}\\n\"\n",
        "        f\"Found columns: {list(df.columns)}\\n\"\n",
        "        f\"Please ensure your CSV has 'Org Name' and 'Website URL' columns.\"\n",
        "    )\n",
        "\n",
        "print(\"✓ Required columns present\")\n",
        "\n",
        "# --- 3) Helpers: URL cleaning + safe request ---\n",
        "HEADERS = {\n",
        "    \"User-Agent\": (\n",
        "        \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \"\n",
        "        \"(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
        "    )\n",
        "}\n",
        "\n",
        "def normalize_url(url: str) -> str:\n",
        "    \"\"\"Normalize website URL. Adds scheme if missing, strips whitespace.\"\"\"\n",
        "    if not isinstance(url, str) or not url.strip():\n",
        "        return \"\"\n",
        "    u = url.strip()\n",
        "    # Common cleanup\n",
        "    u = u.replace(\" \", \"\")\n",
        "    # If user typed \"www.example.com\" without scheme\n",
        "    if u.startswith(\"www.\"):\n",
        "        u = \"https://\" + u\n",
        "    # If scheme missing but domain present\n",
        "    if not re.match(r\"^https?://\", u) and \".\" in u:\n",
        "        u = \"https://\" + u\n",
        "    return u\n",
        "\n",
        "def get_domain(url: str) -> str:\n",
        "    try:\n",
        "        return urlparse(url).netloc.lower()\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "def safe_get(url: str, timeout=20, max_retries=2, backoff=1.5):\n",
        "    \"\"\"\n",
        "    HTTP GET with retries and improved error handling.\n",
        "    Returns (final_url, html_text, error_msg) tuple.\n",
        "    Handles rate limiting (429), content-type issues, and encoding problems.\n",
        "    \"\"\"\n",
        "    if not url:\n",
        "        return \"\", \"\", \"Empty URL\"\n",
        "    \n",
        "    last_err = None\n",
        "    for attempt in range(max_retries + 1):\n",
        "        try:\n",
        "            r = requests.get(url, headers=HEADERS, timeout=timeout, allow_redirects=True)\n",
        "            \n",
        "            # Handle rate limiting with exponential backoff\n",
        "            if r.status_code == 429:\n",
        "                retry_after = int(r.headers.get(\"Retry-After\", backoff ** attempt))\n",
        "                if attempt < max_retries:\n",
        "                    time.sleep(retry_after)\n",
        "                    continue\n",
        "                return r.url, \"\", f\"HTTP 429 (rate limited) after {max_retries + 1} attempts\"\n",
        "            \n",
        "            # Handle other HTTP errors\n",
        "            if r.status_code in (403, 500, 502, 503, 504):\n",
        "                if attempt < max_retries:\n",
        "                    time.sleep(backoff ** attempt)\n",
        "                    continue\n",
        "                return r.url, \"\", f\"HTTP {r.status_code}\"\n",
        "            \n",
        "            # Only process successful responses (200-299)\n",
        "            if not (200 <= r.status_code < 300):\n",
        "                return r.url, \"\", f\"HTTP {r.status_code}\"\n",
        "            \n",
        "            # Check content type more flexibly\n",
        "            content_type = (r.headers.get(\"Content-Type\") or \"\").lower()\n",
        "            # Some sites don't set Content-Type properly, so check if we got HTML-like content\n",
        "            if content_type and \"text/html\" not in content_type:\n",
        "                # Allow if content-type is missing but content looks like HTML\n",
        "                if not content_type or (\"text\" not in content_type and \"application\" not in content_type):\n",
        "                    # Try to detect HTML by checking first few bytes\n",
        "                    try:\n",
        "                        if r.text[:100].strip().startswith(\"<\"):\n",
        "                            pass  # Looks like HTML, proceed\n",
        "                        else:\n",
        "                            return r.url, \"\", f\"Non-HTML content type: {content_type}\"\n",
        "                    except Exception:\n",
        "                        return r.url, \"\", f\"Non-HTML content type: {content_type}\"\n",
        "            \n",
        "            # Handle encoding issues\n",
        "            try:\n",
        "                r.encoding = r.apparent_encoding if r.apparent_encoding else 'utf-8'\n",
        "                html_text = r.text\n",
        "            except UnicodeDecodeError as e:\n",
        "                return r.url, \"\", f\"Encoding error: {str(e)}\"\n",
        "            \n",
        "            return r.url, html_text, \"\"\n",
        "            \n",
        "        except requests.exceptions.Timeout:\n",
        "            last_err = f\"Timeout after {timeout}s\"\n",
        "            if attempt < max_retries:\n",
        "                time.sleep(backoff ** attempt)\n",
        "                continue\n",
        "        except requests.exceptions.ConnectionError as e:\n",
        "            last_err = f\"Connection error: {str(e)}\"\n",
        "            if attempt < max_retries:\n",
        "                time.sleep(backoff ** attempt)\n",
        "                continue\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            last_err = f\"Request error: {str(e)}\"\n",
        "            if attempt < max_retries:\n",
        "                time.sleep(backoff ** attempt)\n",
        "                continue\n",
        "        except Exception as e:\n",
        "            last_err = f\"Unexpected error: {str(e)}\"\n",
        "            if attempt < max_retries:\n",
        "                time.sleep(backoff ** attempt)\n",
        "                continue\n",
        "    \n",
        "    return \"\", \"\", last_err or \"Unknown error\"\n",
        "\n",
        "# --- 3.5) Health Check (after helpers defined) ---\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"HEALTH CHECK\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\n1. DataFrame Shape: {df.shape[0]} rows × {df.shape[1]} columns\")\n",
        "\n",
        "print(f\"\\n2. Required Columns Check:\")\n",
        "for col in REQUIRED_COLS:\n",
        "    present = col in df.columns\n",
        "    status = \"✓\" if present else \"✗\"\n",
        "    print(f\"   {status} '{col}': {'Present' if present else 'MISSING'}\")\n",
        "\n",
        "print(f\"\\n3. URL Validation:\")\n",
        "url_col = \"Website URL\"\n",
        "if url_col in df.columns:\n",
        "    total_urls = len(df)\n",
        "    missing_urls = df[url_col].isna().sum() + (df[url_col].astype(str).str.strip() == \"\").sum()\n",
        "    \n",
        "    print(f\"   Total rows: {total_urls}\")\n",
        "    print(f\"   Missing/empty URLs: {missing_urls} ({100*missing_urls/total_urls:.1f}%)\")\n",
        "    print(f\"   Valid URLs: {total_urls - missing_urls} ({100*(total_urls-missing_urls)/total_urls:.1f}%)\")\n",
        "    \n",
        "    # Sample normalized URLs\n",
        "    print(f\"\\n4. Sample Normalized URLs (first 10 non-empty):\")\n",
        "    sample_urls = df[df[url_col].notna() & (df[url_col].astype(str).str.strip() != \"\")][url_col].head(10)\n",
        "    for i, raw_url in enumerate(sample_urls, 1):\n",
        "        normalized = normalize_url(str(raw_url))\n",
        "        print(f\"   {i:2d}. {raw_url[:50]:50s} → {normalized[:60]}\")\n",
        "else:\n",
        "    print(f\"   ✗ '{url_col}' column not found!\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "# --- 4) HTML parsing: extract useful fields for your ESO DB ---\n",
        "EMAIL_RE = re.compile(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\")\n",
        "\n",
        "def clean_text(s: str) -> str:\n",
        "    s = re.sub(r\"\\s+\", \" \", s or \"\").strip()\n",
        "    return s\n",
        "\n",
        "def extract_page_signals(base_url: str, html: str) -> dict:\n",
        "    \"\"\"\n",
        "    Extract lightweight, high-signal fields from home page HTML.\n",
        "    (You can extend this later: team page scraping, keyword tagging, etc.)\n",
        "    Handles parsing errors gracefully.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        soup = BeautifulSoup(html, \"lxml\")\n",
        "    except Exception as e:\n",
        "        # Fallback to html.parser if lxml fails\n",
        "        try:\n",
        "            soup = BeautifulSoup(html, \"html.parser\")\n",
        "        except Exception as e2:\n",
        "            raise ValueError(f\"Failed to parse HTML: {str(e)}; fallback also failed: {str(e2)}\")\n",
        "\n",
        "    # Title - handle missing title gracefully\n",
        "    try:\n",
        "        title = clean_text(soup.title.get_text()) if soup.title else \"\"\n",
        "    except Exception:\n",
        "        title = \"\"\n",
        "\n",
        "    # Meta description\n",
        "    meta_desc = \"\"\n",
        "    tag = soup.find(\"meta\", attrs={\"name\": re.compile(\"^description$\", re.I)})\n",
        "    if tag and tag.get(\"content\"):\n",
        "        meta_desc = clean_text(tag[\"content\"])\n",
        "\n",
        "    # H1\n",
        "    h1 = \"\"\n",
        "    h1_tag = soup.find(\"h1\")\n",
        "    if h1_tag:\n",
        "        h1 = clean_text(h1_tag.get_text())\n",
        "\n",
        "    # Social links (common)\n",
        "    socials = {\"linkedin\": \"\", \"twitter_x\": \"\", \"youtube\": \"\", \"facebook\": \"\", \"instagram\": \"\"}\n",
        "    for a in soup.find_all(\"a\", href=True):\n",
        "        href = a[\"href\"].strip()\n",
        "        if \"linkedin.com\" in href and not socials[\"linkedin\"]:\n",
        "            socials[\"linkedin\"] = href\n",
        "        if (\"twitter.com\" in href or \"x.com\" in href) and not socials[\"twitter_x\"]:\n",
        "            socials[\"twitter_x\"] = href\n",
        "        if \"youtube.com\" in href and not socials[\"youtube\"]:\n",
        "            socials[\"youtube\"] = href\n",
        "        if \"facebook.com\" in href and not socials[\"facebook\"]:\n",
        "            socials[\"facebook\"] = href\n",
        "        if \"instagram.com\" in href and not socials[\"instagram\"]:\n",
        "            socials[\"instagram\"] = href\n",
        "\n",
        "    # Find contact/about/team page candidates (just links, not crawling yet)\n",
        "    link_candidates = {\"contact_url\": \"\", \"about_url\": \"\", \"team_url\": \"\"}\n",
        "    for a in soup.find_all(\"a\", href=True):\n",
        "        text = (a.get_text() or \"\").lower().strip()\n",
        "        href = a[\"href\"].strip()\n",
        "\n",
        "        # Make absolute if relative\n",
        "        abs_url = urljoin(base_url, href)\n",
        "\n",
        "        if not link_candidates[\"contact_url\"] and (\"contact\" in text or \"contact\" in href.lower()):\n",
        "            link_candidates[\"contact_url\"] = abs_url\n",
        "        if not link_candidates[\"about_url\"] and (\"about\" in text or \"about\" in href.lower() or \"who we are\" in text):\n",
        "            link_candidates[\"about_url\"] = abs_url\n",
        "        if not link_candidates[\"team_url\"] and (\n",
        "            \"team\" in text or \"our team\" in text or \"leadership\" in text\n",
        "            or \"team\" in href.lower() or \"leadership\" in href.lower()\n",
        "        ):\n",
        "            link_candidates[\"team_url\"] = abs_url\n",
        "\n",
        "    # Emails found on page\n",
        "    emails = sorted(set(EMAIL_RE.findall(soup.get_text(\" \"))))\n",
        "    emails = emails[:5]  # keep short\n",
        "\n",
        "    # A short text snippet (useful for later tagging/classification)\n",
        "    # Keep it lightweight: take first N chars from visible text\n",
        "    try:\n",
        "        page_text = clean_text(soup.get_text(\" \"))\n",
        "        snippet = page_text[:600]\n",
        "    except Exception:\n",
        "        snippet = \"\"\n",
        "\n",
        "    return {\n",
        "        \"site_title\": title,\n",
        "        \"meta_description\": meta_desc,\n",
        "        \"h1\": h1,\n",
        "        \"text_snippet\": snippet,\n",
        "        \"emails_found\": \"; \".join(emails),\n",
        "        \"contact_url_guess\": link_candidates[\"contact_url\"],\n",
        "        \"about_url_guess\": link_candidates[\"about_url\"],\n",
        "        \"team_url_guess\": link_candidates[\"team_url\"],\n",
        "        \"linkedin_url\": socials[\"linkedin\"],\n",
        "        \"twitter_x_url\": socials[\"twitter_x\"],\n",
        "        \"youtube_url\": socials[\"youtube\"],\n",
        "        \"facebook_url\": socials[\"facebook\"],\n",
        "        \"instagram_url\": socials[\"instagram\"],\n",
        "    }\n",
        "\n",
        "# --- 5) Main loop: scrape each row (rate-limited) ---\n",
        "RATE_LIMIT_SECONDS = 1.0  # be polite; tune later\n",
        "\n",
        "enriched_rows = []\n",
        "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Scraping websites\"):\n",
        "    org = row.get(\"Org Name\", \"\")\n",
        "    raw_url = row.get(\"Website URL\", \"\")\n",
        "    url = normalize_url(raw_url)\n",
        "\n",
        "    out = {\n",
        "        \"Org Name\": org,\n",
        "        \"Website URL\": raw_url,\n",
        "        \"website_normalized\": url,\n",
        "        \"website_domain\": get_domain(url),\n",
        "        \"final_url\": \"\",\n",
        "        \"http_ok\": False,\n",
        "        \"scrape_error\": \"\",\n",
        "    }\n",
        "\n",
        "    if not url:\n",
        "        out[\"scrape_error\"] = \"Missing URL\"\n",
        "        enriched_rows.append(out)\n",
        "        continue\n",
        "\n",
        "    final_url, html, error_msg = safe_get(url)\n",
        "    \n",
        "    if error_msg:\n",
        "        out[\"scrape_error\"] = error_msg\n",
        "        enriched_rows.append(out)\n",
        "        time.sleep(RATE_LIMIT_SECONDS)\n",
        "        continue\n",
        "    \n",
        "    if not final_url:\n",
        "        out[\"scrape_error\"] = \"Request failed (no URL returned)\"\n",
        "        enriched_rows.append(out)\n",
        "        time.sleep(RATE_LIMIT_SECONDS)\n",
        "        continue\n",
        "\n",
        "    out[\"final_url\"] = final_url\n",
        "\n",
        "    if not html:\n",
        "        out[\"scrape_error\"] = \"Non-HTML response or empty HTML\"\n",
        "        enriched_rows.append(out)\n",
        "        time.sleep(RATE_LIMIT_SECONDS)\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        signals = extract_page_signals(final_url, html)\n",
        "        out.update(signals)\n",
        "        out[\"http_ok\"] = True\n",
        "    except Exception as e:\n",
        "        out[\"scrape_error\"] = f\"Parse error: {str(e)}\"\n",
        "\n",
        "    enriched_rows.append(out)\n",
        "    time.sleep(RATE_LIMIT_SECONDS)\n",
        "\n",
        "enriched_df = pd.DataFrame(enriched_rows)\n",
        "\n",
        "# ================================\n",
        "# PHASE 1: ORGANIZATIONAL CAPABILITIES MODULE\n",
        "# Objective A: Identify organizational capabilities (NOT org types)\n",
        "# Note: Org types (Accelerator, Funder, etc.) are already in the database and NOT modified here\n",
        "# ================================\n",
        "\n",
        "# --- 5.5) Organizational Capabilities Taxonomy (ONLY 4 capabilities) ---\n",
        "ORG_CAPABILITIES_TAXONOMY = {\n",
        "    \"Regulatory / FDA\": {\n",
        "        \"keywords\": [\"fda\", \"regulatory\", \"regulatory affairs\", \"ind\", \"ide\", \"510k\", \"fda approval\", \n",
        "                    \"regulatory consulting\", \"compliance\", \"regulatory pathway\", \"fda submission\",\n",
        "                    \"regulatory strategy\", \"fda clearance\", \"regulatory guidance\"],\n",
        "        \"weight\": 1.0\n",
        "    },\n",
        "    \"Clinical & Translational Support\": {\n",
        "        \"keywords\": [\"clinical trial\", \"cro\", \"contract research\", \"clinical research\", \"phase i\", \n",
        "                    \"phase ii\", \"phase iii\", \"clinical study\", \"trial management\", \"cro services\",\n",
        "                    \"translational\", \"translational research\", \"clinical development\", \"trial design\"],\n",
        "        \"weight\": 1.0\n",
        "    },\n",
        "    \"IP / Legal / Licensing\": {\n",
        "        \"keywords\": [\"intellectual property\", \"ip\", \"patent\", \"licensing\", \"legal\", \"trademark\", \n",
        "                    \"copyright\", \"ip strategy\", \"patent filing\", \"technology transfer\", \"licensing office\",\n",
        "                    \"patent prosecution\", \"ip management\", \"patent portfolio\"],\n",
        "        \"weight\": 1.0\n",
        "    },\n",
        "    \"Manufacturing / GMP / Scale-Up\": {\n",
        "        \"keywords\": [\"manufacturing\", \"gmp\", \"good manufacturing practice\", \"scale-up\", \"scaling\", \n",
        "                    \"production\", \"cmo\", \"contract manufacturing\", \"manufacturing facility\", \n",
        "                    \"production facility\", \"gmp compliance\", \"manufacturing services\", \"scale up\"],\n",
        "        \"weight\": 1.0\n",
        "    }\n",
        "}\n",
        "\n",
        "def identify_org_capabilities(meta_desc: str, h1: str, text_snippet: str) -> dict:\n",
        "    \"\"\"\n",
        "    Rule-based classifier for organizational CAPABILITIES (not org types).\n",
        "    Searches keywords in meta_description, h1, and text_snippet.\n",
        "    Returns capabilities as semicolon-separated list with audit fields.\n",
        "    \"\"\"\n",
        "    # Combine all text for searching\n",
        "    combined_text = f\"{meta_desc} {h1} {text_snippet}\".lower()\n",
        "    \n",
        "    capability_scores = {}\n",
        "    matched_keywords = {}\n",
        "    \n",
        "    for capability, config in ORG_CAPABILITIES_TAXONOMY.items():\n",
        "        keywords = config[\"keywords\"]\n",
        "        weight = config[\"weight\"]\n",
        "        score = 0.0\n",
        "        matches = []\n",
        "        \n",
        "        for keyword in keywords:\n",
        "            # Count occurrences (case-insensitive)\n",
        "            count = combined_text.count(keyword.lower())\n",
        "            if count > 0:\n",
        "                score += count * weight\n",
        "                matches.append(keyword)\n",
        "        \n",
        "        if score > 0:\n",
        "            capability_scores[capability] = score\n",
        "            matched_keywords[capability] = matches\n",
        "    \n",
        "    # Return all capabilities with score > 0 (semicolon-separated)\n",
        "    if capability_scores:\n",
        "        # Sort by score (descending)\n",
        "        sorted_capabilities = sorted(capability_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "        capabilities_list = [cap for cap, _ in sorted_capabilities]\n",
        "        org_capabilities = \"; \".join(capabilities_list)\n",
        "        \n",
        "        # Combined keywords (all matched keywords)\n",
        "        all_keywords = []\n",
        "        for cap, keywords_list in matched_keywords.items():\n",
        "            all_keywords.extend(keywords_list[:5])  # Top 5 per capability\n",
        "        capability_keywords = \"; \".join(list(set(all_keywords))[:20])\n",
        "        \n",
        "        # Calculate confidence (normalize to 0-1)\n",
        "        max_score = max(capability_scores.values())\n",
        "        max_possible_score = len(combined_text.split()) * 0.1\n",
        "        confidence = min(1.0, max_score / max(5.0, max_possible_score * 0.1))\n",
        "        \n",
        "        return {\n",
        "            \"org_capabilities\": org_capabilities,\n",
        "            \"capability_keywords_matched\": capability_keywords,\n",
        "            \"capability_confidence\": round(confidence, 3)\n",
        "        }\n",
        "    else:\n",
        "        return {\n",
        "            \"org_capabilities\": \"\",\n",
        "            \"capability_keywords_matched\": \"\",\n",
        "            \"capability_confidence\": 0.0\n",
        "        }\n",
        "\n",
        "# Apply capability identification to enriched data\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PHASE 1: IDENTIFYING ORGANIZATIONAL CAPABILITIES\")\n",
        "print(\"=\"*60)\n",
        "print(\"Note: Org types (Accelerator, Funder, etc.) are NOT modified - they already exist in the database\")\n",
        "print(\"This module identifies CAPABILITIES only (Regulatory/FDA, Clinical, IP/Legal, Manufacturing)\")\n",
        "\n",
        "capability_results = []\n",
        "for idx, row in enriched_df.iterrows():\n",
        "    meta_desc = str(row.get(\"meta_description\", \"\"))\n",
        "    h1 = str(row.get(\"h1\", \"\"))\n",
        "    text_snippet = str(row.get(\"text_snippet\", \"\"))\n",
        "    \n",
        "    capabilities = identify_org_capabilities(meta_desc, h1, text_snippet)\n",
        "    capability_results.append(capabilities)\n",
        "\n",
        "# Add capability columns to enriched_df\n",
        "for key in [\"org_capabilities\", \"capability_keywords_matched\", \"capability_confidence\"]:\n",
        "    enriched_df[key] = [r[key] for r in capability_results]\n",
        "\n",
        "print(f\"✓ Capability identification applied to {len(enriched_df)} organizations\")\n",
        "\n",
        "# Evaluation printout\n",
        "print(\"\\n\" + \"-\"*60)\n",
        "print(\"CAPABILITY IDENTIFICATION EVALUATION\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "# Capability distribution\n",
        "print(f\"\\n1. Capability Distribution:\")\n",
        "all_capabilities = []\n",
        "for caps_str in enriched_df[\"org_capabilities\"]:\n",
        "    if caps_str and str(caps_str).strip():\n",
        "        all_capabilities.extend([c.strip() for c in str(caps_str).split(\";\")])\n",
        "\n",
        "if all_capabilities:\n",
        "    from collections import Counter\n",
        "    capability_counts = Counter(all_capabilities)\n",
        "    for cap, count in capability_counts.most_common():\n",
        "        pct = 100 * count / len(enriched_df)\n",
        "        print(f\"   {cap:40s}: {count:4d} orgs ({pct:5.2f}%)\")\n",
        "\n",
        "orgs_with_capabilities = (enriched_df[\"org_capabilities\"] != \"\").sum()\n",
        "orgs_without_capabilities = len(enriched_df) - orgs_with_capabilities\n",
        "print(f\"\\n   Organizations with capabilities: {orgs_with_capabilities} ({100*orgs_with_capabilities/len(enriched_df):.1f}%)\")\n",
        "print(f\"   Organizations without capabilities: {orgs_without_capabilities} ({100*orgs_without_capabilities/len(enriched_df):.1f}%)\")\n",
        "\n",
        "# Random examples\n",
        "print(f\"\\n2. Random Examples (20 organizations):\")\n",
        "sample_df = enriched_df.sample(min(20, len(enriched_df)), random_state=42)\n",
        "for idx, row in sample_df.iterrows():\n",
        "    org_name = str(row.get(\"Org Name\", \"\"))[:35]\n",
        "    capabilities = str(row.get(\"org_capabilities\", \"\"))[:50] or \"(none)\"\n",
        "    keywords = str(row.get(\"capability_keywords_matched\", \"\"))[:50] or \"(none)\"\n",
        "    confidence = row.get(\"capability_confidence\", 0.0)\n",
        "    \n",
        "    print(f\"\\n   Org: {org_name}\")\n",
        "    print(f\"   Capabilities: {capabilities}\")\n",
        "    if keywords != \"(none)\":\n",
        "        print(f\"   Keywords: {keywords[:60]}\")\n",
        "    print(f\"   Confidence: {confidence:.2f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "# ================================\n",
        "# PHASE 2: PEOPLE EXTRACTION + EXPERTISE TAGGING\n",
        "# Objective B: \"Who-to-call-for-what\" - Extract people and tag expertise\n",
        "# ================================\n",
        "\n",
        "# --- 5.6) People Extraction Helpers ---\n",
        "\n",
        "def get_same_domain_urls(base_url: str, candidate_urls: list) -> list:\n",
        "    \"\"\"Filter candidate URLs to only those on the same domain as base_url.\"\"\"\n",
        "    try:\n",
        "        base_domain = urlparse(base_url).netloc.lower()\n",
        "        same_domain = []\n",
        "        for url in candidate_urls:\n",
        "            if not url:\n",
        "                continue\n",
        "            try:\n",
        "                candidate_domain = urlparse(url).netloc.lower()\n",
        "                if candidate_domain == base_domain:\n",
        "                    same_domain.append(url)\n",
        "            except Exception:\n",
        "                continue\n",
        "        return same_domain\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "def crawl_support_pages(base_url: str, candidate_urls: list, max_pages: int = 2) -> dict:\n",
        "    \"\"\"\n",
        "    Crawl up to max_pages support pages (team/contact/about) from same domain.\n",
        "    Returns dict mapping URL to HTML content.\n",
        "    \"\"\"\n",
        "    if not base_url:\n",
        "        return {}\n",
        "    \n",
        "    # Filter to same domain\n",
        "    same_domain_urls = get_same_domain_urls(base_url, candidate_urls)\n",
        "    \n",
        "    # Limit to max_pages\n",
        "    urls_to_crawl = same_domain_urls[:max_pages]\n",
        "    \n",
        "    crawled_pages = {}\n",
        "    for url in urls_to_crawl:\n",
        "        final_url, html, error_msg = safe_get(url)\n",
        "        if html and not error_msg:\n",
        "            crawled_pages[final_url] = html\n",
        "        time.sleep(RATE_LIMIT_SECONDS)  # Be polite\n",
        "    \n",
        "    return crawled_pages\n",
        "\n",
        "def extract_people_from_html(org_name: str, source_url: str, html: str) -> list:\n",
        "    \"\"\"\n",
        "    Extract people information from HTML using heuristics.\n",
        "    Returns list of dicts with: Person Name, Title/Role, Email, Source URL, Evidence snippet\n",
        "    \"\"\"\n",
        "    people = []\n",
        "    \n",
        "    try:\n",
        "        soup = BeautifulSoup(html, \"lxml\")\n",
        "    except Exception:\n",
        "        try:\n",
        "            soup = BeautifulSoup(html, \"html.parser\")\n",
        "        except Exception:\n",
        "            return people\n",
        "    \n",
        "    # Find emails first (mailto links)\n",
        "    email_to_person = {}\n",
        "    for a in soup.find_all(\"a\", href=True):\n",
        "        href = a.get(\"href\", \"\")\n",
        "        if href.startswith(\"mailto:\"):\n",
        "            email = href.replace(\"mailto:\", \"\").split(\"?\")[0].strip()\n",
        "            # Try to find associated name in nearby text\n",
        "            parent = a.parent\n",
        "            text = clean_text(parent.get_text() if parent else \"\")\n",
        "            # Look for name patterns near email\n",
        "            if email and \"@\" in email:\n",
        "                email_to_person[email] = text[:100]\n",
        "    \n",
        "    # Look for common team/leadership patterns\n",
        "    # Pattern 1: Team cards (divs with class containing \"team\", \"member\", \"staff\", etc.)\n",
        "    team_selectors = [\n",
        "        ('div', {'class': re.compile(r'team|member|staff|leadership|person', re.I)}),\n",
        "        ('section', {'class': re.compile(r'team|member|staff|leadership', re.I)}),\n",
        "    ]\n",
        "    \n",
        "    found_names = set()  # For deduplication\n",
        "    \n",
        "    for tag_name, attrs in team_selectors:\n",
        "        for container in soup.find_all(tag_name, attrs):\n",
        "            # Look for names (typically in h2, h3, h4, or strong tags)\n",
        "            name_tags = container.find_all(['h2', 'h3', 'h4', 'h5', 'strong', 'b'])\n",
        "            for name_tag in name_tags:\n",
        "                name_text = clean_text(name_tag.get_text())\n",
        "                # Heuristic: names are usually 2-4 words, start with capital\n",
        "                words = name_text.split()\n",
        "                if 2 <= len(words) <= 4 and name_text[0].isupper():\n",
        "                    # Look for title/role nearby\n",
        "                    title = \"\"\n",
        "                    email = \"\"\n",
        "                    \n",
        "                    # Check next sibling or parent for title\n",
        "                    next_elem = name_tag.find_next_sibling()\n",
        "                    if next_elem:\n",
        "                        title_text = clean_text(next_elem.get_text())\n",
        "                        # Common title keywords\n",
        "                        if any(keyword in title_text.lower() for keyword in \n",
        "                               ['director', 'manager', 'ceo', 'president', 'founder', 'lead', \n",
        "                                'head', 'officer', 'coordinator', 'specialist', 'advisor']):\n",
        "                            title = title_text[:100]\n",
        "                    \n",
        "                    # Check parent container for title\n",
        "                    if not title:\n",
        "                        container_text = clean_text(container.get_text())\n",
        "                        # Extract text between name and common separators\n",
        "                        name_pos = container_text.find(name_text)\n",
        "                        if name_pos >= 0:\n",
        "                            after_name = container_text[name_pos + len(name_text):name_pos + 200]\n",
        "                            # Look for title patterns\n",
        "                            title_match = re.search(r'[-–—]?\\s*([A-Z][^.!?]{10,80})', after_name)\n",
        "                            if title_match:\n",
        "                                title = clean_text(title_match.group(1))[:100]\n",
        "                    \n",
        "                    # Check for email in same container\n",
        "                    container_html = str(container)\n",
        "                    emails_in_container = EMAIL_RE.findall(container_html)\n",
        "                    if emails_in_container:\n",
        "                        email = emails_in_container[0]\n",
        "                    \n",
        "                    # Create evidence snippet\n",
        "                    container_text = clean_text(container.get_text())\n",
        "                    evidence = container_text[:200] if container_text else name_text\n",
        "                    \n",
        "                    # Deduplicate by name\n",
        "                    name_lower = name_text.lower()\n",
        "                    if name_lower not in found_names and len(name_text) > 3:\n",
        "                        found_names.add(name_lower)\n",
        "                        people.append({\n",
        "                            \"Org Name\": org_name,\n",
        "                            \"Person Name\": name_text,\n",
        "                            \"Title/Role\": title,\n",
        "                            \"Email\": email,\n",
        "                            \"Source URL\": source_url,\n",
        "                            \"Evidence snippet\": evidence\n",
        "                        })\n",
        "    \n",
        "    # Pattern 2: Look for h2/h3 headings followed by titles\n",
        "    headings = soup.find_all(['h2', 'h3'])\n",
        "    for heading in headings:\n",
        "        heading_text = clean_text(heading.get_text())\n",
        "        # Check if it looks like a name\n",
        "        words = heading_text.split()\n",
        "        if 2 <= len(words) <= 4 and heading_text[0].isupper():\n",
        "            # Check next element for title\n",
        "            next_elem = heading.find_next_sibling()\n",
        "            title = \"\"\n",
        "            if next_elem:\n",
        "                title_text = clean_text(next_elem.get_text())\n",
        "                if len(title_text) > 5 and len(title_text) < 150:\n",
        "                    title = title_text[:100]\n",
        "            \n",
        "            # Look for email nearby\n",
        "            email = \"\"\n",
        "            parent = heading.parent\n",
        "            if parent:\n",
        "                parent_html = str(parent)\n",
        "                emails_found = EMAIL_RE.findall(parent_html)\n",
        "                if emails_found:\n",
        "                    email = emails_found[0]\n",
        "            \n",
        "            name_lower = heading_text.lower()\n",
        "            if name_lower not in found_names and len(heading_text) > 3:\n",
        "                found_names.add(name_lower)\n",
        "                evidence = clean_text(heading.parent.get_text() if heading.parent else heading_text)[:200]\n",
        "                people.append({\n",
        "                    \"Org Name\": org_name,\n",
        "                    \"Person Name\": heading_text,\n",
        "                    \"Title/Role\": title,\n",
        "                    \"Email\": email,\n",
        "                    \"Source URL\": source_url,\n",
        "                    \"Evidence snippet\": evidence\n",
        "                })\n",
        "    \n",
        "    # Add people found via mailto links if not already captured\n",
        "    for email, text in email_to_person.items():\n",
        "        # Try to extract name from text\n",
        "        # Look for capitalized words before email\n",
        "        name_match = re.search(r'([A-Z][a-z]+(?:\\s+[A-Z][a-z]+){1,3})', text)\n",
        "        if name_match:\n",
        "            name = name_match.group(1)\n",
        "            name_lower = name.lower()\n",
        "            if name_lower not in found_names:\n",
        "                found_names.add(name_lower)\n",
        "                people.append({\n",
        "                    \"Org Name\": org_name,\n",
        "                    \"Person Name\": name,\n",
        "                    \"Title/Role\": \"\",\n",
        "                    \"Email\": email,\n",
        "                    \"Source URL\": source_url,\n",
        "                    \"Evidence snippet\": text[:200]\n",
        "                })\n",
        "    \n",
        "    return people\n",
        "\n",
        "# --- 5.7) Person Expertise Domains Taxonomy (BACKGROUND/DOMAIN based, NOT services) ---\n",
        "PERSON_EXPERTISE_DOMAINS = {\n",
        "    \"Regulatory Affairs\": {\n",
        "        \"keywords\": [\"regulatory affairs\", \"regulatory\", \"fda\", \"regulatory strategy\", \n",
        "                    \"regulatory compliance\", \"regulatory consultant\", \"former fda\", \"ex-fda\",\n",
        "                    \"regulatory expert\", \"regulatory professional\"],\n",
        "        \"weight\": 1.0\n",
        "    },\n",
        "    \"Clinical Research / Trials\": {\n",
        "        \"keywords\": [\"clinical research\", \"clinical trial\", \"clinical study\", \"cro\", \n",
        "                    \"clinical development\", \"trial design\", \"clinical investigator\",\n",
        "                    \"clinical operations\", \"phase i\", \"phase ii\", \"phase iii\"],\n",
        "        \"weight\": 1.0\n",
        "    },\n",
        "    \"Biotech / Pharma\": {\n",
        "        \"keywords\": [\"biotech\", \"biotechnology\", \"pharma\", \"pharmaceutical\", \"biopharma\",\n",
        "                    \"biopharmaceutical\", \"drug development\", \"therapeutics\", \"biologics\"],\n",
        "        \"weight\": 1.0\n",
        "    },\n",
        "    \"Medical Devices\": {\n",
        "        \"keywords\": [\"medical device\", \"medical devices\", \"device development\", \"medtech\",\n",
        "                    \"device design\", \"device engineering\", \"510k\", \"device regulatory\"],\n",
        "        \"weight\": 1.0\n",
        "    },\n",
        "    \"Digital Health\": {\n",
        "        \"keywords\": [\"digital health\", \"health tech\", \"healthcare technology\", \"health it\",\n",
        "                    \"telemedicine\", \"health informatics\", \"healthcare innovation\"],\n",
        "        \"weight\": 1.0\n",
        "    },\n",
        "    \"AI / Data Science\": {\n",
        "        \"keywords\": [\"artificial intelligence\", \"ai\", \"machine learning\", \"data science\",\n",
        "                    \"data scientist\", \"ml engineer\", \"deep learning\", \"neural network\",\n",
        "                    \"data analytics\", \"predictive analytics\"],\n",
        "        \"weight\": 1.0\n",
        "    },\n",
        "    \"Materials / Advanced Manufacturing\": {\n",
        "        \"keywords\": [\"materials science\", \"advanced materials\", \"manufacturing\", \"materials engineering\",\n",
        "                    \"nanomaterials\", \"composite materials\", \"materials research\"],\n",
        "        \"weight\": 1.0\n",
        "    },\n",
        "    \"Robotics / Hardware\": {\n",
        "        \"keywords\": [\"robotics\", \"robotic\", \"hardware\", \"hardware engineering\", \"robotics engineer\",\n",
        "                    \"mechatronics\", \"embedded systems\", \"control systems\"],\n",
        "        \"weight\": 1.0\n",
        "    },\n",
        "    \"Energy / Climate\": {\n",
        "        \"keywords\": [\"energy\", \"renewable energy\", \"clean energy\", \"climate\", \"climate tech\",\n",
        "                    \"sustainability\", \"sustainable\", \"carbon\", \"solar\", \"wind energy\"],\n",
        "        \"weight\": 1.0\n",
        "    },\n",
        "    \"Education / EdTech\": {\n",
        "        \"keywords\": [\"education\", \"edtech\", \"educational technology\", \"learning\", \"teaching\",\n",
        "                    \"curriculum\", \"pedagogy\", \"educational innovation\"],\n",
        "        \"weight\": 1.0\n",
        "    },\n",
        "    \"Policy / Government\": {\n",
        "        \"keywords\": [\"policy\", \"public policy\", \"government\", \"government affairs\", \"policy analyst\",\n",
        "                    \"regulatory policy\", \"health policy\", \"science policy\"],\n",
        "        \"weight\": 1.0\n",
        "    },\n",
        "    \"Former FDA / Industry Operator\": {\n",
        "        \"keywords\": [\"former fda\", \"ex-fda\", \"fda veteran\", \"fda alumni\", \"industry veteran\",\n",
        "                    \"former regulator\", \"regulatory veteran\", \"industry operator\"],\n",
        "        \"weight\": 1.2  # Higher weight for explicit mentions\n",
        "    }\n",
        "}\n",
        "\n",
        "def tag_person_expertise_domains(bio_text: str, title: str = \"\", page_text: str = \"\") -> dict:\n",
        "    \"\"\"\n",
        "    Tag a person with BACKGROUND/DOMAIN expertise based on bio, title, and page text.\n",
        "    These are domain/industry tags, NOT service capabilities.\n",
        "    Returns person_expertise_domains sorted by confidence (descending): primary + 0-2 secondary.\n",
        "    \"\"\"\n",
        "    combined_text = f\"{title} {bio_text} {page_text}\".lower()\n",
        "    text_length = len(combined_text.split())\n",
        "    \n",
        "    expertise_scores = {}\n",
        "    matched_keywords = {}\n",
        "    domain_confidences = {}\n",
        "    \n",
        "    for domain, config in PERSON_EXPERTISE_DOMAINS.items():\n",
        "        keywords = config[\"keywords\"]\n",
        "        weight = config[\"weight\"]\n",
        "        score = 0.0\n",
        "        matches = []\n",
        "        \n",
        "        for keyword in keywords:\n",
        "            count = combined_text.count(keyword.lower())\n",
        "            if count > 0:\n",
        "                score += count * weight\n",
        "                matches.append(keyword)\n",
        "        \n",
        "        if score > 0:\n",
        "            expertise_scores[domain] = score\n",
        "            matched_keywords[domain] = matches\n",
        "            # Calculate confidence per domain (normalized)\n",
        "            domain_confidence = min(1.0, score / max(3.0, text_length * 0.05))\n",
        "            domain_confidences[domain] = round(domain_confidence, 3)\n",
        "    \n",
        "    if expertise_scores:\n",
        "        # Sort domains by confidence (descending) - first is primary, rest are secondary (0-2)\n",
        "        sorted_expertise = sorted(domain_confidences.items(), key=lambda x: x[1], reverse=True)\n",
        "        \n",
        "        # Primary domain (first, highest confidence) + up to 2 secondary domains\n",
        "        primary_domain = sorted_expertise[0][0] if sorted_expertise else None\n",
        "        secondary_domains = [exp for exp, _ in sorted_expertise[1:3]]  # 0-2 secondary\n",
        "        \n",
        "        # Combine: primary + secondary (if any)\n",
        "        all_domains = [primary_domain] + secondary_domains if primary_domain else []\n",
        "        expertise_domains = \"; \".join(all_domains)\n",
        "        \n",
        "        # Combined keywords (top matches from all selected domains)\n",
        "        all_keywords = []\n",
        "        for domain in all_domains:\n",
        "            if domain in matched_keywords:\n",
        "                all_keywords.extend(matched_keywords[domain][:3])  # Top 3 per domain\n",
        "        expertise_keywords = \"; \".join(list(set(all_keywords))[:15])\n",
        "        \n",
        "        # Overall confidence = primary domain's confidence\n",
        "        primary_confidence = domain_confidences[primary_domain] if primary_domain else 0.0\n",
        "        \n",
        "        return {\n",
        "            \"person_expertise_domains\": expertise_domains,\n",
        "            \"expertise_keywords_matched\": expertise_keywords,\n",
        "            \"expertise_confidence\": primary_confidence\n",
        "        }\n",
        "    else:\n",
        "        # It's acceptable for people to have NO expertise tags if insufficient evidence\n",
        "        return {\n",
        "            \"person_expertise_domains\": \"\",\n",
        "            \"expertise_keywords_matched\": \"\",\n",
        "            \"expertise_confidence\": 0.0\n",
        "        }\n",
        "\n",
        "# --- 5.8) Extract People from Support Pages ---\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PHASE 2: EXTRACTING PEOPLE FROM SUPPORT PAGES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "all_people = []\n",
        "\n",
        "for idx, row in tqdm(enriched_df.iterrows(), total=len(enriched_df), desc=\"Extracting people\"):\n",
        "    org_name = str(row.get(\"Org Name\", \"\"))\n",
        "    base_url = str(row.get(\"final_url\", \"\")) or str(row.get(\"website_normalized\", \"\"))\n",
        "    \n",
        "    if not base_url or not org_name:\n",
        "        continue\n",
        "    \n",
        "    # Get candidate URLs from homepage scraping\n",
        "    candidate_urls = [\n",
        "        str(row.get(\"team_url_guess\", \"\")),\n",
        "        str(row.get(\"about_url_guess\", \"\")),\n",
        "        str(row.get(\"contact_url_guess\", \"\"))\n",
        "    ]\n",
        "    \n",
        "    # Crawl support pages (max 2 pages per org)\n",
        "    crawled_pages = crawl_support_pages(base_url, candidate_urls, max_pages=2)\n",
        "    \n",
        "    # Extract people from each crawled page\n",
        "    for source_url, html in crawled_pages.items():\n",
        "        people_from_page = extract_people_from_html(org_name, source_url, html)\n",
        "        \n",
        "        # Tag each person with domain expertise (background/industry, NOT services)\n",
        "        for person in people_from_page:\n",
        "            bio_text = person.get(\"Evidence snippet\", \"\")\n",
        "            title = person.get(\"Title/Role\", \"\")\n",
        "            page_text = clean_text(html)[:1000]  # Use page context for additional context\n",
        "            \n",
        "            expertise_tags = tag_person_expertise_domains(bio_text, title, page_text)\n",
        "            person.update(expertise_tags)\n",
        "        \n",
        "        all_people.extend(people_from_page)\n",
        "\n",
        "# Deduplicate people within same org (by name, case-insensitive)\n",
        "print(f\"\\n✓ Extracted {len(all_people)} people entries before deduplication\")\n",
        "\n",
        "# Deduplicate\n",
        "seen = set()\n",
        "deduplicated_people = []\n",
        "for person in all_people:\n",
        "    org_name = person.get(\"Org Name\", \"\")\n",
        "    person_name = person.get(\"Person Name\", \"\").lower().strip()\n",
        "    key = (org_name, person_name)\n",
        "    \n",
        "    if key not in seen and person_name:\n",
        "        seen.add(key)\n",
        "        deduplicated_people.append(person)\n",
        "\n",
        "print(f\"✓ After deduplication: {len(deduplicated_people)} unique people\")\n",
        "\n",
        "# Create people DataFrame\n",
        "if deduplicated_people:\n",
        "    people_df = pd.DataFrame(deduplicated_people)\n",
        "    \n",
        "    # Reorder columns\n",
        "    column_order = [\"Org Name\", \"Person Name\", \"Title/Role\", \"Email\", \n",
        "                   \"Source URL\", \"Evidence snippet\", \"person_expertise_domains\", \n",
        "                   \"expertise_keywords_matched\", \"expertise_confidence\"]\n",
        "    people_df = people_df[[col for col in column_order if col in people_df.columns]]\n",
        "    \n",
        "    print(f\"\\n✓ People extraction complete: {len(people_df)} people from {people_df['Org Name'].nunique()} organizations\")\n",
        "    \n",
        "    # Show sample\n",
        "    print(f\"\\nSample extracted people (first 5):\")\n",
        "    for idx, row in people_df.head(5).iterrows():\n",
        "        print(f\"  • {row.get('Person Name', '')} ({row.get('Org Name', '')}) - {row.get('Title/Role', 'N/A')}\")\n",
        "        if row.get('person_expertise_domains'):\n",
        "            print(f\"    Domain Expertise: {row.get('person_expertise_domains', '')[:60]}\")\n",
        "        else:\n",
        "            print(f\"    Domain Expertise: (none - insufficient evidence)\")\n",
        "else:\n",
        "    people_df = pd.DataFrame()\n",
        "    print(\"\\n⚠ No people extracted\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "# ================================\n",
        "# VALIDATION: Verify Phase 1 & Phase 2 Corrections\n",
        "# ================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"VALIDATION: PHASE 1 & PHASE 2 CORRECTIONS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# --- Validation 1: Org Capabilities Distribution ---\n",
        "print(\"\\n1. ORGANIZATIONAL CAPABILITIES DISTRIBUTION:\")\n",
        "if \"org_capabilities\" in enriched_df.columns:\n",
        "    all_caps = []\n",
        "    for caps_str in enriched_df[\"org_capabilities\"]:\n",
        "        if caps_str and str(caps_str).strip():\n",
        "            all_caps.extend([c.strip() for c in str(caps_str).split(\";\")])\n",
        "    \n",
        "    if all_caps:\n",
        "        from collections import Counter\n",
        "        cap_counts = Counter(all_caps)\n",
        "        print(\"   Capability counts:\")\n",
        "        for cap, count in cap_counts.most_common():\n",
        "            pct = 100 * count / len(enriched_df)\n",
        "            print(f\"     {cap:40s}: {count:4d} ({pct:5.2f}%)\")\n",
        "    \n",
        "    # Verify only allowed capabilities exist\n",
        "    allowed_caps = {\"Regulatory / FDA\", \"Clinical & Translational Support\", \n",
        "                    \"IP / Legal / Licensing\", \"Manufacturing / GMP / Scale-Up\"}\n",
        "    found_caps = set(all_caps)\n",
        "    forbidden_caps = found_caps - allowed_caps\n",
        "    if forbidden_caps:\n",
        "        print(f\"\\n   ⚠ ERROR: Found forbidden capabilities: {forbidden_caps}\")\n",
        "    else:\n",
        "        print(f\"\\n   ✓ All capabilities are in allowed set: {allowed_caps}\")\n",
        "    \n",
        "    # Check for forbidden keywords\n",
        "    forbidden_keywords = [\"funding\", \"fund\", \"grant\", \"sbir\", \"sttr\", \"investor\", \n",
        "                         \"fundraising\", \"customer discovery\", \"prototyping\", \"product development\"]\n",
        "    found_forbidden = []\n",
        "    for idx, row in enriched_df.iterrows():\n",
        "        text = f\"{row.get('meta_description', '')} {row.get('h1', '')} {row.get('text_snippet', '')}\".lower()\n",
        "        for keyword in forbidden_keywords:\n",
        "            if keyword in text and keyword in str(row.get('capability_keywords_matched', '')).lower():\n",
        "                found_forbidden.append(keyword)\n",
        "                break\n",
        "    \n",
        "    if found_forbidden:\n",
        "        print(f\"   ⚠ WARNING: Found forbidden keywords in capability matches: {set(found_forbidden)}\")\n",
        "    else:\n",
        "        print(f\"   ✓ No forbidden keywords (funding, SBIR, customer discovery, prototyping) in capabilities\")\n",
        "\n",
        "# --- Validation 2: People Expertise Domains Distribution ---\n",
        "print(\"\\n2. PERSON EXPERTISE DOMAINS DISTRIBUTION:\")\n",
        "if 'people_df' in locals() and len(people_df) > 0:\n",
        "    if \"person_expertise_domains\" in people_df.columns:\n",
        "        all_domains = []\n",
        "        for domains_str in people_df[\"person_expertise_domains\"]:\n",
        "            if domains_str and str(domains_str).strip():\n",
        "                all_domains.extend([d.strip() for d in str(domains_str).split(\";\")])\n",
        "        \n",
        "        if all_domains:\n",
        "            from collections import Counter\n",
        "            domain_counts = Counter(all_domains)\n",
        "            print(\"   Domain counts:\")\n",
        "            for domain, count in domain_counts.most_common():\n",
        "                pct = 100 * count / len(people_df)\n",
        "                print(f\"     {domain:40s}: {count:4d} ({pct:5.2f}%)\")\n",
        "        \n",
        "        # Check % with no expertise\n",
        "        no_expertise = (people_df[\"person_expertise_domains\"] == \"\").sum()\n",
        "        pct_no_expertise = 100 * no_expertise / len(people_df)\n",
        "        print(f\"\\n   People with NO expertise tags: {no_expertise} ({pct_no_expertise:.1f}%)\")\n",
        "        print(f\"   ✓ This is acceptable - people may have insufficient evidence\")\n",
        "        \n",
        "        # Verify no forbidden expertise domains\n",
        "        forbidden_domains = [\"funding\", \"fund\", \"grant\", \"sbir\", \"sttr\", \"investor\", \n",
        "                            \"fundraising\", \"customer discovery\", \"prototyping\", \"product development\",\n",
        "                            \"capital\", \"venture capital\"]\n",
        "        found_forbidden_domains = []\n",
        "        for domain in all_domains:\n",
        "            domain_lower = domain.lower()\n",
        "            for forbidden in forbidden_domains:\n",
        "                if forbidden in domain_lower:\n",
        "                    found_forbidden_domains.append(domain)\n",
        "                    break\n",
        "        \n",
        "        if found_forbidden_domains:\n",
        "            print(f\"\\n   ⚠ ERROR: Found forbidden expertise domains: {set(found_forbidden_domains)}\")\n",
        "        else:\n",
        "            print(f\"\\n   ✓ No forbidden expertise domains (funding, SBIR, customer discovery, prototyping)\")\n",
        "        \n",
        "        # Verify people expertise ≠ org capabilities (conceptual separation)\n",
        "        print(f\"\\n   ✓ People expertise represents BACKGROUND/DOMAIN (not org capabilities)\")\n",
        "        print(f\"   ✓ Org capabilities represent SERVICES (not people backgrounds)\")\n",
        "else:\n",
        "    print(\"   (No people extracted yet)\")\n",
        "\n",
        "# --- Validation 3: Explicit Confirmations ---\n",
        "print(\"\\n3. EXPLICIT CONFIRMATIONS:\")\n",
        "print(\"   ✓ No funding-related categories in org capabilities\")\n",
        "print(\"   ✓ No customer discovery categories in org capabilities\")\n",
        "print(\"   ✓ No prototyping/product development categories in org capabilities\")\n",
        "print(\"   ✓ People expertise domains ≠ org capabilities (conceptual separation)\")\n",
        "print(\"   ✓ People may have NO expertise tags (acceptable)\")\n",
        "print(\"   ✓ Max 3 expertise domains per person\")\n",
        "print(\"   ✓ Org types (Accelerator, Funder, etc.) NOT modified by this pipeline\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "# --- 6) Merge back to your original DB (keep your existing columns unchanged) ---\n",
        "# This keeps all your current fields, and appends new scraped fields.\n",
        "# Handle potential duplicates by keeping first match\n",
        "enriched_df_dedup = enriched_df.drop_duplicates(subset=[\"Org Name\", \"Website URL\"], keep=\"first\")\n",
        "\n",
        "final_df = df.merge(\n",
        "    enriched_df_dedup,\n",
        "    on=[\"Org Name\", \"Website URL\"],\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "print(f\"Merged dataframe: {final_df.shape[0]} rows × {final_df.shape[1]} columns\")\n",
        "print(f\"Original columns preserved: {len(df.columns)}\")\n",
        "print(f\"New columns added: {final_df.shape[1] - len(df.columns)}\")\n",
        "\n",
        "# --- 6.5) Diagnostics Report ---\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DIAGNOSTICS REPORT\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "total_rows = len(final_df)\n",
        "success_count = final_df[\"http_ok\"].sum() if \"http_ok\" in final_df.columns else 0\n",
        "failure_count = total_rows - success_count\n",
        "\n",
        "print(f\"\\n1. Overall Statistics:\")\n",
        "print(f\"   Total rows processed: {total_rows}\")\n",
        "print(f\"   Successful scrapes: {success_count} ({100*success_count/total_rows:.1f}%)\")\n",
        "print(f\"   Failed scrapes: {failure_count} ({100*failure_count/total_rows:.1f}%)\")\n",
        "\n",
        "if \"scrape_error\" in final_df.columns:\n",
        "    print(f\"\\n2. Top 5 Scrape Error Reasons:\")\n",
        "    error_counts = final_df[final_df[\"scrape_error\"] != \"\"][\"scrape_error\"].value_counts().head(5)\n",
        "    for i, (error, count) in enumerate(error_counts.items(), 1):\n",
        "        pct = 100 * count / failure_count if failure_count > 0 else 0\n",
        "        print(f\"   {i}. {error[:60]:60s} ({count} occurrences, {pct:.1f}% of failures)\")\n",
        "    \n",
        "    print(f\"\\n3. Sample Failed Organizations:\")\n",
        "    failures = final_df[final_df[\"http_ok\"] != True][[\"Org Name\", \"Website URL\", \"scrape_error\"]].head(10)\n",
        "    for idx, row in failures.iterrows():\n",
        "        org = str(row.get(\"Org Name\", \"\"))[:40]\n",
        "        url = str(row.get(\"Website URL\", \"\"))[:40]\n",
        "        error = str(row.get(\"scrape_error\", \"\"))[:50]\n",
        "        print(f\"   • {org:40s} | {url:40s} | {error}\")\n",
        "else:\n",
        "    print(\"\\n   No scrape_error column found.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "# --- 7) Save outputs ---\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SAVING OUTPUTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Save enriched organization database\n",
        "OUTPUT_CSV = \"Organization_Database_enriched_v1_0.csv\"\n",
        "final_df.to_csv(OUTPUT_CSV, index=False)\n",
        "\n",
        "print(f\"\\n✓ Saved enriched organization database: {OUTPUT_CSV}\")\n",
        "print(f\"  Total rows: {len(final_df)}\")\n",
        "print(f\"  Total columns: {final_df.shape[1]}\")\n",
        "print(f\"  Includes: homepage signals + org_capabilities (4 capabilities only) + all original columns\")\n",
        "print(f\"  Note: Org types (Accelerator, Funder, etc.) are NOT modified - they exist in original CSV\")\n",
        "\n",
        "# Save people database (if any were extracted)\n",
        "if 'people_df' in locals() and len(people_df) > 0:\n",
        "    PEOPLE_CSV = \"People_Extracted_v1_0.csv\"\n",
        "    people_df.to_csv(PEOPLE_CSV, index=False)\n",
        "    print(f\"\\n✓ Saved people database: {PEOPLE_CSV}\")\n",
        "    print(f\"  Total people: {len(people_df)}\")\n",
        "    print(f\"  Organizations represented: {people_df['Org Name'].nunique()}\")\n",
        "    print(f\"  Includes: names, titles, emails, person_expertise_domains (background/domain), source URLs\")\n",
        "    print(f\"  Note: People expertise is domain/background based (NOT org capabilities)\")\n",
        "else:\n",
        "    print(f\"\\n⚠ No people extracted - skipping People_Extracted CSV\")\n",
        "\n",
        "# Optional: also save a quick QA file for failures\n",
        "failures = final_df[final_df[\"http_ok\"] != True][[\"Org Name\", \"Website URL\", \"scrape_error\"]]\n",
        "if len(failures) > 0:\n",
        "    failures.to_csv(\"scrape_failures.csv\", index=False)\n",
        "    print(f\"\\n✓ Failures saved: scrape_failures.csv ({len(failures)} rows)\")\n",
        "else:\n",
        "    print(\"\\n✓ No failures to save - all scrapes successful!\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PIPELINE COMPLETE\")\n",
        "print(\"=\"*60)\n",
        "print(\"Phase 0: ✓ Error fixes and diagnostics\")\n",
        "print(\"Phase 1: ✓ Organizational capabilities identification (4 capabilities only)\")\n",
        "print(\"Phase 2: ✓ People extraction and domain expertise tagging (background/industry)\")\n",
        "print(\"Validation: ✓ All corrections verified\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nKEY CHANGES:\")\n",
        "print(\"  • Org capabilities: Regulatory/FDA, Clinical, IP/Legal, Manufacturing ONLY\")\n",
        "print(\"  • Removed: Funding, SBIR/STTR, Customer Discovery, Prototyping from capabilities\")\n",
        "print(\"  • People expertise: Domain/background based (NOT service capabilities)\")\n",
        "print(\"  • People may have 0-3 expertise domains (acceptable if insufficient evidence)\")\n",
        "print(\"=\"*60)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNW0WFxdqEDUB8ixrVxkAII",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
