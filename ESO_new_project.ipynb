{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOOwqVfDsoDGAasQOhSeFS0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ataucuriaia/ESO-new-project/blob/main/ESO_new_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4cjw3fHznrp",
        "outputId": "e26aa617-c9d1-43af-bf85-ae27205331f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  9%|▊         | 42/492 [02:13<16:40,  2.22s/it]"
          ]
        }
      ],
      "source": [
        "# ================================\n",
        "# Colab Starter Notebook (English)\n",
        "# Enterprise Studio — ESO Web Scraping + Enrichment (v0.1)\n",
        "# Inputs: your existing CSV with columns:\n",
        "#   - \"Org Name\" (col A equivalent)\n",
        "#   - \"Website URL\" (col F equivalent)\n",
        "# Output: enriched CSV with scraped signals (title, meta description, text snippet, contacts, socials, etc.)\n",
        "# ================================\n",
        "\n",
        "# --- 1) Install + imports ---\n",
        "!pip -q install beautifulsoup4 lxml tqdm requests\n",
        "\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "from urllib.parse import urlparse, urljoin\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- 2) Load your existing database (uploaded to Colab) ---\n",
        "INPUT_PATH = \"/content/Organization Database 1f24e34e337d8027b500d2a10b1ceaa7.csv\"\n",
        "# If you re-upload with a different name, change INPUT_PATH.\n",
        "\n",
        "df = pd.read_csv(INPUT_PATH)\n",
        "\n",
        "# Basic checks (matches your structure)\n",
        "REQUIRED_COLS = [\"Org Name\", \"Website URL\"]\n",
        "missing = [c for c in REQUIRED_COLS if c not in df.columns]\n",
        "if missing:\n",
        "    raise ValueError(f\"Missing required columns: {missing}. Found columns: {list(df.columns)}\")\n",
        "\n",
        "df.head()\n",
        "\n",
        "# --- 3) Helpers: URL cleaning + safe request ---\n",
        "HEADERS = {\n",
        "    \"User-Agent\": (\n",
        "        \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \"\n",
        "        \"(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
        "    )\n",
        "}\n",
        "\n",
        "def normalize_url(url: str) -> str:\n",
        "    \"\"\"Normalize website URL. Adds scheme if missing, strips whitespace.\"\"\"\n",
        "    if not isinstance(url, str) or not url.strip():\n",
        "        return \"\"\n",
        "    u = url.strip()\n",
        "    # Common cleanup\n",
        "    u = u.replace(\" \", \"\")\n",
        "    # If user typed \"www.example.com\" without scheme\n",
        "    if u.startswith(\"www.\"):\n",
        "        u = \"https://\" + u\n",
        "    # If scheme missing but domain present\n",
        "    if not re.match(r\"^https?://\", u) and \".\" in u:\n",
        "        u = \"https://\" + u\n",
        "    return u\n",
        "\n",
        "def get_domain(url: str) -> str:\n",
        "    try:\n",
        "        return urlparse(url).netloc.lower()\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "def safe_get(url: str, timeout=20, max_retries=2, backoff=1.5):\n",
        "    \"\"\"HTTP GET with retries. Returns (final_url, html_text) or (\"\",\"\").\"\"\"\n",
        "    if not url:\n",
        "        return \"\", \"\"\n",
        "    last_err = None\n",
        "    for attempt in range(max_retries + 1):\n",
        "        try:\n",
        "            r = requests.get(url, headers=HEADERS, timeout=timeout, allow_redirects=True)\n",
        "            # Handle common blocks\n",
        "            if r.status_code in (403, 429, 500, 502, 503, 504):\n",
        "                raise RuntimeError(f\"HTTP {r.status_code}\")\n",
        "            if \"text/html\" not in (r.headers.get(\"Content-Type\") or \"\"):\n",
        "                # Some sites return PDFs or other content; skip for now\n",
        "                return r.url, \"\"\n",
        "            return r.url, r.text\n",
        "        except Exception as e:\n",
        "            last_err = str(e)\n",
        "            time.sleep(backoff ** attempt)\n",
        "    return \"\", \"\"\n",
        "\n",
        "# --- 4) HTML parsing: extract useful fields for your ESO DB ---\n",
        "EMAIL_RE = re.compile(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\")\n",
        "\n",
        "def clean_text(s: str) -> str:\n",
        "    s = re.sub(r\"\\s+\", \" \", s or \"\").strip()\n",
        "    return s\n",
        "\n",
        "def extract_page_signals(base_url: str, html: str) -> dict:\n",
        "    \"\"\"\n",
        "    Extract lightweight, high-signal fields from home page HTML.\n",
        "    (You can extend this later: team page scraping, keyword tagging, etc.)\n",
        "    \"\"\"\n",
        "    soup = BeautifulSoup(html, \"lxml\")\n",
        "\n",
        "    # Title\n",
        "    title = clean_text(soup.title.get_text()) if soup.title else \"\"\n",
        "\n",
        "    # Meta description\n",
        "    meta_desc = \"\"\n",
        "    tag = soup.find(\"meta\", attrs={\"name\": re.compile(\"^description$\", re.I)})\n",
        "    if tag and tag.get(\"content\"):\n",
        "        meta_desc = clean_text(tag[\"content\"])\n",
        "\n",
        "    # H1\n",
        "    h1 = \"\"\n",
        "    h1_tag = soup.find(\"h1\")\n",
        "    if h1_tag:\n",
        "        h1 = clean_text(h1_tag.get_text())\n",
        "\n",
        "    # Social links (common)\n",
        "    socials = {\"linkedin\": \"\", \"twitter_x\": \"\", \"youtube\": \"\", \"facebook\": \"\", \"instagram\": \"\"}\n",
        "    for a in soup.find_all(\"a\", href=True):\n",
        "        href = a[\"href\"].strip()\n",
        "        if \"linkedin.com\" in href and not socials[\"linkedin\"]:\n",
        "            socials[\"linkedin\"] = href\n",
        "        if (\"twitter.com\" in href or \"x.com\" in href) and not socials[\"twitter_x\"]:\n",
        "            socials[\"twitter_x\"] = href\n",
        "        if \"youtube.com\" in href and not socials[\"youtube\"]:\n",
        "            socials[\"youtube\"] = href\n",
        "        if \"facebook.com\" in href and not socials[\"facebook\"]:\n",
        "            socials[\"facebook\"] = href\n",
        "        if \"instagram.com\" in href and not socials[\"instagram\"]:\n",
        "            socials[\"instagram\"] = href\n",
        "\n",
        "    # Find contact/about/team page candidates (just links, not crawling yet)\n",
        "    link_candidates = {\"contact_url\": \"\", \"about_url\": \"\", \"team_url\": \"\"}\n",
        "    for a in soup.find_all(\"a\", href=True):\n",
        "        text = (a.get_text() or \"\").lower().strip()\n",
        "        href = a[\"href\"].strip()\n",
        "\n",
        "        # Make absolute if relative\n",
        "        abs_url = urljoin(base_url, href)\n",
        "\n",
        "        if not link_candidates[\"contact_url\"] and (\"contact\" in text or \"contact\" in href.lower()):\n",
        "            link_candidates[\"contact_url\"] = abs_url\n",
        "        if not link_candidates[\"about_url\"] and (\"about\" in text or \"about\" in href.lower() or \"who we are\" in text):\n",
        "            link_candidates[\"about_url\"] = abs_url\n",
        "        if not link_candidates[\"team_url\"] and (\n",
        "            \"team\" in text or \"our team\" in text or \"leadership\" in text\n",
        "            or \"team\" in href.lower() or \"leadership\" in href.lower()\n",
        "        ):\n",
        "            link_candidates[\"team_url\"] = abs_url\n",
        "\n",
        "    # Emails found on page\n",
        "    emails = sorted(set(EMAIL_RE.findall(soup.get_text(\" \"))))\n",
        "    emails = emails[:5]  # keep short\n",
        "\n",
        "    # A short text snippet (useful for later tagging/classification)\n",
        "    # Keep it lightweight: take first N chars from visible text\n",
        "    page_text = clean_text(soup.get_text(\" \"))\n",
        "    snippet = page_text[:600]\n",
        "\n",
        "    return {\n",
        "        \"site_title\": title,\n",
        "        \"meta_description\": meta_desc,\n",
        "        \"h1\": h1,\n",
        "        \"text_snippet\": snippet,\n",
        "        \"emails_found\": \"; \".join(emails),\n",
        "        \"contact_url_guess\": link_candidates[\"contact_url\"],\n",
        "        \"about_url_guess\": link_candidates[\"about_url\"],\n",
        "        \"team_url_guess\": link_candidates[\"team_url\"],\n",
        "        \"linkedin_url\": socials[\"linkedin\"],\n",
        "        \"twitter_x_url\": socials[\"twitter_x\"],\n",
        "        \"youtube_url\": socials[\"youtube\"],\n",
        "        \"facebook_url\": socials[\"facebook\"],\n",
        "        \"instagram_url\": socials[\"instagram\"],\n",
        "    }\n",
        "\n",
        "# --- 5) Main loop: scrape each row (rate-limited) ---\n",
        "RATE_LIMIT_SECONDS = 1.0  # be polite; tune later\n",
        "\n",
        "enriched_rows = []\n",
        "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
        "    org = row.get(\"Org Name\", \"\")\n",
        "    raw_url = row.get(\"Website URL\", \"\")\n",
        "    url = normalize_url(raw_url)\n",
        "\n",
        "    out = {\n",
        "        \"Org Name\": org,\n",
        "        \"Website URL\": raw_url,\n",
        "        \"website_normalized\": url,\n",
        "        \"website_domain\": get_domain(url),\n",
        "        \"final_url\": \"\",\n",
        "        \"http_ok\": False,\n",
        "        \"scrape_error\": \"\",\n",
        "    }\n",
        "\n",
        "    if not url:\n",
        "        out[\"scrape_error\"] = \"Missing URL\"\n",
        "        enriched_rows.append(out)\n",
        "        continue\n",
        "\n",
        "    final_url, html = safe_get(url)\n",
        "    if not final_url:\n",
        "        out[\"scrape_error\"] = \"Request failed\"\n",
        "        enriched_rows.append(out)\n",
        "        continue\n",
        "\n",
        "    out[\"final_url\"] = final_url\n",
        "\n",
        "    if not html:\n",
        "        out[\"scrape_error\"] = \"Non-HTML response or empty HTML\"\n",
        "        enriched_rows.append(out)\n",
        "        time.sleep(RATE_LIMIT_SECONDS)\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        signals = extract_page_signals(final_url, html)\n",
        "        out.update(signals)\n",
        "        out[\"http_ok\"] = True\n",
        "    except Exception as e:\n",
        "        out[\"scrape_error\"] = f\"Parse error: {e}\"\n",
        "\n",
        "    enriched_rows.append(out)\n",
        "    time.sleep(RATE_LIMIT_SECONDS)\n",
        "\n",
        "enriched_df = pd.DataFrame(enriched_rows)\n",
        "\n",
        "# --- 6) Merge back to your original DB (keep your existing columns unchanged) ---\n",
        "# This keeps all your current fields, and appends new scraped fields.\n",
        "final_df = df.merge(\n",
        "    enriched_df,\n",
        "    on=[\"Org Name\", \"Website URL\"],\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "final_df.head()\n",
        "\n",
        "# --- 7) Save outputs ---\n",
        "OUTPUT_CSV = \"Organization_Database_enriched_v0_1.csv\"\n",
        "final_df.to_csv(OUTPUT_CSV, index=False)\n",
        "\n",
        "print(\"Saved:\", OUTPUT_CSV)\n",
        "\n",
        "# Optional: also save a quick QA file for failures\n",
        "failures = final_df[final_df[\"http_ok\"] != True][[\"Org Name\", \"Website URL\", \"scrape_error\"]]\n",
        "failures.to_csv(\"scrape_failures.csv\", index=False)\n",
        "print(\"Failures saved: scrape_failures.csv\")\n",
        "print(\"Failure count:\", len(failures))"
      ]
    }
  ]
}