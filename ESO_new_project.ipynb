{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ataucuriaia/ESO-new-project/blob/main/ESO_new_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4cjw3fHznrp",
        "outputId": "e26aa617-c9d1-43af-bf85-ae27205331f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ All required packages imported successfully\n",
            "Loaded CSV: 492 rows, 14 columns\n",
            "Columns: ['Org Name', 'Organization Type', 'HQ Related Organization', 'Org Contacts', 'Long Name / Description', 'Website URL', 'Project Name', 'Full Name', 'Sub / HQ', 'Resources', 'Industry ', 'Track Record', 'Charlottesville?', 'Virginia?']\n",
            "✓ Required columns present\n",
            "\n",
            "============================================================\n",
            "HEALTH CHECK (Original Database)\n",
            "============================================================\n",
            "\n",
            "1. DataFrame Shape: 492 rows × 14 columns\n",
            "\n",
            "2. Required Columns Check:\n",
            "   ✓ 'Org Name': Present\n",
            "   ✓ 'Website URL': Present\n",
            "\n",
            "3. URL Validation:\n",
            "   Total rows: 492\n",
            "   Missing/empty URLs: 5 (1.0%)\n",
            "   Valid URLs: 487 (99.0%)\n",
            "\n",
            "4. Sample Normalized URLs (first 10 non-empty):\n",
            "    1. lunalabs.us                                        → https://lunalabs.us\n",
            "    2. https://www.virginiacatalyst.org/                  → https://www.virginiacatalyst.org/\n",
            "    3. https://www.nsf.gov                                → https://www.nsf.gov\n",
            "    4. https://cvillebiohub.org/accelerator/              → https://cvillebiohub.org/accelerator/\n",
            "    5. https://www.titletowntech.com                      → https://www.titletowntech.com\n",
            "    6. https://vipc.org/funding/universities/             → https://vipc.org/funding/universities/\n",
            "    7. https://lvg.virginia.edu                           → https://lvg.virginia.edu\n",
            "    8. https://ithriv.org/pilot                           → https://ithriv.org/pilot\n",
            "    9. https://www.msufoundation.org/                     → https://www.msufoundation.org/\n",
            "   10. https://redcedarventures.com                       → https://redcedarventures.com\n",
            "\n",
            "============================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "============================================================\n",
            "SEARCH API CONFIGURATION CHECK\n",
            "============================================================\n",
            "============================================================\n",
            "GOOGLE_CSE_API_KEY: ✗ Not set (N/A)\n",
            "GOOGLE_CSE_ENGINE_ID: ✗ Not set\n",
            "SERPAPI_KEY: ✗ Not set (N/A)\n",
            "BING_SEARCH_KEY: ✗ Not set (N/A)\n",
            "\n",
            "SEARCH_PROVIDER explicitly set to: serpapi\n",
            "  ⚠ WARNING: SEARCH_PROVIDER=serpapi but SERPAPI_KEY is not set!\n",
            "  Please set SERPAPI_KEY environment variable or restart kernel after setting it.\n",
            "============================================================\n",
            "\n",
            "✓ Using SerpAPI key from notebook (last 4 chars: ...f2dc)\n",
            "✓ Auto-discovery enabled: SerpAPI (key set in notebook)\n",
            "\n",
            "============================================================\n",
            "============================================================\n",
            "PHASE 1: ORGANIZATION EXPANSION\n",
            "============================================================\n",
            "============================================================\n",
            "\n",
            "Original organizations: 492\n",
            "Starting auto-discovery process...\n",
            "\n",
            "Loaded backlog: 157 candidates queued\n",
            "\n",
            "Auto-discovery enabled (Provider: serpapi)\n",
            "\n",
            "  Searching for: Regulatory / FDA\n",
            "    Executing search queries... (this may take 10-30 seconds)\n",
            "      Query 1/5: Found 10 results\n",
            "      Query 2/5: Found 10 results\n",
            "      Query 3/5: Found 9 results\n",
            "      Query 4/5: Found 10 results\n",
            "      Query 5/5: Found 9 results\n",
            "    ✓ Search complete: Found 35 candidates from search\n",
            "    ✓ 16 high-confidence candidates added to processing queue\n",
            "\n",
            "  Searching for: Clinical & Translational Support\n",
            "    Executing search queries... (this may take 10-30 seconds)\n",
            "      Query 1/5: Found 10 results\n",
            "      Query 2/5: Found 10 results\n",
            "      Query 3/5: Found 10 results\n",
            "      Query 4/5: Found 10 results\n",
            "      Query 5/5: Found 10 results\n",
            "    ✓ Search complete: Found 33 candidates from search\n",
            "    ✓ 22 high-confidence candidates added to processing queue\n",
            "\n",
            "  Searching for: IP / Legal / Licensing\n",
            "    Executing search queries... (this may take 10-30 seconds)\n",
            "      Query 1/5: Found 10 results\n",
            "      Query 2/5: Found 10 results\n",
            "      Query 3/5: Found 10 results\n",
            "      Query 4/5: Found 10 results\n",
            "      Query 5/5: Found 9 results\n",
            "    ✓ Search complete: Found 28 candidates from search\n",
            "    ✓ 5 high-confidence candidates added to processing queue\n",
            "\n",
            "  Searching for: Manufacturing / GMP / Scale-Up\n",
            "    Executing search queries... (this may take 10-30 seconds)\n",
            "      Query 1/5: Found 10 results\n",
            "      Query 2/5: Found 9 results\n",
            "      Query 3/5: Found 10 results\n",
            "      Query 4/5: Found 10 results\n",
            "      Query 5/5: Found 10 results\n",
            "    ✓ Search complete: Found 28 candidates from search\n",
            "    ✓ 7 high-confidence candidates added to processing queue\n",
            "\n",
            "============================================================\n",
            "PROCESSING 50 TOTAL CANDIDATES\n",
            "============================================================\n",
            "Prioritizing and filtering candidates...\n",
            "  Regulatory / FDA: 16 candidates selected (max 25 per capability)\n",
            "  Clinical & Translational Support: 22 candidates selected (max 25 per capability)\n",
            "  IP / Legal / Licensing: 5 candidates selected (max 25 per capability)\n",
            "  Manufacturing / GMP / Scale-Up: 7 candidates selected (max 25 per capability)\n",
            "\n",
            "Deduplicating against existing database...\n",
            "\n",
            "✓ Deduplication complete:\n",
            "  Candidates processed: 50\n",
            "  New orgs to add: 47\n",
            "  Duplicates skipped: 3\n",
            "\n",
            "============================================================\n",
            "✓ EXPANSION COMPLETE: Unified org database created\n",
            "============================================================\n",
            "  Original orgs: 492\n",
            "  New orgs added: 47\n",
            "  Total orgs to scrape: 539\n",
            "============================================================\n",
            "\n",
            "✓ Saved backlog: expansion_backlog.csv (181 total candidates)\n",
            "✓ Saved deduplication log: dedupe_log.csv (3 rows)\n",
            "✓ Saved discovery log: expansion_discovery_log.csv\n",
            "\n",
            "Discovery Summary:\n",
            "  Regulatory / FDA: 35 results, 16 high-confidence, 16 added, 19 queued\n",
            "  Clinical & Translational Support: 33 results, 22 high-confidence, 20 added, 11 queued\n",
            "  IP / Legal / Licensing: 28 results, 5 high-confidence, 4 added, 23 queued\n",
            "  Manufacturing / GMP / Scale-Up: 28 results, 7 high-confidence, 7 added, 21 queued\n",
            "\n",
            "============================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "PHASE 2: HOMEPAGE SCRAPING\n",
            "============================================================\n",
            "Starting to scrape 539 organizations...\n",
            "(This includes 492 original + 47 newly discovered orgs)\n",
            "============================================================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Scraping websites: 100%|██████████| 539/539 [18:15<00:00,  2.03s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "ENRICHING REQUIRED FIELDS FOR NEW ORGS\n",
            "============================================================\n",
            "Found 47 new orgs to enrich\n",
            "✓ Enriched 47 new org rows\n",
            "\n",
            "------------------------------------------------------------\n",
            "COMPLETENESS REPORT (NEW ORGS ONLY)\n",
            "------------------------------------------------------------\n",
            "\n",
            "1. Placeholder Usage (% of new orgs):\n",
            "   Track Record                  :   2.1% (Not publicly stated: 1)\n",
            "   Charlottesville?              :   2.1% (Unknown: 1)\n",
            "   Virginia?                     :   2.1% (Unknown: 1)\n",
            "\n",
            "2. Top QA Notes (reasons for placeholders):\n",
            "   no location found on site                         : 2 (4.3%)\n",
            "   no metrics published                              : 1 (2.1%)\n",
            "\n",
            "3. Summary:\n",
            "   New orgs with missing fields: 1 (2.1%)\n",
            "   New orgs fully populated: 46 (97.9%)\n",
            "\n",
            "============================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "PHASE 1: IDENTIFYING ORGANIZATIONAL CAPABILITIES\n",
            "============================================================\n",
            "Note: Org types (Accelerator, Funder, etc.) are NOT modified - they already exist in the database\n",
            "This module identifies CAPABILITIES only (Regulatory/FDA, Clinical, IP/Legal, Manufacturing)\n",
            "✓ Capability identification applied to 539 organizations\n",
            "\n",
            "------------------------------------------------------------\n",
            "CAPABILITY IDENTIFICATION EVALUATION\n",
            "------------------------------------------------------------\n",
            "\n",
            "1. Capability Distribution:\n",
            "   IP / Legal / Licensing                  :  332 orgs (61.60%)\n",
            "   Regulatory / FDA                        :  267 orgs (49.54%)\n",
            "   Clinical & Translational Support        :   70 orgs (12.99%)\n",
            "   Manufacturing / GMP / Scale-Up          :   24 orgs ( 4.45%)\n",
            "\n",
            "   Organizations with capabilities: 397 (73.7%)\n",
            "   Organizations without capabilities: 142 (26.3%)\n",
            "\n",
            "2. Random Examples (20 organizations):\n",
            "\n",
            "   Org: Activate VA\n",
            "   Capabilities: Regulatory / FDA\n",
            "   Keywords: ide\n",
            "   Confidence: 0.20\n",
            "\n",
            "   Org: BlackRock\n",
            "   Capabilities: Regulatory / FDA; IP / Legal / Licensing\n",
            "   Keywords: ide; ip; ind\n",
            "   Confidence: 0.80\n",
            "\n",
            "   Org: Verizon - Basking Ridge, NJ (Major \n",
            "   Capabilities: IP / Legal / Licensing\n",
            "   Keywords: ip\n",
            "   Confidence: 0.80\n",
            "\n",
            "   Org: Edison Partners\n",
            "   Capabilities: Regulatory / FDA; IP / Legal / Licensing\n",
            "   Keywords: ide; ip\n",
            "   Confidence: 0.20\n",
            "\n",
            "   Org: Small Business Innovation Research \n",
            "   Capabilities: IP / Legal / Licensing\n",
            "   Keywords: ip\n",
            "   Confidence: 0.20\n",
            "\n",
            "   Org: Sequoia Capital\n",
            "   Capabilities: Regulatory / FDA; IP / Legal / Licensing\n",
            "   Keywords: ide; ip\n",
            "   Confidence: 0.40\n",
            "\n",
            "   Org: Translational Research Commercializ\n",
            "   Capabilities: Clinical & Translational Support; IP / Legal / Lic\n",
            "   Keywords: translational research; ip; translational\n",
            "   Confidence: 1.00\n",
            "\n",
            "   Org: Mid-Atlantic Technology and Researc\n",
            "   Capabilities: IP / Legal / Licensing\n",
            "   Keywords: ip\n",
            "   Confidence: 0.20\n",
            "\n",
            "   Org: Blu Venture Investors\n",
            "   Capabilities: (none)\n",
            "   Confidence: 0.00\n",
            "\n",
            "   Org: Route 66 Ventures, LLC\n",
            "   Capabilities: Regulatory / FDA\n",
            "   Keywords: ide\n",
            "   Confidence: 0.80\n",
            "\n",
            "   Org: LVG\n",
            "   Capabilities: IP / Legal / Licensing; Regulatory / FDA\n",
            "   Keywords: ide; licensing; technology transfer\n",
            "   Confidence: 0.80\n",
            "\n",
            "   Org: UVA Student Council\n",
            "   Capabilities: Regulatory / FDA; IP / Legal / Licensing\n",
            "   Keywords: ide; ip; legal\n",
            "   Confidence: 0.40\n",
            "\n",
            "   Org: City of Fredericksburg Department o\n",
            "   Capabilities: IP / Legal / Licensing; Regulatory / FDA\n",
            "   Keywords: ip; ind\n",
            "   Confidence: 0.40\n",
            "\n",
            "   Org: Lightspeed Venture Partners\n",
            "   Capabilities: IP / Legal / Licensing; Regulatory / FDA\n",
            "   Keywords: ip; ind\n",
            "   Confidence: 0.40\n",
            "\n",
            "   Org: Virginia Department of Education \n",
            "   Capabilities: (none)\n",
            "   Confidence: 0.00\n",
            "\n",
            "   Org: CTSA Trial Innovation Network\n",
            "   Capabilities: Clinical & Translational Support; Regulatory / FDA\n",
            "   Keywords: ide; clinical trial; ip; translational\n",
            "   Confidence: 0.40\n",
            "\n",
            "   Org: NASA Langley Research Center (Hampt\n",
            "   Capabilities: Regulatory / FDA\n",
            "   Keywords: ide\n",
            "   Confidence: 0.40\n",
            "\n",
            "   Org: The Jefferson Trust\n",
            "   Capabilities: Regulatory / FDA; Clinical & Translational Support\n",
            "   Keywords: ide; ip; cro\n",
            "   Confidence: 0.60\n",
            "\n",
            "   Org: City of Winchester Economic Develop\n",
            "   Capabilities: (none)\n",
            "   Confidence: 0.00\n",
            "\n",
            "   Org: Galant Challenge\n",
            "   Capabilities: IP / Legal / Licensing\n",
            "   Keywords: ip\n",
            "   Confidence: 0.60\n",
            "\n",
            "============================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "PHASE 2: EXTRACTING PEOPLE FROM SUPPORT PAGES\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting people: 100%|██████████| 539/539 [20:01<00:00,  2.23s/it] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✓ Extracted 6241 people entries before deduplication\n",
            "✓ After deduplication: 4856 unique people across 370 organizations\n",
            "\n",
            "Sample formatted people (first 3 orgs):\n",
            "\n",
            "  1. Luna Labs:\n",
            "     More Articles (Title: — | Domains: Clinical Research / Trials; AI / Data Science; Biotech / Pharma | Email: — | Source: https://lunalabs.us/latest-news-acuity-december-2025/), Newsletter Signup (Title...\n",
            "\n",
            "  2. Virginia Catalyst:\n",
            "     Michael Grisham​President & CEO (Title: — | Domains: — | Email: — | Source: https://www.virginiacatalyst.org/our-team.html), Frequently Asked Questions (Title: About the Virginia Catalyst Grant Fundin...\n",
            "\n",
            "  3. National Science Foundation:\n",
            "     NSF announces organizational realignment (Title: Read the Dec. 15, 2025 announcement. | Domains: Clinical Research / Trials; Education / EdTech | Email: — | Source: https://www.nsf.gov/about), About N...\n",
            "\n",
            "============================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "VALIDATION: PHASE 1 & PHASE 2 CORRECTIONS\n",
            "============================================================\n",
            "\n",
            "1. ORGANIZATIONAL CAPABILITIES DISTRIBUTION:\n",
            "   Capability counts:\n",
            "     IP / Legal / Licensing                  :  332 (61.60%)\n",
            "     Regulatory / FDA                        :  267 (49.54%)\n",
            "     Clinical & Translational Support        :   70 (12.99%)\n",
            "     Manufacturing / GMP / Scale-Up          :   24 ( 4.45%)\n",
            "\n",
            "   ✓ All capabilities are in allowed set: {'IP / Legal / Licensing', 'Regulatory / FDA', 'Clinical & Translational Support', 'Manufacturing / GMP / Scale-Up'}\n",
            "   ✓ No forbidden keywords (funding, SBIR, customer discovery, prototyping) in capabilities\n",
            "\n",
            "============================================================\n",
            "\n",
            "Merged dataframe: 539 rows × 48 columns\n",
            "\n",
            "============================================================\n",
            "VALIDATION: PEOPLE INTEGRATION\n",
            "============================================================\n",
            "\n",
            "2. PEOPLE INTEGRATION:\n",
            "   Organizations with people extracted: 373 (69.2%)\n",
            "   Organizations without people: 166 (30.8%)\n",
            "   Total people extracted: 4856\n",
            "\n",
            "   Sample formatted people entries:\n",
            "     Luna Labs: More Articles (Title: — | Domains: Clinical Research / Trials; AI / Data Science; Biotech / Pharma | Email: — | Source: https://lunalabs.us/latest-new...\n",
            "     Virginia Catalyst: Michael Grisham​President & CEO (Title: — | Domains: — | Email: — | Source: https://www.virginiacatalyst.org/our-team.html), Frequently Asked Question...\n",
            "     National Science Foundation: NSF announces organizational realignment (Title: Read the Dec. 15, 2025 announcement. | Domains: Clinical Research / Trials; Education / EdTech | Emai...\n",
            "\n",
            "3. EXPLICIT CONFIRMATIONS:\n",
            "   ✓ No funding-related categories in org capabilities\n",
            "   ✓ No customer discovery categories in org capabilities\n",
            "   ✓ No prototyping/product development categories in org capabilities\n",
            "   ✓ People expertise domains ≠ org capabilities (conceptual separation)\n",
            "   ✓ People may have NO expertise tags (acceptable)\n",
            "   ✓ Max 3 expertise domains per person\n",
            "   ✓ Org types (Accelerator, Funder, etc.) NOT modified by this pipeline\n",
            "\n",
            "============================================================\n",
            "\n",
            "Original columns preserved: 14\n",
            "New columns added: 34\n",
            "\n",
            "============================================================\n",
            "DIAGNOSTICS REPORT\n",
            "============================================================\n",
            "\n",
            "1. Overall Statistics:\n",
            "   Total rows processed: 539\n",
            "   Successful scrapes: 455 (84.4%)\n",
            "   Failed scrapes: 84 (15.6%)\n",
            "\n",
            "2. Top 5 Scrape Error Reasons:\n",
            "   1. HTTP 403                                                     (39 occurrences, 46.4% of failures)\n",
            "   2. HTTP 404                                                     (16 occurrences, 19.0% of failures)\n",
            "   3. Missing URL                                                  (5 occurrences, 6.0% of failures)\n",
            "   4. HTTP 307                                                     (2 occurrences, 2.4% of failures)\n",
            "   5. Connection error: HTTPSConnectionPool(host='www.coulterfound (1 occurrences, 1.2% of failures)\n",
            "\n",
            "3. Sample Failed Organizations:\n",
            "   • Commonwealth Bio Accelerator             | https://cvillebiohub.org/accelerator/    | HTTP 307\n",
            "   • iTHRIV Pilot Translational and Clinical  | https://ithriv.org/pilot                 | HTTP 404\n",
            "   • Ivy Biomedical Innovation Fund           | https://research.virginia.edu/initiative | HTTP 403\n",
            "   • UVA Strategic Investment Fund            | https://sif.virginia.edu/                | HTTP 403\n",
            "   • W.H. Coulter Foundation                  | https://www.coulterfound.org             | Connection error: HTTPSConnectionPool(host='www.co\n",
            "   • Belarusian Republican Foundation for Fun | nan                                      | Missing URL\n",
            "   • CvilleBioHub                             | https://www.cvillebiohub.org             | HTTP 307\n",
            "   • Minority Business Alliance               | https://www.cvillechamber.com/mba/       | HTTP 404\n",
            "   • Luminosity Wearables                     | nan                                      | Missing URL\n",
            "   • University of Virginia's 40Core Program  | https://engineering.virginia.edu/about-o | HTTP 403\n",
            "\n",
            "============================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "SAVING MASTER OUTPUT\n",
            "============================================================\n"
          ]
        },
        {
          "ename": "Error",
          "evalue": "need to escape, but no escapechar set",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mError\u001b[0m                                     Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 2348\u001b[0m\n\u001b[0;32m   2346\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcsv\u001b[39;00m\n\u001b[0;32m   2347\u001b[0m \u001b[38;5;66;03m# QUOTE_ALL ensures all fields are quoted, avoiding escape issues\u001b[39;00m\n\u001b[1;32m-> 2348\u001b[0m \u001b[43mfinal_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMASTER_CSV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8-sig\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mQUOTE_ALL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   2350\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m✓ Saved MASTER database: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMASTER_CSV\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2351\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Total rows: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(final_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\alons\\anaconda3\\envs\\eso_env\\lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\alons\\anaconda3\\envs\\eso_env\\lib\\site-packages\\pandas\\core\\generic.py:3989\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3978\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3980\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3981\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3982\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3986\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3987\u001b[0m )\n\u001b[1;32m-> 3989\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3990\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3991\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3992\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3993\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3994\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3995\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3996\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3997\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3998\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3999\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4000\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4001\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4002\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4003\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4004\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4005\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4006\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\alons\\anaconda3\\envs\\eso_env\\lib\\site-packages\\pandas\\io\\formats\\format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1013\u001b[0m )\n\u001b[1;32m-> 1014\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
            "File \u001b[1;32mc:\\Users\\alons\\anaconda3\\envs\\eso_env\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:270\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath_or_buffer,\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    268\u001b[0m     )\n\u001b[1;32m--> 270\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\alons\\anaconda3\\envs\\eso_env\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:275\u001b[0m, in \u001b[0;36mCSVFormatter._save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_need_to_save_header:\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_header()\n\u001b[1;32m--> 275\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\alons\\anaconda3\\envs\\eso_env\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:313\u001b[0m, in \u001b[0;36mCSVFormatter._save_body\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m start_i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m end_i:\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 313\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_i\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\alons\\anaconda3\\envs\\eso_env\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:324\u001b[0m, in \u001b[0;36mCSVFormatter._save_chunk\u001b[1;34m(self, start_i, end_i)\u001b[0m\n\u001b[0;32m    321\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(res\u001b[38;5;241m.\u001b[39m_iter_column_arrays())\n\u001b[0;32m    323\u001b[0m ix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_index[slicer]\u001b[38;5;241m.\u001b[39m_get_values_for_csv(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_number_format)\n\u001b[1;32m--> 324\u001b[0m \u001b[43mlibwriters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_csv_rows\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mpandas/_libs/writers.pyx:73\u001b[0m, in \u001b[0;36mpandas._libs.writers.write_csv_rows\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;31mError\u001b[0m: need to escape, but no escapechar set"
          ]
        }
      ],
      "source": [
        "# ================================\n",
        "# ESO Web Scraping + Enrichment Pipeline (v2.0)\n",
        "# Enterprise Studio — Complete Pipeline with Expansion & People Integration\n",
        "# \n",
        "# PHASES:\n",
        "#   0) Setup & Data Load + Health Check\n",
        "#   1) Phase 1: Org Expansion (discover new orgs from directories)\n",
        "#   2) URL Normalization & Homepage Scraping (all orgs)\n",
        "#   3) Homepage Signals Extraction\n",
        "#   4) Org Capabilities Identification\n",
        "#   5) Phase 2: People Extraction (all orgs) + Integration into org table\n",
        "#   6) Single Master CSV Export\n",
        "#\n",
        "# INPUTS: CSV with columns \"Org Name\" and \"Website URL\"\n",
        "# OUTPUTS: \n",
        "#   - Organization_Database_MASTER_v1_0.csv (single unified file with people integrated)\n",
        "# ================================\n",
        "\n",
        "# --- 1) Install + imports ---\n",
        "# Handle pip install for both Colab and local environments\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Check and install missing packages\n",
        "required_packages = {\n",
        "    \"pandas\": \"pandas\",\n",
        "    \"requests\": \"requests\",\n",
        "    \"beautifulsoup4\": \"bs4\",\n",
        "    \"lxml\": \"lxml\",\n",
        "    \"tqdm\": \"tqdm\"\n",
        "}\n",
        "\n",
        "missing_packages = []\n",
        "for package_name, import_name in required_packages.items():\n",
        "    try:\n",
        "        __import__(import_name)\n",
        "    except ImportError:\n",
        "        missing_packages.append(package_name)\n",
        "\n",
        "if missing_packages:\n",
        "    print(f\"Installing missing packages: {', '.join(missing_packages)}...\")\n",
        "    try:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + missing_packages)\n",
        "        print(\"✓ Packages installed successfully\")\n",
        "        print(\"⚠ NOTE: If imports still fail, restart the kernel (Kernel → Restart)\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error installing packages: {e}\")\n",
        "        print(f\"Please install manually: pip install {' '.join(missing_packages)}\")\n",
        "        raise\n",
        "\n",
        "# Now import all packages\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"✓ All required packages imported successfully\")\n",
        "\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "import os\n",
        "from urllib.parse import urlparse, urljoin\n",
        "\n",
        "# --- 2) Load your existing database ---\n",
        "INPUT_PATH = \"Organization Database 1f24e34e337d8027b500d2a10b1ceaa7.csv\"\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(INPUT_PATH)\n",
        "except FileNotFoundError:\n",
        "    raise FileNotFoundError(\n",
        "        f\"CSV file not found: {INPUT_PATH}\\n\"\n",
        "        f\"Please ensure the file is in the current directory: {os.getcwd()}\"\n",
        "    )\n",
        "\n",
        "print(f\"Loaded CSV: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
        "print(f\"Columns: {list(df.columns)}\")\n",
        "\n",
        "# Basic checks (matches your structure)\n",
        "REQUIRED_COLS = [\"Org Name\", \"Website URL\"]\n",
        "missing = [c for c in REQUIRED_COLS if c not in df.columns]\n",
        "if missing:\n",
        "    raise ValueError(\n",
        "        f\"Missing required columns: {missing}\\n\"\n",
        "        f\"Found columns: {list(df.columns)}\\n\"\n",
        "        f\"Please ensure your CSV has 'Org Name' and 'Website URL' columns.\"\n",
        "    )\n",
        "\n",
        "print(\"✓ Required columns present\")\n",
        "\n",
        "# --- 3) Helpers: URL cleaning + safe request ---\n",
        "HEADERS = {\n",
        "    \"User-Agent\": (\n",
        "        \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \"\n",
        "        \"(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
        "    )\n",
        "}\n",
        "\n",
        "def normalize_url(url: str) -> str:\n",
        "    \"\"\"Normalize website URL. Adds scheme if missing, strips whitespace.\"\"\"\n",
        "    if not isinstance(url, str) or not url.strip():\n",
        "        return \"\"\n",
        "    u = url.strip()\n",
        "    # Common cleanup\n",
        "    u = u.replace(\" \", \"\")\n",
        "    # If user typed \"www.example.com\" without scheme\n",
        "    if u.startswith(\"www.\"):\n",
        "        u = \"https://\" + u\n",
        "    # If scheme missing but domain present\n",
        "    if not re.match(r\"^https?://\", u) and \".\" in u:\n",
        "        u = \"https://\" + u\n",
        "    return u\n",
        "\n",
        "def get_domain(url: str) -> str:\n",
        "    try:\n",
        "        return urlparse(url).netloc.lower()\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "def safe_get(url: str, timeout=20, max_retries=2, backoff=1.5):\n",
        "    \"\"\"\n",
        "    HTTP GET with retries and improved error handling.\n",
        "    Returns (final_url, html_text, error_msg) tuple.\n",
        "    Handles rate limiting (429), content-type issues, and encoding problems.\n",
        "    \"\"\"\n",
        "    if not url:\n",
        "        return \"\", \"\", \"Empty URL\"\n",
        "    \n",
        "    last_err = None\n",
        "    for attempt in range(max_retries + 1):\n",
        "        try:\n",
        "            r = requests.get(url, headers=HEADERS, timeout=timeout, allow_redirects=True)\n",
        "            \n",
        "            # Handle rate limiting with exponential backoff\n",
        "            if r.status_code == 429:\n",
        "                retry_after = int(r.headers.get(\"Retry-After\", backoff ** attempt))\n",
        "                if attempt < max_retries:\n",
        "                    time.sleep(retry_after)\n",
        "                    continue\n",
        "                return r.url, \"\", f\"HTTP 429 (rate limited) after {max_retries + 1} attempts\"\n",
        "            \n",
        "            # Handle other HTTP errors\n",
        "            if r.status_code in (403, 500, 502, 503, 504):\n",
        "                if attempt < max_retries:\n",
        "                    time.sleep(backoff ** attempt)\n",
        "                    continue\n",
        "                return r.url, \"\", f\"HTTP {r.status_code}\"\n",
        "            \n",
        "            # Only process successful responses (200-299)\n",
        "            if not (200 <= r.status_code < 300):\n",
        "                return r.url, \"\", f\"HTTP {r.status_code}\"\n",
        "            \n",
        "            # Check content type more flexibly\n",
        "            content_type = (r.headers.get(\"Content-Type\") or \"\").lower()\n",
        "            # Some sites don't set Content-Type properly, so check if we got HTML-like content\n",
        "            if content_type and \"text/html\" not in content_type:\n",
        "                # Allow if content-type is missing but content looks like HTML\n",
        "                if not content_type or (\"text\" not in content_type and \"application\" not in content_type):\n",
        "                    # Try to detect HTML by checking first few bytes\n",
        "                    try:\n",
        "                        if r.text[:100].strip().startswith(\"<\"):\n",
        "                            pass  # Looks like HTML, proceed\n",
        "                        else:\n",
        "                            return r.url, \"\", f\"Non-HTML content type: {content_type}\"\n",
        "                    except Exception:\n",
        "                        return r.url, \"\", f\"Non-HTML content type: {content_type}\"\n",
        "            \n",
        "            # Handle encoding issues\n",
        "            try:\n",
        "                r.encoding = r.apparent_encoding if r.apparent_encoding else 'utf-8'\n",
        "                html_text = r.text\n",
        "            except UnicodeDecodeError as e:\n",
        "                return r.url, \"\", f\"Encoding error: {str(e)}\"\n",
        "            \n",
        "            return r.url, html_text, \"\"\n",
        "            \n",
        "        except requests.exceptions.Timeout:\n",
        "            last_err = f\"Timeout after {timeout}s\"\n",
        "            if attempt < max_retries:\n",
        "                time.sleep(backoff ** attempt)\n",
        "                continue\n",
        "        except requests.exceptions.ConnectionError as e:\n",
        "            last_err = f\"Connection error: {str(e)}\"\n",
        "            if attempt < max_retries:\n",
        "                time.sleep(backoff ** attempt)\n",
        "                continue\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            last_err = f\"Request error: {str(e)}\"\n",
        "            if attempt < max_retries:\n",
        "                time.sleep(backoff ** attempt)\n",
        "                continue\n",
        "        except Exception as e:\n",
        "            last_err = f\"Unexpected error: {str(e)}\"\n",
        "            if attempt < max_retries:\n",
        "                time.sleep(backoff ** attempt)\n",
        "                continue\n",
        "    \n",
        "    return \"\", \"\", last_err or \"Unknown error\"\n",
        "\n",
        "# --- 3.5) Health Check (after helpers defined, before expansion) ---\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"HEALTH CHECK (Original Database)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\n1. DataFrame Shape: {df.shape[0]} rows × {df.shape[1]} columns\")\n",
        "\n",
        "print(f\"\\n2. Required Columns Check:\")\n",
        "for col in REQUIRED_COLS:\n",
        "    present = col in df.columns\n",
        "    status = \"✓\" if present else \"✗\"\n",
        "    print(f\"   {status} '{col}': {'Present' if present else 'MISSING'}\")\n",
        "\n",
        "print(f\"\\n3. URL Validation:\")\n",
        "url_col = \"Website URL\"\n",
        "if url_col in df.columns:\n",
        "    total_urls = len(df)\n",
        "    missing_urls = df[url_col].isna().sum() + (df[url_col].astype(str).str.strip() == \"\").sum()\n",
        "    \n",
        "    print(f\"   Total rows: {total_urls}\")\n",
        "    print(f\"   Missing/empty URLs: {missing_urls} ({100*missing_urls/total_urls:.1f}%)\")\n",
        "    print(f\"   Valid URLs: {total_urls - missing_urls} ({100*(total_urls-missing_urls)/total_urls:.1f}%)\")\n",
        "    \n",
        "    # Sample normalized URLs\n",
        "    print(f\"\\n4. Sample Normalized URLs (first 10 non-empty):\")\n",
        "    sample_urls = df[df[url_col].notna() & (df[url_col].astype(str).str.strip() != \"\")][url_col].head(10)\n",
        "    for i, raw_url in enumerate(sample_urls, 1):\n",
        "        normalized = normalize_url(str(raw_url))\n",
        "        print(f\"   {i:2d}. {raw_url[:50]:50s} → {normalized[:60]}\")\n",
        "else:\n",
        "    print(f\"   ✗ '{url_col}' column not found!\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "# --- 3.6) Helper function for text cleaning (needed for expansion) ---\n",
        "def clean_text(s: str) -> str:\n",
        "    \"\"\"Clean and normalize text.\"\"\"\n",
        "    s = re.sub(r\"\\s+\", \" \", s or \"\").strip()\n",
        "    return s\n",
        "\n",
        "# Rate limiting constant (used throughout)\n",
        "RATE_LIMIT_SECONDS = 1.0  # be polite; tune later\n",
        "\n",
        "# ================================\n",
        "# PHASE 1: ORGANIZATION EXPANSION MODULE\n",
        "# Discover and add new organizations from candidate sources\n",
        "# ================================\n",
        "\n",
        "# --- 3.7) Expansion Configuration ---\n",
        "# Search API Configuration\n",
        "# Auto-discovery will be enabled if API keys are found in environment variables\n",
        "# Priority: Google CSE > SerpAPI > Bing\n",
        "# To manually set provider: export SEARCH_PROVIDER=google_cse|serpapi|bing|none\n",
        "\n",
        "# API Keys (read from environment variables only - NEVER hardcode)\n",
        "GOOGLE_CSE_API_KEY = os.getenv(\"GOOGLE_CSE_API_KEY\", \"\")\n",
        "GOOGLE_CSE_ENGINE_ID = os.getenv(\"GOOGLE_CSE_ENGINE_ID\", \"\")\n",
        "SERPAPI_KEY = os.getenv(\"SERPAPI_KEY\", \"\")\n",
        "BING_SEARCH_KEY = os.getenv(\"BING_SEARCH_KEY\", \"\")\n",
        "\n",
        "# Debug: Show what keys are detected (without exposing actual keys)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"=\"*60)\n",
        "print(\"SEARCH API CONFIGURATION CHECK\")\n",
        "print(\"=\"*60)\n",
        "print(\"=\"*60)\n",
        "print(f\"GOOGLE_CSE_API_KEY: {'✓ Set' if GOOGLE_CSE_API_KEY else '✗ Not set'} ({'***' + GOOGLE_CSE_API_KEY[-4:] if GOOGLE_CSE_API_KEY and len(GOOGLE_CSE_API_KEY) > 4 else 'N/A'})\")\n",
        "print(f\"GOOGLE_CSE_ENGINE_ID: {'✓ Set' if GOOGLE_CSE_ENGINE_ID else '✗ Not set'}\")\n",
        "print(f\"SERPAPI_KEY: {'✓ Set' if SERPAPI_KEY else '✗ Not set'} ({'***' + SERPAPI_KEY[-4:] if SERPAPI_KEY and len(SERPAPI_KEY) > 4 else 'N/A'})\")\n",
        "print(f\"BING_SEARCH_KEY: {'✓ Set' if BING_SEARCH_KEY else '✗ Not set'} ({'***' + BING_SEARCH_KEY[-4:] if BING_SEARCH_KEY and len(BING_SEARCH_KEY) > 4 else 'N/A'})\")\n",
        "\n",
        "# Auto-detect provider if not explicitly set\n",
        "SEARCH_PROVIDER_MANUAL = os.getenv(\"SEARCH_PROVIDER\", \"\").lower()\n",
        "if SEARCH_PROVIDER_MANUAL and SEARCH_PROVIDER_MANUAL != \"none\":\n",
        "    SEARCH_PROVIDER = SEARCH_PROVIDER_MANUAL\n",
        "    print(f\"\\nSEARCH_PROVIDER explicitly set to: {SEARCH_PROVIDER}\")\n",
        "    if SEARCH_PROVIDER == \"serpapi\" and not SERPAPI_KEY:\n",
        "        print(\"  ⚠ WARNING: SEARCH_PROVIDER=serpapi but SERPAPI_KEY is not set!\")\n",
        "        print(\"  Please set SERPAPI_KEY environment variable or restart kernel after setting it.\")\n",
        "elif GOOGLE_CSE_API_KEY and GOOGLE_CSE_ENGINE_ID:\n",
        "    SEARCH_PROVIDER = \"google_cse\"\n",
        "    print(\"\\n✓ Auto-discovery enabled: Google Custom Search (keys found in environment)\")\n",
        "elif SERPAPI_KEY:\n",
        "    SEARCH_PROVIDER = \"serpapi\"\n",
        "    print(\"\\n✓ Auto-discovery enabled: SerpAPI (key found in environment)\")\n",
        "elif BING_SEARCH_KEY:\n",
        "    SEARCH_PROVIDER = \"bing\"\n",
        "    print(\"\\n✓ Auto-discovery enabled: Bing Web Search (key found in environment)\")\n",
        "else:\n",
        "    SEARCH_PROVIDER = \"none\"\n",
        "    print(\"\\nℹ Auto-discovery disabled: No API keys found in environment variables\")\n",
        "    print(\"  To enable, set one of:\")\n",
        "    print(\"    - GOOGLE_CSE_API_KEY + GOOGLE_CSE_ENGINE_ID\")\n",
        "    print(\"    - SERPAPI_KEY\")\n",
        "    print(\"    - BING_SEARCH_KEY\")\n",
        "    print(\"\\n  NOTE: If you set environment variables, you may need to:\")\n",
        "    print(\"    1. Restart the Jupyter kernel (Kernel → Restart)\")\n",
        "    print(\"    2. Or set them in this notebook cell before running\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# ============================================\n",
        "# OPTIONAL: Set API Key Directly in Notebook\n",
        "# ============================================\n",
        "# If environment variables aren't working, you can set the key here directly.\n",
        "# WARNING: This will expose your API key in the notebook file.\n",
        "# Only use this for testing, then remove it before committing to git.\n",
        "# ============================================\n",
        "\n",
        "# Uncomment and set your SerpAPI key here if environment variable isn't working:\n",
        "SERPAPI_KEY = \"07c20e0d88e60090f893e86c5b9da5ca0554041bfb237bec28322a6be183f2dc\"\n",
        "if SERPAPI_KEY and SERPAPI_KEY != \"your_serpapi_key_here\":\n",
        "    print(f\"✓ Using SerpAPI key from notebook (last 4 chars: ...{SERPAPI_KEY[-4:]})\")\n",
        "    SEARCH_PROVIDER = \"serpapi\"\n",
        "    # Re-check configuration after setting key\n",
        "    if SERPAPI_KEY:\n",
        "        print(\"✓ Auto-discovery enabled: SerpAPI (key set in notebook)\")\n",
        "\n",
        "# ============================================\n",
        "\n",
        "# Manual candidates (optional - can still provide manually)\n",
        "CANDIDATE_ORGS = []  # Example: [{\"name\": \"Org Name\", \"url\": \"https://example.com\"}, ...]\n",
        "\n",
        "# Seed directory URLs (optional)\n",
        "SEED_DIRECTORY_URLS = []  # Example: [\"https://directory.example.com/orgs\", ...]\n",
        "\n",
        "# Discovery limits\n",
        "MAX_NEW_ORGS_PER_CAPABILITY = 25\n",
        "MAX_TOTAL_NEW_ORGS = 100\n",
        "\n",
        "def normalize_org_name(name: str) -> str:\n",
        "    \"\"\"Normalize org name for deduplication.\"\"\"\n",
        "    if not isinstance(name, str):\n",
        "        return \"\"\n",
        "    # Lowercase, strip, remove common suffixes\n",
        "    normalized = name.lower().strip()\n",
        "    normalized = re.sub(r'\\s+', ' ', normalized)\n",
        "    # Remove common legal suffixes\n",
        "    normalized = re.sub(r'\\b(inc|llc|corp|ltd|foundation|foundation|org|nonprofit)\\b\\.?$', '', normalized)\n",
        "    return normalized.strip()\n",
        "\n",
        "def search_orgs_via_api(capability: str, max_results: int = 20) -> list:\n",
        "    \"\"\"\n",
        "    Search for organizations using configured search API.\n",
        "    Returns list of candidate dicts with: name, url, title, snippet, query, matched_keywords, confidence_score\n",
        "    \"\"\"\n",
        "    candidates = []\n",
        "    \n",
        "    if SEARCH_PROVIDER == \"none\" or not any([GOOGLE_CSE_API_KEY, SERPAPI_KEY, BING_SEARCH_KEY]):\n",
        "        return candidates\n",
        "    \n",
        "    # Generate search queries for this capability\n",
        "    queries = generate_search_queries(capability)\n",
        "    \n",
        "    for i, query in enumerate(queries, 1):\n",
        "        try:\n",
        "            if SEARCH_PROVIDER == \"google_cse\" and GOOGLE_CSE_API_KEY and GOOGLE_CSE_ENGINE_ID:\n",
        "                results = search_google_cse(query, max_results=10)\n",
        "            elif SEARCH_PROVIDER == \"serpapi\" and SERPAPI_KEY:\n",
        "                results = search_serpapi(query, max_results=10)\n",
        "            elif SEARCH_PROVIDER == \"bing\" and BING_SEARCH_KEY:\n",
        "                results = search_bing(query, max_results=10)\n",
        "            else:\n",
        "                continue\n",
        "            \n",
        "            if len(results) > 0:\n",
        "                print(f\"      Query {i}/{len(queries)}: Found {len(results)} results\")\n",
        "            \n",
        "            for result in results:\n",
        "                # Quick classification\n",
        "                confidence, matched_keywords = classify_candidate_confidence(\n",
        "                    result.get(\"title\", \"\"), \n",
        "                    result.get(\"snippet\", \"\"), \n",
        "                    result.get(\"url\", \"\"),\n",
        "                    capability\n",
        "                )\n",
        "                \n",
        "                if confidence > 0.3:  # High-confidence threshold\n",
        "                    candidates.append({\n",
        "                        \"name\": extract_org_name_from_title(result.get(\"title\", \"\")),\n",
        "                        \"url\": result.get(\"url\", \"\"),\n",
        "                        \"title\": result.get(\"title\", \"\"),\n",
        "                        \"snippet\": result.get(\"snippet\", \"\")[:200],\n",
        "                        \"query\": query,\n",
        "                        \"matched_keywords\": \"; \".join(matched_keywords),\n",
        "                        \"confidence_score\": confidence,\n",
        "                        \"capability_bucket\": capability\n",
        "                    })\n",
        "            \n",
        "            time.sleep(0.5)  # Rate limit between queries\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"   Warning: Search error for query '{query}': {e}\")\n",
        "            continue\n",
        "    \n",
        "    return candidates\n",
        "\n",
        "def generate_search_queries(capability: str) -> list:\n",
        "    \"\"\"Generate high-yield search queries for a capability.\"\"\"\n",
        "    queries = []\n",
        "    \n",
        "    if capability == \"Regulatory / FDA\":\n",
        "        queries = [\n",
        "            \"FDA regulatory consulting nonprofit accelerator Virginia\",\n",
        "            \"IND regulatory support organization\",\n",
        "            \"FDA pathway guidance nonprofit\",\n",
        "            \"regulatory affairs consulting startup support\",\n",
        "            \"FDA submission support program\"\n",
        "        ]\n",
        "    elif capability == \"Clinical & Translational Support\":\n",
        "        queries = [\n",
        "            \"clinical translation support program Virginia\",\n",
        "            \"translational research commercialization support\",\n",
        "            \"clinical trials support innovation hub\",\n",
        "            \"CRO support startup accelerator\",\n",
        "            \"clinical development support nonprofit\"\n",
        "        ]\n",
        "    elif capability == \"IP / Legal / Licensing\":\n",
        "        queries = [\n",
        "            \"technology transfer licensing office Virginia\",\n",
        "            \"startup IP legal clinic university\",\n",
        "            \"patent support entrepreneurship nonprofit\",\n",
        "            \"intellectual property support accelerator\",\n",
        "            \"technology licensing support program\"\n",
        "        ]\n",
        "    elif capability == \"Manufacturing / GMP / Scale-Up\":\n",
        "        queries = [\n",
        "            \"GMP manufacturing support program Virginia\",\n",
        "            \"scale-up manufacturing accelerator\",\n",
        "            \"prototype to manufacturing support center\",\n",
        "            \"manufacturing scale-up support nonprofit\",\n",
        "            \"GMP compliance support startup\"\n",
        "        ]\n",
        "    \n",
        "    return queries[:5]  # Limit to 5 queries per capability\n",
        "\n",
        "def classify_candidate_confidence(title: str, snippet: str, url: str, capability: str) -> tuple:\n",
        "    \"\"\"\n",
        "    Quick classification of candidate confidence.\n",
        "    Returns (confidence_score, matched_keywords_list).\n",
        "    \"\"\"\n",
        "    combined_text = f\"{title} {snippet}\".lower()\n",
        "    matched_keywords = []\n",
        "    score = 0.0\n",
        "    \n",
        "    # Capability-specific keywords\n",
        "    capability_keywords = {\n",
        "        \"Regulatory / FDA\": [\"fda\", \"regulatory\", \"regulatory affairs\", \"ind\", \"ide\", \"510k\", \"regulatory consulting\"],\n",
        "        \"Clinical & Translational Support\": [\"clinical\", \"translational\", \"cro\", \"clinical trial\", \"clinical research\", \"translational research\"],\n",
        "        \"IP / Legal / Licensing\": [\"ip\", \"intellectual property\", \"patent\", \"licensing\", \"technology transfer\", \"patent support\"],\n",
        "        \"Manufacturing / GMP / Scale-Up\": [\"manufacturing\", \"gmp\", \"scale-up\", \"scaling\", \"production\", \"manufacturing support\"]\n",
        "    }\n",
        "    \n",
        "    keywords = capability_keywords.get(capability, [])\n",
        "    for keyword in keywords:\n",
        "        if keyword in combined_text:\n",
        "            matched_keywords.append(keyword)\n",
        "            score += 1.0\n",
        "    \n",
        "    # Organization indicators (positive)\n",
        "    org_indicators = [\"organization\", \"nonprofit\", \"foundation\", \"institute\", \"center\", \"program\", \n",
        "                     \"accelerator\", \"hub\", \"network\", \"alliance\", \"association\"]\n",
        "    for indicator in org_indicators:\n",
        "        if indicator in combined_text:\n",
        "            score += 0.5\n",
        "    \n",
        "    # Negative indicators (reduce confidence)\n",
        "    negative_indicators = [\"job\", \"careers\", \"hiring\", \"blog\", \"news\", \"article\", \"pdf\", \"download\"]\n",
        "    for neg in negative_indicators:\n",
        "        if neg in combined_text:\n",
        "            score -= 0.5\n",
        "    \n",
        "    # URL quality check\n",
        "    if url:\n",
        "        url_lower = url.lower()\n",
        "        # Prefer official-looking domains\n",
        "        if any(domain in url_lower for domain in [\".org\", \".edu\", \".gov\", \".com\"]):\n",
        "            if not any(bad in url_lower for bad in [\"/blog/\", \"/news/\", \"/article/\", \"/pdf\", \".pdf\"]):\n",
        "                score += 0.5\n",
        "    \n",
        "    # Normalize confidence (0-1 scale)\n",
        "    confidence = min(1.0, max(0.0, score / 5.0))\n",
        "    \n",
        "    return confidence, matched_keywords\n",
        "\n",
        "def extract_org_name_from_title(title: str) -> str:\n",
        "    \"\"\"Extract organization name from search result title.\"\"\"\n",
        "    if not title:\n",
        "        return \"\"\n",
        "    \n",
        "    # Remove common prefixes/suffixes\n",
        "    title = title.strip()\n",
        "    # Remove \"|\", \"-\", \"—\" and everything after\n",
        "    for sep in [\"|\", \"-\", \"—\", \"::\"]:\n",
        "        if sep in title:\n",
        "            title = title.split(sep)[0].strip()\n",
        "    \n",
        "    # Remove common suffixes\n",
        "    title = re.sub(r'\\s*-\\s*(Home|Official|Website).*$', '', title, flags=re.I)\n",
        "    \n",
        "    return title[:100]  # Limit length\n",
        "\n",
        "def search_google_cse(query: str, max_results: int = 10) -> list:\n",
        "    \"\"\"Search using Google Custom Search JSON API.\"\"\"\n",
        "    if not GOOGLE_CSE_API_KEY or not GOOGLE_CSE_ENGINE_ID:\n",
        "        return []\n",
        "    \n",
        "    try:\n",
        "        url = \"https://www.googleapis.com/customsearch/v1\"\n",
        "        params = {\n",
        "            \"key\": GOOGLE_CSE_API_KEY,\n",
        "            \"cx\": GOOGLE_CSE_ENGINE_ID,\n",
        "            \"q\": query,\n",
        "            \"num\": min(max_results, 10)  # Google CSE max is 10 per request\n",
        "        }\n",
        "        \n",
        "        response = requests.get(url, params=params, timeout=10)\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            results = []\n",
        "            for item in data.get(\"items\", [])[:max_results]:\n",
        "                results.append({\n",
        "                    \"title\": item.get(\"title\", \"\"),\n",
        "                    \"url\": item.get(\"link\", \"\"),\n",
        "                    \"snippet\": item.get(\"snippet\", \"\")\n",
        "                })\n",
        "            return results\n",
        "    except Exception as e:\n",
        "        print(f\"   Google CSE error: {e}\")\n",
        "    \n",
        "    return []\n",
        "\n",
        "def search_serpapi(query: str, max_results: int = 10) -> list:\n",
        "    \"\"\"Search using SerpAPI.\"\"\"\n",
        "    if not SERPAPI_KEY:\n",
        "        return []\n",
        "    \n",
        "    try:\n",
        "        url = \"https://serpapi.com/search\"\n",
        "        params = {\n",
        "            \"api_key\": SERPAPI_KEY,\n",
        "            \"engine\": \"google\",\n",
        "            \"q\": query,\n",
        "            \"num\": min(max_results, 10)\n",
        "        }\n",
        "        \n",
        "        response = requests.get(url, params=params, timeout=15)\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            results = []\n",
        "            organic_results = data.get(\"organic_results\", [])\n",
        "            if not organic_results:\n",
        "                # Check for API errors in response\n",
        "                if \"error\" in data:\n",
        "                    print(f\"      ⚠ SerpAPI error: {data.get('error', 'Unknown error')}\")\n",
        "                return []\n",
        "            \n",
        "            for item in organic_results[:max_results]:\n",
        "                results.append({\n",
        "                    \"title\": item.get(\"title\", \"\"),\n",
        "                    \"url\": item.get(\"link\", \"\"),\n",
        "                    \"snippet\": item.get(\"snippet\", \"\")\n",
        "                })\n",
        "            return results\n",
        "        else:\n",
        "            print(f\"      ⚠ SerpAPI HTTP {response.status_code}: {response.text[:100]}\")\n",
        "            return []\n",
        "    except requests.exceptions.Timeout:\n",
        "        print(f\"      ⚠ SerpAPI timeout for query: {query[:50]}\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(f\"      ⚠ SerpAPI error: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "def search_bing(query: str, max_results: int = 10) -> list:\n",
        "    \"\"\"Search using Bing Web Search API.\"\"\"\n",
        "    if not BING_SEARCH_KEY:\n",
        "        return []\n",
        "    \n",
        "    try:\n",
        "        url = \"https://api.bing.microsoft.com/v7.0/search\"\n",
        "        headers = {\"Ocp-Apim-Subscription-Key\": BING_SEARCH_KEY}\n",
        "        params = {\n",
        "            \"q\": query,\n",
        "            \"count\": min(max_results, 50),\n",
        "            \"responseFilter\": \"Webpages\"\n",
        "        }\n",
        "        \n",
        "        response = requests.get(url, headers=headers, params=params, timeout=10)\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            results = []\n",
        "            for item in data.get(\"webPages\", {}).get(\"value\", [])[:max_results]:\n",
        "                results.append({\n",
        "                    \"title\": item.get(\"name\", \"\"),\n",
        "                    \"url\": item.get(\"url\", \"\"),\n",
        "                    \"snippet\": item.get(\"snippet\", \"\")\n",
        "                })\n",
        "            return results\n",
        "    except Exception as e:\n",
        "        print(f\"   Bing API error: {e}\")\n",
        "    \n",
        "    return []\n",
        "\n",
        "def discover_orgs_from_directory(url: str, max_orgs: int = 50) -> list:\n",
        "    \"\"\"\n",
        "    Discover candidate organizations from a directory page.\n",
        "    Returns list of dicts with name and url.\n",
        "    \"\"\"\n",
        "    discovered = []\n",
        "    try:\n",
        "        final_url, html, error_msg = safe_get(url)\n",
        "        if not html or error_msg:\n",
        "            return discovered\n",
        "        \n",
        "        soup = BeautifulSoup(html, \"lxml\")\n",
        "        \n",
        "        # Look for common patterns: links with org names, list items, etc.\n",
        "        # Pattern 1: Links that look like org names (heuristic)\n",
        "        for a in soup.find_all(\"a\", href=True):\n",
        "            text = clean_text(a.get_text())\n",
        "            href = a.get(\"href\", \"\")\n",
        "            \n",
        "            # Heuristic: org names are usually 2-6 words, start with capital\n",
        "            words = text.split()\n",
        "            if 2 <= len(words) <= 6 and text[0].isupper() and len(text) > 5:\n",
        "                # Make URL absolute\n",
        "                abs_url = urljoin(url, href)\n",
        "                if abs_url.startswith(\"http\"):\n",
        "                    discovered.append({\n",
        "                        \"name\": text,\n",
        "                        \"url\": abs_url\n",
        "                    })\n",
        "                    if len(discovered) >= max_orgs:\n",
        "                        break\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"   Warning: Error discovering orgs from {url}: {e}\")\n",
        "    \n",
        "    return discovered\n",
        "\n",
        "def deduplicate_new_orgs(existing_df: pd.DataFrame, candidate_orgs: list) -> tuple:\n",
        "    \"\"\"\n",
        "    Deduplicate candidate orgs against existing database.\n",
        "    Returns (new_orgs_list, duplicates_log).\n",
        "    \"\"\"\n",
        "    if not candidate_orgs:\n",
        "        return [], []\n",
        "    \n",
        "    # Build lookup sets from existing data\n",
        "    existing_domains = set()\n",
        "    existing_names_normalized = set()\n",
        "    \n",
        "    for idx, row in existing_df.iterrows():\n",
        "        url = str(row.get(\"Website URL\", \"\"))\n",
        "        name = str(row.get(\"Org Name\", \"\"))\n",
        "        \n",
        "        if url:\n",
        "            domain = get_domain(normalize_url(url))\n",
        "            if domain:\n",
        "                existing_domains.add(domain)\n",
        "        \n",
        "        if name:\n",
        "            existing_names_normalized.add(normalize_org_name(name))\n",
        "    \n",
        "    # Check candidates\n",
        "    new_orgs = []\n",
        "    duplicates_log = []\n",
        "    \n",
        "    for candidate in candidate_orgs:\n",
        "        name = str(candidate.get(\"name\", \"\")).strip()\n",
        "        url = str(candidate.get(\"url\", \"\")).strip()\n",
        "        \n",
        "        if not name or not url:\n",
        "            continue\n",
        "        \n",
        "        # Normalize for comparison\n",
        "        normalized_name = normalize_org_name(name)\n",
        "        normalized_url = normalize_url(url)\n",
        "        domain = get_domain(normalized_url)\n",
        "        \n",
        "        # Check for duplicates\n",
        "        is_duplicate = False\n",
        "        reason = \"\"\n",
        "        \n",
        "        if domain and domain in existing_domains:\n",
        "            is_duplicate = True\n",
        "            reason = f\"Domain match: {domain}\"\n",
        "        elif normalized_name and normalized_name in existing_names_normalized:\n",
        "            is_duplicate = True\n",
        "            reason = f\"Name match: {normalized_name}\"\n",
        "        \n",
        "        if is_duplicate:\n",
        "            duplicates_log.append({\"name\": name, \"url\": url, \"reason\": reason})\n",
        "        else:\n",
        "            new_org = {\n",
        "                \"Org Name\": name,\n",
        "                \"Website URL\": normalized_url,\n",
        "                \"Organization Type\": \"\",\n",
        "                \"Long Name / Description\": \"\",\n",
        "                \"Resources\": \"\",\n",
        "                \"Industry \": \"\",\n",
        "                \"Track Record\": \"\",\n",
        "                \"Charlottesville?\": \"\",\n",
        "                \"Virginia?\": \"\"\n",
        "            }\n",
        "            # Preserve capability_bucket if present (for tracking)\n",
        "            if \"capability_bucket\" in candidate:\n",
        "                new_org[\"capability_bucket\"] = candidate[\"capability_bucket\"]\n",
        "            new_orgs.append(new_org)\n",
        "    \n",
        "    return new_orgs, duplicates_log\n",
        "\n",
        "# --- 3.8) Backlog Management ---\n",
        "BACKLOG_FILE = \"expansion_backlog.csv\"\n",
        "\n",
        "def load_backlog() -> pd.DataFrame:\n",
        "    \"\"\"Load existing backlog file or create empty one.\"\"\"\n",
        "    if os.path.exists(BACKLOG_FILE):\n",
        "        try:\n",
        "            return pd.read_csv(BACKLOG_FILE)\n",
        "        except Exception:\n",
        "            pass\n",
        "    \n",
        "    # Create empty backlog\n",
        "    return pd.DataFrame(columns=[\n",
        "        \"discovered_at\", \"capability_bucket\", \"org_name_guess\", \"url\", \"title\", \n",
        "        \"snippet\", \"query\", \"matched_keywords\", \"confidence_score\", \"status\"\n",
        "    ])\n",
        "\n",
        "def save_backlog(backlog_df: pd.DataFrame):\n",
        "    \"\"\"Save backlog to CSV.\"\"\"\n",
        "    backlog_df.to_csv(BACKLOG_FILE, index=False)\n",
        "\n",
        "def add_to_backlog(candidates: list, backlog_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Add new candidates to backlog, deduplicating by URL.\"\"\"\n",
        "    if not candidates:\n",
        "        return backlog_df\n",
        "    \n",
        "    # Get existing URLs from backlog\n",
        "    existing_urls = set(backlog_df[\"url\"].astype(str).str.lower()) if \"url\" in backlog_df.columns else set()\n",
        "    \n",
        "    new_rows = []\n",
        "    for candidate in candidates:\n",
        "        url_lower = candidate.get(\"url\", \"\").lower()\n",
        "        if url_lower and url_lower not in existing_urls:\n",
        "            new_rows.append({\n",
        "                \"discovered_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                \"capability_bucket\": candidate.get(\"capability_bucket\", \"\"),\n",
        "                \"org_name_guess\": candidate.get(\"name\", \"\"),\n",
        "                \"url\": candidate.get(\"url\", \"\"),\n",
        "                \"title\": candidate.get(\"title\", \"\"),\n",
        "                \"snippet\": candidate.get(\"snippet\", \"\"),\n",
        "                \"query\": candidate.get(\"query\", \"\"),\n",
        "                \"matched_keywords\": candidate.get(\"matched_keywords\", \"\"),\n",
        "                \"confidence_score\": candidate.get(\"confidence_score\", 0.0),\n",
        "                \"status\": \"queued\"\n",
        "            })\n",
        "            existing_urls.add(url_lower)\n",
        "    \n",
        "    if new_rows:\n",
        "        new_df = pd.DataFrame(new_rows)\n",
        "        backlog_df = pd.concat([backlog_df, new_df], ignore_index=True)\n",
        "    \n",
        "    return backlog_df\n",
        "\n",
        "# ================================\n",
        "# EXECUTE EXPANSION - THIS MUST RUN BEFORE SCRAPING\n",
        "# ================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"=\"*60)\n",
        "print(\"PHASE 1: ORGANIZATION EXPANSION\")\n",
        "print(\"=\"*60)\n",
        "print(\"=\"*60)\n",
        "\n",
        "original_count = len(df)\n",
        "print(f\"\\nOriginal organizations: {original_count}\")\n",
        "print(\"Starting auto-discovery process...\\n\")\n",
        "\n",
        "# Initialize variables\n",
        "duplicates_log = []\n",
        "all_candidates = []\n",
        "discovery_log = []\n",
        "\n",
        "# Load backlog\n",
        "backlog_df = load_backlog()\n",
        "print(f\"Loaded backlog: {len(backlog_df)} candidates queued\")\n",
        "\n",
        "# Auto-discovery via search API\n",
        "if SEARCH_PROVIDER != \"none\":\n",
        "    print(f\"\\nAuto-discovery enabled (Provider: {SEARCH_PROVIDER})\")\n",
        "    \n",
        "    if SEARCH_PROVIDER == \"google_cse\" and not (GOOGLE_CSE_API_KEY and GOOGLE_CSE_ENGINE_ID):\n",
        "        print(\"  ⚠ Google CSE API key or Engine ID not found in environment variables\")\n",
        "        print(\"  Set GOOGLE_CSE_API_KEY and GOOGLE_CSE_ENGINE_ID to enable\")\n",
        "    elif SEARCH_PROVIDER == \"serpapi\" and not SERPAPI_KEY:\n",
        "        print(\"  ⚠ SerpAPI key not found in environment variable\")\n",
        "        print(\"  Set SERPAPI_KEY to enable\")\n",
        "    elif SEARCH_PROVIDER == \"bing\" and not BING_SEARCH_KEY:\n",
        "        print(\"  ⚠ Bing Search API key not found in environment variable\")\n",
        "        print(\"  Set BING_SEARCH_KEY to enable\")\n",
        "    else:\n",
        "        # Search for each capability\n",
        "        capabilities = [\"Regulatory / FDA\", \"Clinical & Translational Support\", \n",
        "                       \"IP / Legal / Licensing\", \"Manufacturing / GMP / Scale-Up\"]\n",
        "        \n",
        "        for capability in capabilities:\n",
        "            print(f\"\\n  Searching for: {capability}\")\n",
        "            print(f\"    Executing search queries... (this may take 10-30 seconds)\")\n",
        "            search_candidates = search_orgs_via_api(capability, max_results=20)\n",
        "            print(f\"    ✓ Search complete: Found {len(search_candidates)} candidates from search\")\n",
        "            \n",
        "            if len(search_candidates) == 0:\n",
        "                print(f\"    ⚠ No candidates found. Check search queries or API response.\")\n",
        "            \n",
        "            # Add to backlog\n",
        "            backlog_df = add_to_backlog(search_candidates, backlog_df)\n",
        "            \n",
        "            # Add high-confidence candidates to processing queue\n",
        "            high_confidence = [c for c in search_candidates if c.get(\"confidence_score\", 0) > 0.5]\n",
        "            all_candidates.extend(high_confidence)\n",
        "            \n",
        "            if len(high_confidence) > 0:\n",
        "                print(f\"    ✓ {len(high_confidence)} high-confidence candidates added to processing queue\")\n",
        "            \n",
        "            discovery_log.append({\n",
        "                \"capability\": capability,\n",
        "                \"results_pulled\": len(search_candidates),\n",
        "                \"candidates\": len(search_candidates),\n",
        "                \"high_confidence\": len(high_confidence),\n",
        "                \"added\": 0,  # Will update after deduplication\n",
        "                \"queued\": len(search_candidates) - len(high_confidence)\n",
        "            })\n",
        "else:\n",
        "    print(\"\\nAuto-discovery disabled (SEARCH_PROVIDER='none')\")\n",
        "    print(\"  To enable, set environment variable SEARCH_PROVIDER to: google_cse, serpapi, or bing\")\n",
        "    print(\"  And set corresponding API key environment variables\")\n",
        "\n",
        "# Discover from directories\n",
        "if SEED_DIRECTORY_URLS:\n",
        "    print(f\"\\nDiscovering from {len(SEED_DIRECTORY_URLS)} directory URLs...\")\n",
        "    for directory_url in SEED_DIRECTORY_URLS:\n",
        "        print(f\"  Directory: {directory_url}\")\n",
        "        discovered = discover_orgs_from_directory(directory_url)\n",
        "        print(f\"    Found {len(discovered)} candidate orgs\")\n",
        "        all_candidates.extend(discovered)\n",
        "        time.sleep(RATE_LIMIT_SECONDS)\n",
        "\n",
        "# Add manual candidates\n",
        "if CANDIDATE_ORGS:\n",
        "    print(f\"\\nAdding {len(CANDIDATE_ORGS)} manual candidates...\")\n",
        "    all_candidates.extend(CANDIDATE_ORGS)\n",
        "\n",
        "# Deduplicate and prioritize candidates\n",
        "if all_candidates:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"PROCESSING {len(all_candidates)} TOTAL CANDIDATES\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Prioritizing and filtering candidates...\")\n",
        "    \n",
        "    # Prioritize by confidence score\n",
        "    all_candidates_sorted = sorted(all_candidates, key=lambda x: x.get(\"confidence_score\", 0), reverse=True)\n",
        "    \n",
        "    # Group by capability and limit per capability\n",
        "    candidates_by_capability = {}\n",
        "    for candidate in all_candidates_sorted:\n",
        "        capability = candidate.get(\"capability_bucket\", \"unknown\")\n",
        "        if capability not in candidates_by_capability:\n",
        "            candidates_by_capability[capability] = []\n",
        "        if len(candidates_by_capability[capability]) < MAX_NEW_ORGS_PER_CAPABILITY:\n",
        "            candidates_by_capability[capability].append(candidate)\n",
        "    \n",
        "    # Flatten back to list (limited per capability)\n",
        "    prioritized_candidates = []\n",
        "    for capability, candidates in candidates_by_capability.items():\n",
        "        prioritized_candidates.extend(candidates)\n",
        "        print(f\"  {capability}: {len(candidates)} candidates selected (max {MAX_NEW_ORGS_PER_CAPABILITY} per capability)\")\n",
        "        # Update discovery log\n",
        "        for log_entry in discovery_log:\n",
        "            if log_entry.get(\"capability\") == capability:\n",
        "                log_entry[\"high_confidence\"] = len(candidates)\n",
        "    \n",
        "    # Add remaining to backlog\n",
        "    remaining_candidates = all_candidates_sorted[len(prioritized_candidates):]\n",
        "    if remaining_candidates:\n",
        "        backlog_df = add_to_backlog(remaining_candidates, backlog_df)\n",
        "        print(f\"  Added {len(remaining_candidates)} additional candidates to backlog\")\n",
        "    \n",
        "    # Limit total new orgs\n",
        "    if len(prioritized_candidates) > MAX_TOTAL_NEW_ORGS:\n",
        "        prioritized_candidates = prioritized_candidates[:MAX_TOTAL_NEW_ORGS]\n",
        "        print(f\"  Limited to {MAX_TOTAL_NEW_ORGS} total new orgs (max limit)\")\n",
        "    \n",
        "    print(f\"\\nDeduplicating against existing database...\")\n",
        "    # Deduplicate against existing database\n",
        "    new_orgs, duplicates_log = deduplicate_new_orgs(df, prioritized_candidates)\n",
        "    print(f\"\\n✓ Deduplication complete:\")\n",
        "    print(f\"  Candidates processed: {len(prioritized_candidates)}\")\n",
        "    print(f\"  New orgs to add: {len(new_orgs)}\")\n",
        "    print(f\"  Duplicates skipped: {len(duplicates_log)}\")\n",
        "    \n",
        "    # Update backlog status for added orgs\n",
        "    if \"url\" in backlog_df.columns:\n",
        "        added_urls = {org.get(\"Website URL\", \"\").lower() for org in new_orgs}\n",
        "        backlog_df.loc[backlog_df[\"url\"].str.lower().isin(added_urls), \"status\"] = \"added\"\n",
        "        \n",
        "        duplicate_urls = {dup.get(\"url\", \"\").lower() for dup in duplicates_log}\n",
        "        backlog_df.loc[backlog_df[\"url\"].str.lower().isin(duplicate_urls), \"status\"] = \"skipped_duplicate\"\n",
        "    \n",
        "    # Update discovery log with final counts\n",
        "    for log_entry in discovery_log:\n",
        "        capability = log_entry.get(\"capability\")\n",
        "        added_count = sum(1 for org in new_orgs if org.get(\"capability_bucket\") == capability)\n",
        "        log_entry[\"added\"] = added_count\n",
        "    \n",
        "    # Create DataFrame for new orgs\n",
        "    if new_orgs:\n",
        "        df_new = pd.DataFrame(new_orgs)\n",
        "        # Ensure all original columns exist\n",
        "        for col in df.columns:\n",
        "            if col not in df_new.columns:\n",
        "                df_new[col] = \"\"\n",
        "        \n",
        "        # Combine: original + new\n",
        "        df_org_all = pd.concat([df, df_new], ignore_index=True)\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"✓ EXPANSION COMPLETE: Unified org database created\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"  Original orgs: {original_count}\")\n",
        "        print(f\"  New orgs added: {len(new_orgs)}\")\n",
        "        print(f\"  Total orgs to scrape: {len(df_org_all)}\")\n",
        "        print(f\"{'='*60}\\n\")\n",
        "    else:\n",
        "        df_org_all = df.copy()\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"✓ EXPANSION COMPLETE: No new orgs to add\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"  Using original database: {len(df_org_all)} orgs\")\n",
        "        print(f\"{'='*60}\\n\")\n",
        "else:\n",
        "    df_org_all = df.copy()\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"✓ EXPANSION COMPLETE: No candidate orgs provided\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"  Using original database: {len(df_org_all)} orgs\")\n",
        "    print(f\"  (No new orgs discovered - auto-discovery may be disabled or found no candidates)\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "# CRITICAL: Ensure df_org_all is always defined before scraping\n",
        "if 'df_org_all' not in locals():\n",
        "    print(\"⚠ ERROR: df_org_all not defined! This should never happen.\")\n",
        "    df_org_all = df.copy()\n",
        "    original_count = len(df)\n",
        "\n",
        "# Save backlog\n",
        "save_backlog(backlog_df)\n",
        "print(f\"✓ Saved backlog: {BACKLOG_FILE} ({len(backlog_df)} total candidates)\")\n",
        "\n",
        "# Save dedupe log if any duplicates\n",
        "if duplicates_log:\n",
        "    dedupe_df = pd.DataFrame(duplicates_log)\n",
        "    dedupe_df.to_csv(\"dedupe_log.csv\", index=False)\n",
        "    print(f\"✓ Saved deduplication log: dedupe_log.csv ({len(duplicates_log)} rows)\")\n",
        "\n",
        "# Save discovery log\n",
        "if discovery_log:\n",
        "    discovery_df = pd.DataFrame(discovery_log)\n",
        "    discovery_df.to_csv(\"expansion_discovery_log.csv\", index=False)\n",
        "    print(f\"✓ Saved discovery log: expansion_discovery_log.csv\")\n",
        "    \n",
        "    # Print discovery summary\n",
        "    print(f\"\\nDiscovery Summary:\")\n",
        "    for entry in discovery_log:\n",
        "        print(f\"  {entry['capability']}: {entry['results_pulled']} results, \"\n",
        "              f\"{entry['high_confidence']} high-confidence, {entry['added']} added, \"\n",
        "              f\"{entry['queued']} queued\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "# --- 4) HTML parsing: extract useful fields for your ESO DB ---\n",
        "EMAIL_RE = re.compile(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\")\n",
        "\n",
        "# clean_text is already defined above (needed for expansion module)\n",
        "\n",
        "def extract_page_signals(base_url: str, html: str) -> dict:\n",
        "    \"\"\"\n",
        "    Extract lightweight, high-signal fields from home page HTML.\n",
        "    (You can extend this later: team page scraping, keyword tagging, etc.)\n",
        "    Handles parsing errors gracefully.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        soup = BeautifulSoup(html, \"lxml\")\n",
        "    except Exception as e:\n",
        "        # Fallback to html.parser if lxml fails\n",
        "        try:\n",
        "            soup = BeautifulSoup(html, \"html.parser\")\n",
        "        except Exception as e2:\n",
        "            raise ValueError(f\"Failed to parse HTML: {str(e)}; fallback also failed: {str(e2)}\")\n",
        "\n",
        "    # Title - handle missing title gracefully\n",
        "    try:\n",
        "        title = clean_text(soup.title.get_text()) if soup.title else \"\"\n",
        "    except Exception:\n",
        "        title = \"\"\n",
        "\n",
        "    # Meta description\n",
        "    meta_desc = \"\"\n",
        "    tag = soup.find(\"meta\", attrs={\"name\": re.compile(\"^description$\", re.I)})\n",
        "    if tag and tag.get(\"content\"):\n",
        "        meta_desc = clean_text(tag[\"content\"])\n",
        "\n",
        "    # H1\n",
        "    h1 = \"\"\n",
        "    h1_tag = soup.find(\"h1\")\n",
        "    if h1_tag:\n",
        "        h1 = clean_text(h1_tag.get_text())\n",
        "\n",
        "    # Social links (common)\n",
        "    socials = {\"linkedin\": \"\", \"twitter_x\": \"\", \"youtube\": \"\", \"facebook\": \"\", \"instagram\": \"\"}\n",
        "    for a in soup.find_all(\"a\", href=True):\n",
        "        href = a[\"href\"].strip()\n",
        "        if \"linkedin.com\" in href and not socials[\"linkedin\"]:\n",
        "            socials[\"linkedin\"] = href\n",
        "        if (\"twitter.com\" in href or \"x.com\" in href) and not socials[\"twitter_x\"]:\n",
        "            socials[\"twitter_x\"] = href\n",
        "        if \"youtube.com\" in href and not socials[\"youtube\"]:\n",
        "            socials[\"youtube\"] = href\n",
        "        if \"facebook.com\" in href and not socials[\"facebook\"]:\n",
        "            socials[\"facebook\"] = href\n",
        "        if \"instagram.com\" in href and not socials[\"instagram\"]:\n",
        "            socials[\"instagram\"] = href\n",
        "\n",
        "    # Find contact/about/team page candidates (just links, not crawling yet)\n",
        "    link_candidates = {\"contact_url\": \"\", \"about_url\": \"\", \"team_url\": \"\"}\n",
        "    for a in soup.find_all(\"a\", href=True):\n",
        "        text = (a.get_text() or \"\").lower().strip()\n",
        "        href = a[\"href\"].strip()\n",
        "\n",
        "        # Make absolute if relative\n",
        "        abs_url = urljoin(base_url, href)\n",
        "\n",
        "        if not link_candidates[\"contact_url\"] and (\"contact\" in text or \"contact\" in href.lower()):\n",
        "            link_candidates[\"contact_url\"] = abs_url\n",
        "        if not link_candidates[\"about_url\"] and (\"about\" in text or \"about\" in href.lower() or \"who we are\" in text):\n",
        "            link_candidates[\"about_url\"] = abs_url\n",
        "        if not link_candidates[\"team_url\"] and (\n",
        "            \"team\" in text or \"our team\" in text or \"leadership\" in text\n",
        "            or \"team\" in href.lower() or \"leadership\" in href.lower()\n",
        "        ):\n",
        "            link_candidates[\"team_url\"] = abs_url\n",
        "\n",
        "    # Emails found on page\n",
        "    emails = sorted(set(EMAIL_RE.findall(soup.get_text(\" \"))))\n",
        "    emails = emails[:5]  # keep short\n",
        "\n",
        "    # A short text snippet (useful for later tagging/classification)\n",
        "    # Keep it lightweight: take first N chars from visible text\n",
        "    try:\n",
        "        page_text = clean_text(soup.get_text(\" \"))\n",
        "        snippet = page_text[:600]\n",
        "    except Exception:\n",
        "        snippet = \"\"\n",
        "\n",
        "    return {\n",
        "        \"site_title\": title,\n",
        "        \"meta_description\": meta_desc,\n",
        "        \"h1\": h1,\n",
        "        \"text_snippet\": snippet,\n",
        "        \"emails_found\": \"; \".join(emails),\n",
        "        \"contact_url_guess\": link_candidates[\"contact_url\"],\n",
        "        \"about_url_guess\": link_candidates[\"about_url\"],\n",
        "        \"team_url_guess\": link_candidates[\"team_url\"],\n",
        "        \"linkedin_url\": socials[\"linkedin\"],\n",
        "        \"twitter_x_url\": socials[\"twitter_x\"],\n",
        "        \"youtube_url\": socials[\"youtube\"],\n",
        "        \"facebook_url\": socials[\"facebook\"],\n",
        "        \"instagram_url\": socials[\"instagram\"],\n",
        "    }\n",
        "\n",
        "# ================================\n",
        "# PHASE 2: URL NORMALIZATION & HOMEPAGE SCRAPING\n",
        "# Scrape all organizations (existing + newly discovered)\n",
        "# ================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PHASE 2: HOMEPAGE SCRAPING\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Starting to scrape {len(df_org_all)} organizations...\")\n",
        "print(f\"(This includes {original_count} original + {len(df_org_all) - original_count} newly discovered orgs)\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# --- 5) Main loop: scrape each row (rate-limited) ---\n",
        "# RATE_LIMIT_SECONDS is already defined above\n",
        "\n",
        "enriched_rows = []\n",
        "for idx, row in tqdm(df_org_all.iterrows(), total=len(df_org_all), desc=\"Scraping websites\"):\n",
        "    org = row.get(\"Org Name\", \"\")\n",
        "    raw_url = row.get(\"Website URL\", \"\")\n",
        "    url = normalize_url(raw_url)\n",
        "\n",
        "    out = {\n",
        "        \"Org Name\": org,\n",
        "        \"Website URL\": raw_url,\n",
        "        \"website_normalized\": url,\n",
        "        \"website_domain\": get_domain(url),\n",
        "        \"final_url\": \"\",\n",
        "        \"http_ok\": False,\n",
        "        \"scrape_error\": \"\",\n",
        "    }\n",
        "\n",
        "    if not url:\n",
        "        out[\"scrape_error\"] = \"Missing URL\"\n",
        "        enriched_rows.append(out)\n",
        "        continue\n",
        "\n",
        "    final_url, html, error_msg = safe_get(url)\n",
        "    \n",
        "    if error_msg:\n",
        "        out[\"scrape_error\"] = error_msg\n",
        "        enriched_rows.append(out)\n",
        "        time.sleep(RATE_LIMIT_SECONDS)\n",
        "        continue\n",
        "    \n",
        "    if not final_url:\n",
        "        out[\"scrape_error\"] = \"Request failed (no URL returned)\"\n",
        "        enriched_rows.append(out)\n",
        "        time.sleep(RATE_LIMIT_SECONDS)\n",
        "        continue\n",
        "\n",
        "    out[\"final_url\"] = final_url\n",
        "\n",
        "    if not html:\n",
        "        out[\"scrape_error\"] = \"Non-HTML response or empty HTML\"\n",
        "        enriched_rows.append(out)\n",
        "        time.sleep(RATE_LIMIT_SECONDS)\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        signals = extract_page_signals(final_url, html)\n",
        "        out.update(signals)\n",
        "        # Store HTML for later extraction (truncated to save memory)\n",
        "        out[\"homepage_html_stored\"] = html[:50000] if html else \"\"  # Store first 50KB\n",
        "        out[\"http_ok\"] = True\n",
        "    except Exception as e:\n",
        "        out[\"scrape_error\"] = f\"Parse error: {str(e)}\"\n",
        "        out[\"homepage_html_stored\"] = \"\"\n",
        "\n",
        "    enriched_rows.append(out)\n",
        "    time.sleep(RATE_LIMIT_SECONDS)\n",
        "\n",
        "enriched_df = pd.DataFrame(enriched_rows)\n",
        "\n",
        "# ================================\n",
        "# ENRICHMENT: Fill Required Fields for New Orgs (Before Capabilities)\n",
        "# Extract missing required fields and add QA columns\n",
        "# ================================\n",
        "\n",
        "# --- 4.5) Enhanced Field Extraction Functions ---\n",
        "\n",
        "def extract_description(meta_desc: str, h1: str, text_snippet: str, about_html: str = \"\") -> tuple:\n",
        "    \"\"\"Extract description from multiple sources. Returns (description, source_checked).\"\"\"\n",
        "    sources_checked = []\n",
        "    \n",
        "    # Try 1: Meta description\n",
        "    if meta_desc and len(meta_desc) > 20:\n",
        "        return meta_desc[:500], [\"homepage meta\"]\n",
        "    \n",
        "    # Try 2: About page paragraph\n",
        "    if about_html:\n",
        "        try:\n",
        "            soup = BeautifulSoup(about_html, \"html.parser\")\n",
        "            # Look for first substantial paragraph\n",
        "            for p in soup.find_all(\"p\"):\n",
        "                text = clean_text(p.get_text())\n",
        "                if len(text) > 50:\n",
        "                    sources_checked.append(\"about page\")\n",
        "                    return text[:500], sources_checked\n",
        "        except Exception:\n",
        "            pass\n",
        "    \n",
        "    # Try 3: H1 + text snippet\n",
        "    if h1 and text_snippet:\n",
        "        combined = f\"{h1}. {text_snippet[:400]}\"\n",
        "        if len(combined) > 50:\n",
        "            sources_checked.append(\"homepage h1+snippet\")\n",
        "            return combined[:500], sources_checked\n",
        "    \n",
        "    return \"Unknown\", sources_checked\n",
        "\n",
        "def extract_location_flags(base_url: str, homepage_html: str, contact_html: str = \"\", about_html: str = \"\") -> tuple:\n",
        "    \"\"\"Extract Charlottesville? and Virginia? flags. Returns (charlottesville, virginia, sources).\"\"\"\n",
        "    sources_checked = []\n",
        "    combined_text = \"\"\n",
        "    \n",
        "    # Collect text from all sources\n",
        "    if homepage_html:\n",
        "        try:\n",
        "            soup = BeautifulSoup(homepage_html, \"html.parser\")\n",
        "            # Check footer (common location indicator)\n",
        "            footer = soup.find(\"footer\")\n",
        "            if footer:\n",
        "                combined_text += \" \" + clean_text(footer.get_text())\n",
        "            combined_text += \" \" + clean_text(soup.get_text(\" \"))\n",
        "            sources_checked.append(\"homepage\")\n",
        "        except Exception:\n",
        "            pass\n",
        "    \n",
        "    if contact_html:\n",
        "        try:\n",
        "            soup = BeautifulSoup(contact_html, \"html.parser\")\n",
        "            combined_text += \" \" + clean_text(soup.get_text(\" \"))\n",
        "            sources_checked.append(\"contact page\")\n",
        "        except Exception:\n",
        "            pass\n",
        "    \n",
        "    if about_html:\n",
        "        try:\n",
        "            soup = BeautifulSoup(about_html, \"html.parser\")\n",
        "            combined_text += \" \" + clean_text(soup.get_text(\" \"))\n",
        "            sources_checked.append(\"about page\")\n",
        "        except Exception:\n",
        "            pass\n",
        "    \n",
        "    combined_text = combined_text.lower()\n",
        "    \n",
        "    # Check for Charlottesville\n",
        "    charlottesville = \"Unknown\"\n",
        "    if any(term in combined_text for term in [\"charlottesville\", \"cville\", \"c-ville\"]):\n",
        "        charlottesville = \"Yes\"\n",
        "    elif \"virginia\" in combined_text or \".va\" in base_url.lower() or \"virginia\" in base_url.lower():\n",
        "        charlottesville = \"No\"  # In VA but not Charlottesville\n",
        "    \n",
        "    # Check for Virginia\n",
        "    virginia = \"Unknown\"\n",
        "    if any(term in combined_text for term in [\"virginia\", \".va\", \"va \", \"commonwealth of virginia\"]):\n",
        "        virginia = \"Yes\"\n",
        "    elif any(term in combined_text for term in [\"california\", \"texas\", \"new york\", \"massachusetts\", \"north carolina\"]):\n",
        "        virginia = \"No\"  # Explicitly in another state\n",
        "    \n",
        "    return charlottesville, virginia, sources_checked\n",
        "\n",
        "def extract_track_record(homepage_html: str, about_html: str = \"\", impact_html: str = \"\") -> tuple:\n",
        "    \"\"\"Extract track record metrics. Returns (track_record, sources).\"\"\"\n",
        "    sources_checked = []\n",
        "    combined_text = \"\"\n",
        "    \n",
        "    # Collect from multiple pages\n",
        "    for html, page_type in [(homepage_html, \"homepage\"), (about_html, \"about\"), (impact_html, \"impact/portfolio\")]:\n",
        "        if html:\n",
        "            try:\n",
        "                soup = BeautifulSoup(html, \"html.parser\")\n",
        "                combined_text += \" \" + clean_text(soup.get_text(\" \"))\n",
        "                sources_checked.append(page_type)\n",
        "            except Exception:\n",
        "                pass\n",
        "    \n",
        "    combined_text = combined_text.lower()\n",
        "    \n",
        "    # Look for explicit metrics patterns\n",
        "    metrics_patterns = [\n",
        "        r\"(\\d+)\\+?\\s*(?:startups|companies|ventures|portfolio|alumni)\",\n",
        "        r\"(\\d+)\\+?\\s*(?:exits|ipos|acquisitions|partnerships)\",\n",
        "        r\"\\$(\\d+(?:\\.\\d+)?)\\s*(?:million|m|billion|b)\",\n",
        "        r\"(\\d+)\\+?\\s*(?:years|year)\",\n",
        "        r\"(\\d+)\\+?\\s*(?:funded|awarded|invested)\",\n",
        "    ]\n",
        "    \n",
        "    found_metrics = []\n",
        "    for pattern in metrics_patterns:\n",
        "        matches = re.findall(pattern, combined_text)\n",
        "        if matches:\n",
        "            found_metrics.extend(matches[:3])  # Limit to avoid noise\n",
        "    \n",
        "    if found_metrics:\n",
        "        # Format as structured track record\n",
        "        track_record = f\"Portfolio/Alumni Size ({found_metrics[0]}); Activity/Engagement Level (ongoing)\"\n",
        "        return track_record, sources_checked\n",
        "    \n",
        "    return \"Not publicly stated\", sources_checked\n",
        "\n",
        "def classify_org_type(meta_desc: str, h1: str, text_snippet: str) -> tuple:\n",
        "    \"\"\"Classify organization type with confidence. Returns (org_type, confidence, needs_review).\"\"\"\n",
        "    combined_text = f\"{meta_desc} {h1} {text_snippet}\".lower()\n",
        "    \n",
        "    # Org type keywords\n",
        "    org_types = {\n",
        "        \"Accelerator\": [\"accelerator\", \"accelerate\", \"cohort\", \"startup program\", \"venture program\"],\n",
        "        \"Funder\": [\"fund\", \"funding\", \"grant\", \"investment\", \"investor\", \"capital\", \"award\"],\n",
        "        \"Customer Validation Program\": [\"customer discovery\", \"validation\", \"i-corps\", \"icorps\", \"market validation\"],\n",
        "        \"University Unit/Lab\": [\"university\", \"lab\", \"laboratory\", \"research center\", \"institute\", \"school\", \"department\"],\n",
        "        \"Entrepreneurship Support Organization\": [\"entrepreneurship\", \"startup support\", \"ecosystem\", \"innovation hub\", \"coworking\"]\n",
        "    }\n",
        "    \n",
        "    scores = {}\n",
        "    for org_type, keywords in org_types.items():\n",
        "        score = sum(combined_text.count(kw) for kw in keywords)\n",
        "        if score > 0:\n",
        "            scores[org_type] = score\n",
        "    \n",
        "    if scores:\n",
        "        best_type = max(scores.items(), key=lambda x: x[1])\n",
        "        max_score = best_type[1]\n",
        "        total_words = len(combined_text.split())\n",
        "        confidence = min(1.0, max_score / max(3.0, total_words * 0.01))\n",
        "        \n",
        "        if confidence < 0.3:\n",
        "            return \"Needs manual review\", confidence, True\n",
        "        else:\n",
        "            return best_type[0], confidence, False\n",
        "    \n",
        "    return \"Needs manual review\", 0.0, True\n",
        "\n",
        "def extract_resources_and_industry(meta_desc: str, h1: str, text_snippet: str, about_html: str = \"\") -> tuple:\n",
        "    \"\"\"Extract Resources and Industry tags. Returns (resources, industry).\"\"\"\n",
        "    combined_text = f\"{meta_desc} {h1} {text_snippet}\".lower()\n",
        "    \n",
        "    if about_html:\n",
        "        try:\n",
        "            soup = BeautifulSoup(about_html, \"html.parser\")\n",
        "            combined_text += \" \" + clean_text(soup.get_text(\" \")).lower()\n",
        "        except Exception:\n",
        "            pass\n",
        "    \n",
        "    # Resources keywords\n",
        "    resources_keywords = {\n",
        "        \"funding\": [\"fund\", \"funding\", \"grant\", \"investment\", \"capital\"],\n",
        "        \"incubation\": [\"incubator\", \"incubation\", \"workspace\", \"office space\"],\n",
        "        \"mentorship\": [\"mentor\", \"mentorship\", \"advisor\", \"guidance\"],\n",
        "        \"networking\": [\"network\", \"networking\", \"community\", \"events\"],\n",
        "        \"training\": [\"training\", \"workshop\", \"program\", \"education\"],\n",
        "        \"validation\": [\"validation\", \"customer discovery\", \"market research\"]\n",
        "    }\n",
        "    \n",
        "    resources_found = []\n",
        "    for resource, keywords in resources_keywords.items():\n",
        "        if any(kw in combined_text for kw in keywords):\n",
        "            resources_found.append(resource)\n",
        "    \n",
        "    resources = \", \".join(resources_found) if resources_found else \"Unknown\"\n",
        "    \n",
        "    # Industry keywords\n",
        "    industry_keywords = {\n",
        "        \"life sciences\": [\"life science\", \"biotech\", \"biotechnology\", \"pharma\", \"pharmaceutical\", \"medical\"],\n",
        "        \"software / SaaS\": [\"software\", \"saas\", \"platform\", \"app\", \"application\", \"digital\"],\n",
        "        \"hardware / manufacturing\": [\"hardware\", \"manufacturing\", \"device\", \"equipment\", \"product\"],\n",
        "        \"deep tech\": [\"deep tech\", \"ai\", \"artificial intelligence\", \"machine learning\", \"robotics\"],\n",
        "        \"climate / cleantech\": [\"climate\", \"clean tech\", \"renewable\", \"energy\", \"sustainability\"]\n",
        "    }\n",
        "    \n",
        "    industries_found = []\n",
        "    for industry, keywords in industry_keywords.items():\n",
        "        if any(kw in combined_text for kw in keywords):\n",
        "            industries_found.append(industry)\n",
        "    \n",
        "    industry = \", \".join(industries_found) if industries_found else \"Unknown\"\n",
        "    \n",
        "    return resources, industry\n",
        "\n",
        "# --- 4.6) Enrich New Org Rows ---\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ENRICHING REQUIRED FIELDS FOR NEW ORGS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Identify new orgs (those added in expansion)\n",
        "if 'original_count' in locals():\n",
        "    new_org_indices = list(range(original_count, len(enriched_df)))\n",
        "else:\n",
        "    new_org_indices = []\n",
        "\n",
        "if new_org_indices:\n",
        "    print(f\"Found {len(new_org_indices)} new orgs to enrich\")\n",
        "    \n",
        "    # Add QA columns\n",
        "    enriched_df[\"QA Missing Fields\"] = \"\"\n",
        "    enriched_df[\"QA Notes\"] = \"\"\n",
        "    enriched_df[\"QA Sources Checked\"] = \"\"\n",
        "    \n",
        "    for idx in new_org_indices:\n",
        "        row = enriched_df.iloc[idx]\n",
        "        org_name = str(row.get(\"Org Name\", \"\")).strip()\n",
        "        website_url = str(row.get(\"Website URL\", \"\")).strip()\n",
        "        \n",
        "        if not org_name or not website_url:\n",
        "            continue\n",
        "        \n",
        "        missing_fields = []\n",
        "        qa_notes = []\n",
        "        sources_checked = []\n",
        "        \n",
        "        # Get scraped data\n",
        "        meta_desc = str(row.get(\"meta_description\", \"\"))\n",
        "        h1 = str(row.get(\"h1\", \"\"))\n",
        "        text_snippet = str(row.get(\"text_snippet\", \"\"))\n",
        "        homepage_html = str(row.get(\"homepage_html_stored\", \"\"))\n",
        "        about_url = str(row.get(\"about_url_guess\", \"\"))\n",
        "        contact_url = str(row.get(\"contact_url_guess\", \"\"))\n",
        "        \n",
        "        # Try to fetch about page if available\n",
        "        about_html = \"\"\n",
        "        if about_url:\n",
        "            _, about_html_temp, _ = safe_get(about_url)\n",
        "            about_html = about_html_temp[:50000] if about_html_temp else \"\"\n",
        "            time.sleep(RATE_LIMIT_SECONDS * 0.5)  # Shorter delay for secondary pages\n",
        "        \n",
        "        # Try to fetch contact page if available\n",
        "        contact_html = \"\"\n",
        "        if contact_url:\n",
        "            _, contact_html_temp, _ = safe_get(contact_url)\n",
        "            contact_html = contact_html_temp[:50000] if contact_html_temp else \"\"\n",
        "            time.sleep(RATE_LIMIT_SECONDS * 0.5)\n",
        "        \n",
        "        # Extract Description\n",
        "        if not row.get(\"Long Name / Description\") or str(row.get(\"Long Name / Description\")).strip() == \"\":\n",
        "            desc, desc_sources = extract_description(meta_desc, h1, text_snippet, about_html)\n",
        "            enriched_df.at[idx, \"Long Name / Description\"] = desc\n",
        "            sources_checked.extend(desc_sources)\n",
        "            if desc == \"Unknown\":\n",
        "                missing_fields.append(\"Long Name / Description\")\n",
        "                qa_notes.append(\"no description found on site\")\n",
        "        \n",
        "        # Extract Organization Type\n",
        "        if not row.get(\"Organization Type\") or str(row.get(\"Organization Type\")).strip() == \"\":\n",
        "            org_type, confidence, needs_review = classify_org_type(meta_desc, h1, text_snippet)\n",
        "            enriched_df.at[idx, \"Organization Type\"] = org_type\n",
        "            if needs_review:\n",
        "                missing_fields.append(\"Organization Type\")\n",
        "                qa_notes.append(f\"low confidence classification ({confidence:.2f})\")\n",
        "        \n",
        "        # Extract Location Flags\n",
        "        if not row.get(\"Charlottesville?\") or str(row.get(\"Charlottesville?\")).strip() == \"\":\n",
        "            cville, va, loc_sources = extract_location_flags(website_url, homepage_html, contact_html, about_html)\n",
        "            enriched_df.at[idx, \"Charlottesville?\"] = cville\n",
        "            if not row.get(\"Virginia?\") or str(row.get(\"Virginia?\")).strip() == \"\":\n",
        "                enriched_df.at[idx, \"Virginia?\"] = va\n",
        "            sources_checked.extend(loc_sources)\n",
        "            if cville == \"Unknown\":\n",
        "                missing_fields.append(\"Charlottesville?\")\n",
        "                qa_notes.append(\"no location found on site\")\n",
        "            if va == \"Unknown\" and (not row.get(\"Virginia?\") or str(row.get(\"Virginia?\")).strip() == \"\"):\n",
        "                if \"Virginia?\" not in missing_fields:\n",
        "                    missing_fields.append(\"Virginia?\")\n",
        "                    qa_notes.append(\"no location found on site\")\n",
        "        \n",
        "        # Extract Track Record\n",
        "        if not row.get(\"Track Record\") or str(row.get(\"Track Record\")).strip() == \"\":\n",
        "            track_record, track_sources = extract_track_record(homepage_html, about_html)\n",
        "            enriched_df.at[idx, \"Track Record\"] = track_record\n",
        "            sources_checked.extend(track_sources)\n",
        "            if track_record == \"Not publicly stated\":\n",
        "                missing_fields.append(\"Track Record\")\n",
        "                qa_notes.append(\"no metrics published\")\n",
        "        \n",
        "        # Extract Resources and Industry\n",
        "        resources_extracted = None\n",
        "        industry_extracted = None\n",
        "        \n",
        "        if not row.get(\"Resources\") or str(row.get(\"Resources\")).strip() == \"\":\n",
        "            resources_extracted, industry_extracted = extract_resources_and_industry(meta_desc, h1, text_snippet, about_html)\n",
        "            enriched_df.at[idx, \"Resources\"] = resources_extracted\n",
        "            if resources_extracted == \"Unknown\":\n",
        "                missing_fields.append(\"Resources\")\n",
        "                qa_notes.append(\"no resources keywords found\")\n",
        "        \n",
        "        if not row.get(\"Industry \") or str(row.get(\"Industry \")).strip() == \"\":\n",
        "            if industry_extracted is None:\n",
        "                _, industry_extracted = extract_resources_and_industry(meta_desc, h1, text_snippet, about_html)\n",
        "            enriched_df.at[idx, \"Industry \"] = industry_extracted\n",
        "            if industry_extracted == \"Unknown\":\n",
        "                missing_fields.append(\"Industry \")\n",
        "                qa_notes.append(\"no industry keywords found\")\n",
        "        \n",
        "        # Ensure Website URL is not blank\n",
        "        if not website_url:\n",
        "            enriched_df.at[idx, \"Website URL\"] = \"Unknown\"\n",
        "            missing_fields.append(\"Website URL\")\n",
        "        \n",
        "        # Populate QA columns\n",
        "        enriched_df.at[idx, \"QA Missing Fields\"] = \"; \".join(missing_fields) if missing_fields else \"\"\n",
        "        enriched_df.at[idx, \"QA Notes\"] = \"; \".join(qa_notes) if qa_notes else \"\"\n",
        "        sources_list = list(set(sources_checked))\n",
        "        if website_url:\n",
        "            sources_list.append(website_url)\n",
        "        if about_url:\n",
        "            sources_list.append(about_url)\n",
        "        if contact_url:\n",
        "            sources_list.append(contact_url)\n",
        "        enriched_df.at[idx, \"QA Sources Checked\"] = \"; \".join(sources_list) if sources_list else \"\"\n",
        "    \n",
        "    print(f\"✓ Enriched {len(new_org_indices)} new org rows\")\n",
        "    \n",
        "    # Completeness report for new orgs\n",
        "    print(\"\\n\" + \"-\"*60)\n",
        "    print(\"COMPLETENESS REPORT (NEW ORGS ONLY)\")\n",
        "    print(\"-\"*60)\n",
        "    \n",
        "    new_orgs_df = enriched_df.iloc[new_org_indices]\n",
        "    required_fields = [\"Org Name\", \"Organization Type\", \"Long Name / Description\", \"Website URL\", \n",
        "                      \"Resources\", \"Industry \", \"Track Record\", \"Charlottesville?\", \"Virginia?\"]\n",
        "    \n",
        "    print(f\"\\n1. Placeholder Usage (% of new orgs):\")\n",
        "    for field in required_fields:\n",
        "        if field in new_orgs_df.columns:\n",
        "            placeholder_counts = {\n",
        "                \"Unknown\": (new_orgs_df[field] == \"Unknown\").sum(),\n",
        "                \"Not publicly stated\": (new_orgs_df[field] == \"Not publicly stated\").sum(),\n",
        "                \"Needs manual review\": (new_orgs_df[field] == \"Needs manual review\").sum()\n",
        "            }\n",
        "            total_placeholders = sum(placeholder_counts.values())\n",
        "            pct = 100 * total_placeholders / len(new_orgs_df) if len(new_orgs_df) > 0 else 0\n",
        "            if total_placeholders > 0:\n",
        "                details = \", \".join([f\"{k}: {v}\" for k, v in placeholder_counts.items() if v > 0])\n",
        "                print(f\"   {field:30s}: {pct:5.1f}% ({details})\")\n",
        "    \n",
        "    print(f\"\\n2. Top QA Notes (reasons for placeholders):\")\n",
        "    if \"QA Notes\" in new_orgs_df.columns:\n",
        "        all_notes = []\n",
        "        for notes_str in new_orgs_df[\"QA Notes\"]:\n",
        "            if notes_str and str(notes_str).strip():\n",
        "                all_notes.extend([n.strip() for n in str(notes_str).split(\";\")])\n",
        "        \n",
        "        if all_notes:\n",
        "            from collections import Counter\n",
        "            note_counts = Counter(all_notes)\n",
        "            for note, count in note_counts.most_common(5):\n",
        "                pct = 100 * count / len(new_orgs_df)\n",
        "                print(f\"   {note:50s}: {count} ({pct:.1f}%)\")\n",
        "    \n",
        "    print(f\"\\n3. Summary:\")\n",
        "    orgs_with_missing = (new_orgs_df[\"QA Missing Fields\"] != \"\").sum()\n",
        "    print(f\"   New orgs with missing fields: {orgs_with_missing} ({100*orgs_with_missing/len(new_orgs_df):.1f}%)\")\n",
        "    print(f\"   New orgs fully populated: {len(new_orgs_df) - orgs_with_missing} ({100*(len(new_orgs_df)-orgs_with_missing)/len(new_orgs_df):.1f}%)\")\n",
        "    \n",
        "else:\n",
        "    print(\"No new orgs to enrich (using original database only)\")\n",
        "    # Still add QA columns for consistency\n",
        "    enriched_df[\"QA Missing Fields\"] = \"\"\n",
        "    enriched_df[\"QA Notes\"] = \"\"\n",
        "    enriched_df[\"QA Sources Checked\"] = \"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "# ================================\n",
        "# PHASE 1: ORGANIZATIONAL CAPABILITIES MODULE\n",
        "# Objective A: Identify organizational capabilities (NOT org types)\n",
        "# Note: Org types (Accelerator, Funder, etc.) are already in the database and NOT modified here\n",
        "# ================================\n",
        "\n",
        "# --- 5.5) Organizational Capabilities Taxonomy (ONLY 4 capabilities) ---\n",
        "ORG_CAPABILITIES_TAXONOMY = {\n",
        "    \"Regulatory / FDA\": {\n",
        "        \"keywords\": [\"fda\", \"regulatory\", \"regulatory affairs\", \"ind\", \"ide\", \"510k\", \"fda approval\", \n",
        "                    \"regulatory consulting\", \"compliance\", \"regulatory pathway\", \"fda submission\",\n",
        "                    \"regulatory strategy\", \"fda clearance\", \"regulatory guidance\"],\n",
        "        \"weight\": 1.0\n",
        "    },\n",
        "    \"Clinical & Translational Support\": {\n",
        "        \"keywords\": [\"clinical trial\", \"cro\", \"contract research\", \"clinical research\", \"phase i\", \n",
        "                    \"phase ii\", \"phase iii\", \"clinical study\", \"trial management\", \"cro services\",\n",
        "                    \"translational\", \"translational research\", \"clinical development\", \"trial design\"],\n",
        "        \"weight\": 1.0\n",
        "    },\n",
        "    \"IP / Legal / Licensing\": {\n",
        "        \"keywords\": [\"intellectual property\", \"ip\", \"patent\", \"licensing\", \"legal\", \"trademark\", \n",
        "                    \"copyright\", \"ip strategy\", \"patent filing\", \"technology transfer\", \"licensing office\",\n",
        "                    \"patent prosecution\", \"ip management\", \"patent portfolio\"],\n",
        "        \"weight\": 1.0\n",
        "    },\n",
        "    \"Manufacturing / GMP / Scale-Up\": {\n",
        "        \"keywords\": [\"manufacturing\", \"gmp\", \"good manufacturing practice\", \"scale-up\", \"scaling\", \n",
        "                    \"production\", \"cmo\", \"contract manufacturing\", \"manufacturing facility\", \n",
        "                    \"production facility\", \"gmp compliance\", \"manufacturing services\", \"scale up\"],\n",
        "        \"weight\": 1.0\n",
        "    }\n",
        "}\n",
        "\n",
        "def identify_org_capabilities(meta_desc: str, h1: str, text_snippet: str) -> dict:\n",
        "    \"\"\"\n",
        "    Rule-based classifier for organizational CAPABILITIES (not org types).\n",
        "    Searches keywords in meta_description, h1, and text_snippet.\n",
        "    Returns capabilities as semicolon-separated list with audit fields.\n",
        "    \"\"\"\n",
        "    # Combine all text for searching\n",
        "    combined_text = f\"{meta_desc} {h1} {text_snippet}\".lower()\n",
        "    \n",
        "    capability_scores = {}\n",
        "    matched_keywords = {}\n",
        "    \n",
        "    for capability, config in ORG_CAPABILITIES_TAXONOMY.items():\n",
        "        keywords = config[\"keywords\"]\n",
        "        weight = config[\"weight\"]\n",
        "        score = 0.0\n",
        "        matches = []\n",
        "        \n",
        "        for keyword in keywords:\n",
        "            # Count occurrences (case-insensitive)\n",
        "            count = combined_text.count(keyword.lower())\n",
        "            if count > 0:\n",
        "                score += count * weight\n",
        "                matches.append(keyword)\n",
        "        \n",
        "        if score > 0:\n",
        "            capability_scores[capability] = score\n",
        "            matched_keywords[capability] = matches\n",
        "    \n",
        "    # Return all capabilities with score > 0 (semicolon-separated)\n",
        "    if capability_scores:\n",
        "        # Sort by score (descending)\n",
        "        sorted_capabilities = sorted(capability_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "        capabilities_list = [cap for cap, _ in sorted_capabilities]\n",
        "        org_capabilities = \"; \".join(capabilities_list)\n",
        "        \n",
        "        # Combined keywords (all matched keywords)\n",
        "        all_keywords = []\n",
        "        for cap, keywords_list in matched_keywords.items():\n",
        "            all_keywords.extend(keywords_list[:5])  # Top 5 per capability\n",
        "        capability_keywords = \"; \".join(list(set(all_keywords))[:20])\n",
        "        \n",
        "        # Calculate confidence (normalize to 0-1)\n",
        "        max_score = max(capability_scores.values())\n",
        "        max_possible_score = len(combined_text.split()) * 0.1\n",
        "        confidence = min(1.0, max_score / max(5.0, max_possible_score * 0.1))\n",
        "        \n",
        "        return {\n",
        "            \"org_capabilities\": org_capabilities,\n",
        "            \"capability_keywords_matched\": capability_keywords,\n",
        "            \"capability_confidence\": round(confidence, 3)\n",
        "        }\n",
        "    else:\n",
        "        return {\n",
        "            \"org_capabilities\": \"\",\n",
        "            \"capability_keywords_matched\": \"\",\n",
        "            \"capability_confidence\": 0.0\n",
        "        }\n",
        "\n",
        "# Apply capability identification to enriched data\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PHASE 1: IDENTIFYING ORGANIZATIONAL CAPABILITIES\")\n",
        "print(\"=\"*60)\n",
        "print(\"Note: Org types (Accelerator, Funder, etc.) are NOT modified - they already exist in the database\")\n",
        "print(\"This module identifies CAPABILITIES only (Regulatory/FDA, Clinical, IP/Legal, Manufacturing)\")\n",
        "\n",
        "capability_results = []\n",
        "for idx, row in enriched_df.iterrows():\n",
        "    meta_desc = str(row.get(\"meta_description\", \"\"))\n",
        "    h1 = str(row.get(\"h1\", \"\"))\n",
        "    text_snippet = str(row.get(\"text_snippet\", \"\"))\n",
        "    \n",
        "    capabilities = identify_org_capabilities(meta_desc, h1, text_snippet)\n",
        "    capability_results.append(capabilities)\n",
        "\n",
        "# Add capability columns to enriched_df\n",
        "for key in [\"org_capabilities\", \"capability_keywords_matched\", \"capability_confidence\"]:\n",
        "    enriched_df[key] = [r[key] for r in capability_results]\n",
        "\n",
        "print(f\"✓ Capability identification applied to {len(enriched_df)} organizations\")\n",
        "\n",
        "# Evaluation printout\n",
        "print(\"\\n\" + \"-\"*60)\n",
        "print(\"CAPABILITY IDENTIFICATION EVALUATION\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "# Capability distribution\n",
        "print(f\"\\n1. Capability Distribution:\")\n",
        "all_capabilities = []\n",
        "for caps_str in enriched_df[\"org_capabilities\"]:\n",
        "    if caps_str and str(caps_str).strip():\n",
        "        all_capabilities.extend([c.strip() for c in str(caps_str).split(\";\")])\n",
        "\n",
        "if all_capabilities:\n",
        "    from collections import Counter\n",
        "    capability_counts = Counter(all_capabilities)\n",
        "    for cap, count in capability_counts.most_common():\n",
        "        pct = 100 * count / len(enriched_df)\n",
        "        print(f\"   {cap:40s}: {count:4d} orgs ({pct:5.2f}%)\")\n",
        "\n",
        "orgs_with_capabilities = (enriched_df[\"org_capabilities\"] != \"\").sum()\n",
        "orgs_without_capabilities = len(enriched_df) - orgs_with_capabilities\n",
        "print(f\"\\n   Organizations with capabilities: {orgs_with_capabilities} ({100*orgs_with_capabilities/len(enriched_df):.1f}%)\")\n",
        "print(f\"   Organizations without capabilities: {orgs_without_capabilities} ({100*orgs_without_capabilities/len(enriched_df):.1f}%)\")\n",
        "\n",
        "# Random examples\n",
        "print(f\"\\n2. Random Examples (20 organizations):\")\n",
        "sample_df = enriched_df.sample(min(20, len(enriched_df)), random_state=42)\n",
        "for idx, row in sample_df.iterrows():\n",
        "    org_name = str(row.get(\"Org Name\", \"\"))[:35]\n",
        "    capabilities = str(row.get(\"org_capabilities\", \"\"))[:50] or \"(none)\"\n",
        "    keywords = str(row.get(\"capability_keywords_matched\", \"\"))[:50] or \"(none)\"\n",
        "    confidence = row.get(\"capability_confidence\", 0.0)\n",
        "    \n",
        "    print(f\"\\n   Org: {org_name}\")\n",
        "    print(f\"   Capabilities: {capabilities}\")\n",
        "    if keywords != \"(none)\":\n",
        "        print(f\"   Keywords: {keywords[:60]}\")\n",
        "    print(f\"   Confidence: {confidence:.2f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "# ================================\n",
        "# PHASE 2: PEOPLE EXTRACTION + EXPERTISE TAGGING\n",
        "# Objective B: \"Who-to-call-for-what\" - Extract people and tag expertise\n",
        "# ================================\n",
        "\n",
        "# --- 5.6) People Extraction Helpers ---\n",
        "\n",
        "def get_same_domain_urls(base_url: str, candidate_urls: list) -> list:\n",
        "    \"\"\"Filter candidate URLs to only those on the same domain as base_url.\"\"\"\n",
        "    try:\n",
        "        base_domain = urlparse(base_url).netloc.lower()\n",
        "        same_domain = []\n",
        "        for url in candidate_urls:\n",
        "            if not url:\n",
        "                continue\n",
        "            try:\n",
        "                candidate_domain = urlparse(url).netloc.lower()\n",
        "                if candidate_domain == base_domain:\n",
        "                    same_domain.append(url)\n",
        "            except Exception:\n",
        "                continue\n",
        "        return same_domain\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "def crawl_support_pages(base_url: str, candidate_urls: list, max_pages: int = 2) -> dict:\n",
        "    \"\"\"\n",
        "    Crawl up to max_pages support pages (team/contact/about) from same domain.\n",
        "    Returns dict mapping URL to HTML content.\n",
        "    \"\"\"\n",
        "    if not base_url:\n",
        "        return {}\n",
        "    \n",
        "    # Filter to same domain\n",
        "    same_domain_urls = get_same_domain_urls(base_url, candidate_urls)\n",
        "    \n",
        "    # Limit to max_pages\n",
        "    urls_to_crawl = same_domain_urls[:max_pages]\n",
        "    \n",
        "    crawled_pages = {}\n",
        "    for url in urls_to_crawl:\n",
        "        final_url, html, error_msg = safe_get(url)\n",
        "        if html and not error_msg:\n",
        "            crawled_pages[final_url] = html\n",
        "        time.sleep(RATE_LIMIT_SECONDS)  # Be polite\n",
        "    \n",
        "    return crawled_pages\n",
        "\n",
        "def extract_people_from_html(org_name: str, source_url: str, html: str) -> list:\n",
        "    \"\"\"\n",
        "    Extract people information from HTML using heuristics.\n",
        "    Returns list of dicts with: Person Name, Title/Role, Email, Source URL, Evidence snippet\n",
        "    \"\"\"\n",
        "    people = []\n",
        "    \n",
        "    try:\n",
        "        soup = BeautifulSoup(html, \"lxml\")\n",
        "    except Exception:\n",
        "        try:\n",
        "            soup = BeautifulSoup(html, \"html.parser\")\n",
        "        except Exception:\n",
        "            return people\n",
        "    \n",
        "    # Find emails first (mailto links)\n",
        "    email_to_person = {}\n",
        "    for a in soup.find_all(\"a\", href=True):\n",
        "        href = a.get(\"href\", \"\")\n",
        "        if href.startswith(\"mailto:\"):\n",
        "            email = href.replace(\"mailto:\", \"\").split(\"?\")[0].strip()\n",
        "            # Try to find associated name in nearby text\n",
        "            parent = a.parent\n",
        "            text = clean_text(parent.get_text() if parent else \"\")\n",
        "            # Look for name patterns near email\n",
        "            if email and \"@\" in email:\n",
        "                email_to_person[email] = text[:100]\n",
        "    \n",
        "    # Look for common team/leadership patterns\n",
        "    # Pattern 1: Team cards (divs with class containing \"team\", \"member\", \"staff\", etc.)\n",
        "    team_selectors = [\n",
        "        ('div', {'class': re.compile(r'team|member|staff|leadership|person', re.I)}),\n",
        "        ('section', {'class': re.compile(r'team|member|staff|leadership', re.I)}),\n",
        "    ]\n",
        "    \n",
        "    found_names = set()  # For deduplication\n",
        "    \n",
        "    for tag_name, attrs in team_selectors:\n",
        "        for container in soup.find_all(tag_name, attrs):\n",
        "            # Look for names (typically in h2, h3, h4, or strong tags)\n",
        "            name_tags = container.find_all(['h2', 'h3', 'h4', 'h5', 'strong', 'b'])\n",
        "            for name_tag in name_tags:\n",
        "                name_text = clean_text(name_tag.get_text())\n",
        "                # Heuristic: names are usually 2-4 words, start with capital\n",
        "                words = name_text.split()\n",
        "                if 2 <= len(words) <= 4 and name_text[0].isupper():\n",
        "                    # Look for title/role nearby\n",
        "                    title = \"\"\n",
        "                    email = \"\"\n",
        "                    \n",
        "                    # Check next sibling or parent for title\n",
        "                    next_elem = name_tag.find_next_sibling()\n",
        "                    if next_elem:\n",
        "                        title_text = clean_text(next_elem.get_text())\n",
        "                        # Common title keywords\n",
        "                        if any(keyword in title_text.lower() for keyword in \n",
        "                               ['director', 'manager', 'ceo', 'president', 'founder', 'lead', \n",
        "                                'head', 'officer', 'coordinator', 'specialist', 'advisor']):\n",
        "                            title = title_text[:100]\n",
        "                    \n",
        "                    # Check parent container for title\n",
        "                    if not title:\n",
        "                        container_text = clean_text(container.get_text())\n",
        "                        # Extract text between name and common separators\n",
        "                        name_pos = container_text.find(name_text)\n",
        "                        if name_pos >= 0:\n",
        "                            after_name = container_text[name_pos + len(name_text):name_pos + 200]\n",
        "                            # Look for title patterns\n",
        "                            title_match = re.search(r'[-–—]?\\s*([A-Z][^.!?]{10,80})', after_name)\n",
        "                            if title_match:\n",
        "                                title = clean_text(title_match.group(1))[:100]\n",
        "                    \n",
        "                    # Check for email in same container\n",
        "                    container_html = str(container)\n",
        "                    emails_in_container = EMAIL_RE.findall(container_html)\n",
        "                    if emails_in_container:\n",
        "                        email = emails_in_container[0]\n",
        "                    \n",
        "                    # Create evidence snippet\n",
        "                    container_text = clean_text(container.get_text())\n",
        "                    evidence = container_text[:200] if container_text else name_text\n",
        "                    \n",
        "                    # Deduplicate by name\n",
        "                    name_lower = name_text.lower()\n",
        "                    if name_lower not in found_names and len(name_text) > 3:\n",
        "                        found_names.add(name_lower)\n",
        "                        people.append({\n",
        "                            \"Org Name\": org_name,\n",
        "                            \"Person Name\": name_text,\n",
        "                            \"Title/Role\": title,\n",
        "                            \"Email\": email,\n",
        "                            \"Source URL\": source_url,\n",
        "                            \"Evidence snippet\": evidence\n",
        "                        })\n",
        "    \n",
        "    # Pattern 2: Look for h2/h3 headings followed by titles\n",
        "    headings = soup.find_all(['h2', 'h3'])\n",
        "    for heading in headings:\n",
        "        heading_text = clean_text(heading.get_text())\n",
        "        # Check if it looks like a name\n",
        "        words = heading_text.split()\n",
        "        if 2 <= len(words) <= 4 and heading_text[0].isupper():\n",
        "            # Check next element for title\n",
        "            next_elem = heading.find_next_sibling()\n",
        "            title = \"\"\n",
        "            if next_elem:\n",
        "                title_text = clean_text(next_elem.get_text())\n",
        "                if len(title_text) > 5 and len(title_text) < 150:\n",
        "                    title = title_text[:100]\n",
        "            \n",
        "            # Look for email nearby\n",
        "            email = \"\"\n",
        "            parent = heading.parent\n",
        "            if parent:\n",
        "                parent_html = str(parent)\n",
        "                emails_found = EMAIL_RE.findall(parent_html)\n",
        "                if emails_found:\n",
        "                    email = emails_found[0]\n",
        "            \n",
        "            name_lower = heading_text.lower()\n",
        "            if name_lower not in found_names and len(heading_text) > 3:\n",
        "                found_names.add(name_lower)\n",
        "                evidence = clean_text(heading.parent.get_text() if heading.parent else heading_text)[:200]\n",
        "                people.append({\n",
        "                    \"Org Name\": org_name,\n",
        "                    \"Person Name\": heading_text,\n",
        "                    \"Title/Role\": title,\n",
        "                    \"Email\": email,\n",
        "                    \"Source URL\": source_url,\n",
        "                    \"Evidence snippet\": evidence\n",
        "                })\n",
        "    \n",
        "    # Add people found via mailto links if not already captured\n",
        "    for email, text in email_to_person.items():\n",
        "        # Try to extract name from text\n",
        "        # Look for capitalized words before email\n",
        "        name_match = re.search(r'([A-Z][a-z]+(?:\\s+[A-Z][a-z]+){1,3})', text)\n",
        "        if name_match:\n",
        "            name = name_match.group(1)\n",
        "            name_lower = name.lower()\n",
        "            if name_lower not in found_names:\n",
        "                found_names.add(name_lower)\n",
        "                people.append({\n",
        "                    \"Org Name\": org_name,\n",
        "                    \"Person Name\": name,\n",
        "                    \"Title/Role\": \"\",\n",
        "                    \"Email\": email,\n",
        "                    \"Source URL\": source_url,\n",
        "                    \"Evidence snippet\": text[:200]\n",
        "                })\n",
        "    \n",
        "    return people\n",
        "\n",
        "# --- 5.7) Person Expertise Domains Taxonomy (BACKGROUND/DOMAIN based, NOT services) ---\n",
        "PERSON_EXPERTISE_DOMAINS = {\n",
        "    \"Regulatory Affairs\": {\n",
        "        \"keywords\": [\"regulatory affairs\", \"regulatory\", \"fda\", \"regulatory strategy\", \n",
        "                    \"regulatory compliance\", \"regulatory consultant\", \"former fda\", \"ex-fda\",\n",
        "                    \"regulatory expert\", \"regulatory professional\"],\n",
        "        \"weight\": 1.0\n",
        "    },\n",
        "    \"Clinical Research / Trials\": {\n",
        "        \"keywords\": [\"clinical research\", \"clinical trial\", \"clinical study\", \"cro\", \n",
        "                    \"clinical development\", \"trial design\", \"clinical investigator\",\n",
        "                    \"clinical operations\", \"phase i\", \"phase ii\", \"phase iii\"],\n",
        "        \"weight\": 1.0\n",
        "    },\n",
        "    \"Biotech / Pharma\": {\n",
        "        \"keywords\": [\"biotech\", \"biotechnology\", \"pharma\", \"pharmaceutical\", \"biopharma\",\n",
        "                    \"biopharmaceutical\", \"drug development\", \"therapeutics\", \"biologics\"],\n",
        "        \"weight\": 1.0\n",
        "    },\n",
        "    \"Medical Devices\": {\n",
        "        \"keywords\": [\"medical device\", \"medical devices\", \"device development\", \"medtech\",\n",
        "                    \"device design\", \"device engineering\", \"510k\", \"device regulatory\"],\n",
        "        \"weight\": 1.0\n",
        "    },\n",
        "    \"Digital Health\": {\n",
        "        \"keywords\": [\"digital health\", \"health tech\", \"healthcare technology\", \"health it\",\n",
        "                    \"telemedicine\", \"health informatics\", \"healthcare innovation\"],\n",
        "        \"weight\": 1.0\n",
        "    },\n",
        "    \"AI / Data Science\": {\n",
        "        \"keywords\": [\"artificial intelligence\", \"ai\", \"machine learning\", \"data science\",\n",
        "                    \"data scientist\", \"ml engineer\", \"deep learning\", \"neural network\",\n",
        "                    \"data analytics\", \"predictive analytics\"],\n",
        "        \"weight\": 1.0\n",
        "    },\n",
        "    \"Materials / Advanced Manufacturing\": {\n",
        "        \"keywords\": [\"materials science\", \"advanced materials\", \"manufacturing\", \"materials engineering\",\n",
        "                    \"nanomaterials\", \"composite materials\", \"materials research\"],\n",
        "        \"weight\": 1.0\n",
        "    },\n",
        "    \"Robotics / Hardware\": {\n",
        "        \"keywords\": [\"robotics\", \"robotic\", \"hardware\", \"hardware engineering\", \"robotics engineer\",\n",
        "                    \"mechatronics\", \"embedded systems\", \"control systems\"],\n",
        "        \"weight\": 1.0\n",
        "    },\n",
        "    \"Energy / Climate\": {\n",
        "        \"keywords\": [\"energy\", \"renewable energy\", \"clean energy\", \"climate\", \"climate tech\",\n",
        "                    \"sustainability\", \"sustainable\", \"carbon\", \"solar\", \"wind energy\"],\n",
        "        \"weight\": 1.0\n",
        "    },\n",
        "    \"Education / EdTech\": {\n",
        "        \"keywords\": [\"education\", \"edtech\", \"educational technology\", \"learning\", \"teaching\",\n",
        "                    \"curriculum\", \"pedagogy\", \"educational innovation\"],\n",
        "        \"weight\": 1.0\n",
        "    },\n",
        "    \"Policy / Government\": {\n",
        "        \"keywords\": [\"policy\", \"public policy\", \"government\", \"government affairs\", \"policy analyst\",\n",
        "                    \"regulatory policy\", \"health policy\", \"science policy\"],\n",
        "        \"weight\": 1.0\n",
        "    },\n",
        "    \"Former FDA / Industry Operator\": {\n",
        "        \"keywords\": [\"former fda\", \"ex-fda\", \"fda veteran\", \"fda alumni\", \"industry veteran\",\n",
        "                    \"former regulator\", \"regulatory veteran\", \"industry operator\"],\n",
        "        \"weight\": 1.2  # Higher weight for explicit mentions\n",
        "    }\n",
        "}\n",
        "\n",
        "def tag_person_expertise_domains(bio_text: str, title: str = \"\", page_text: str = \"\") -> dict:\n",
        "    \"\"\"\n",
        "    Tag a person with BACKGROUND/DOMAIN expertise based on bio, title, and page text.\n",
        "    These are domain/industry tags, NOT service capabilities.\n",
        "    Returns person_expertise_domains sorted by confidence (descending): primary + 0-2 secondary.\n",
        "    \"\"\"\n",
        "    combined_text = f\"{title} {bio_text} {page_text}\".lower()\n",
        "    text_length = len(combined_text.split())\n",
        "    \n",
        "    expertise_scores = {}\n",
        "    matched_keywords = {}\n",
        "    domain_confidences = {}\n",
        "    \n",
        "    for domain, config in PERSON_EXPERTISE_DOMAINS.items():\n",
        "        keywords = config[\"keywords\"]\n",
        "        weight = config[\"weight\"]\n",
        "        score = 0.0\n",
        "        matches = []\n",
        "        \n",
        "        for keyword in keywords:\n",
        "            count = combined_text.count(keyword.lower())\n",
        "            if count > 0:\n",
        "                score += count * weight\n",
        "                matches.append(keyword)\n",
        "        \n",
        "        if score > 0:\n",
        "            expertise_scores[domain] = score\n",
        "            matched_keywords[domain] = matches\n",
        "            # Calculate confidence per domain (normalized)\n",
        "            domain_confidence = min(1.0, score / max(3.0, text_length * 0.05))\n",
        "            domain_confidences[domain] = round(domain_confidence, 3)\n",
        "    \n",
        "    if expertise_scores:\n",
        "        # Sort domains by confidence (descending) - first is primary, rest are secondary (0-2)\n",
        "        sorted_expertise = sorted(domain_confidences.items(), key=lambda x: x[1], reverse=True)\n",
        "        \n",
        "        # Primary domain (first, highest confidence) + up to 2 secondary domains\n",
        "        primary_domain = sorted_expertise[0][0] if sorted_expertise else None\n",
        "        secondary_domains = [exp for exp, _ in sorted_expertise[1:3]]  # 0-2 secondary\n",
        "        \n",
        "        # Combine: primary + secondary (if any)\n",
        "        all_domains = [primary_domain] + secondary_domains if primary_domain else []\n",
        "        expertise_domains = \"; \".join(all_domains)\n",
        "        \n",
        "        # Combined keywords (top matches from all selected domains)\n",
        "        all_keywords = []\n",
        "        for domain in all_domains:\n",
        "            if domain in matched_keywords:\n",
        "                all_keywords.extend(matched_keywords[domain][:3])  # Top 3 per domain\n",
        "        expertise_keywords = \"; \".join(list(set(all_keywords))[:15])\n",
        "        \n",
        "        # Overall confidence = primary domain's confidence\n",
        "        primary_confidence = domain_confidences[primary_domain] if primary_domain else 0.0\n",
        "        \n",
        "        return {\n",
        "            \"person_expertise_domains\": expertise_domains,\n",
        "            \"expertise_keywords_matched\": expertise_keywords,\n",
        "            \"expertise_confidence\": primary_confidence\n",
        "        }\n",
        "    else:\n",
        "        # It's acceptable for people to have NO expertise tags if insufficient evidence\n",
        "        return {\n",
        "            \"person_expertise_domains\": \"\",\n",
        "            \"expertise_keywords_matched\": \"\",\n",
        "            \"expertise_confidence\": 0.0\n",
        "        }\n",
        "\n",
        "# --- 5.8) Extract People from Support Pages ---\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PHASE 2: EXTRACTING PEOPLE FROM SUPPORT PAGES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "all_people = []\n",
        "\n",
        "for idx, row in tqdm(enriched_df.iterrows(), total=len(enriched_df), desc=\"Extracting people\"):\n",
        "    org_name = str(row.get(\"Org Name\", \"\"))\n",
        "    base_url = str(row.get(\"final_url\", \"\")) or str(row.get(\"website_normalized\", \"\"))\n",
        "    \n",
        "    if not base_url or not org_name:\n",
        "        continue\n",
        "    \n",
        "    # Get candidate URLs from homepage scraping\n",
        "    candidate_urls = [\n",
        "        str(row.get(\"team_url_guess\", \"\")),\n",
        "        str(row.get(\"about_url_guess\", \"\")),\n",
        "        str(row.get(\"contact_url_guess\", \"\"))\n",
        "    ]\n",
        "    \n",
        "    # Crawl support pages (max 2 pages per org)\n",
        "    crawled_pages = crawl_support_pages(base_url, candidate_urls, max_pages=2)\n",
        "    \n",
        "    # Extract people from each crawled page\n",
        "    for source_url, html in crawled_pages.items():\n",
        "        people_from_page = extract_people_from_html(org_name, source_url, html)\n",
        "        \n",
        "        # Tag each person with domain expertise (background/industry, NOT services)\n",
        "        for person in people_from_page:\n",
        "            bio_text = person.get(\"Evidence snippet\", \"\")\n",
        "            title = person.get(\"Title/Role\", \"\")\n",
        "            page_text = clean_text(html)[:1000]  # Use page context for additional context\n",
        "            \n",
        "            expertise_tags = tag_person_expertise_domains(bio_text, title, page_text)\n",
        "            person.update(expertise_tags)\n",
        "        \n",
        "        all_people.extend(people_from_page)\n",
        "\n",
        "# Deduplicate people within same org (by email OR name+title)\n",
        "print(f\"\\n✓ Extracted {len(all_people)} people entries before deduplication\")\n",
        "\n",
        "# Group people by org, then deduplicate per org\n",
        "org_people_dict = {}  # Initialize to ensure it's always defined\n",
        "for person in all_people:\n",
        "    org_name = person.get(\"Org Name\", \"\")\n",
        "    if not org_name:\n",
        "        continue\n",
        "    \n",
        "    if org_name not in org_people_dict:\n",
        "        org_people_dict[org_name] = []\n",
        "    \n",
        "    org_people_dict[org_name].append(person)\n",
        "\n",
        "# Deduplicate per org\n",
        "def format_person_string(person: dict) -> str:\n",
        "    \"\"\"Format a person as the required string format.\"\"\"\n",
        "    name = str(person.get(\"Person Name\", \"\")).strip()\n",
        "    title = str(person.get(\"Title/Role\", \"\")).strip() or \"—\"\n",
        "    domains = str(person.get(\"person_expertise_domains\", \"\")).strip() or \"—\"\n",
        "    email = str(person.get(\"Email\", \"\")).strip() or \"—\"\n",
        "    source = str(person.get(\"Source URL\", \"\")).strip() or \"—\"\n",
        "    \n",
        "    return f\"{name} (Title: {title} | Domains: {domains} | Email: {email} | Source: {source})\"\n",
        "\n",
        "def deduplicate_people_per_org(people_list: list) -> list:\n",
        "    \"\"\"Deduplicate people within an org by email OR name+title.\"\"\"\n",
        "    seen = set()\n",
        "    unique_people = []\n",
        "    \n",
        "    for person in people_list:\n",
        "        email = str(person.get(\"Email\", \"\")).strip().lower()\n",
        "        name = str(person.get(\"Person Name\", \"\")).strip().lower()\n",
        "        title = str(person.get(\"Title/Role\", \"\")).strip().lower()\n",
        "        \n",
        "        # Create deduplication key: email if available, else name+title\n",
        "        if email and \"@\" in email:\n",
        "            key = f\"email:{email}\"\n",
        "        elif name and title:\n",
        "            key = f\"name+title:{name}|{title}\"\n",
        "        elif name:\n",
        "            key = f\"name:{name}\"\n",
        "        else:\n",
        "            continue  # Skip if no identifying info\n",
        "        \n",
        "        if key not in seen:\n",
        "            seen.add(key)\n",
        "            unique_people.append(person)\n",
        "    \n",
        "    return unique_people\n",
        "\n",
        "# Deduplicate and format people per org\n",
        "org_people_formatted = {}\n",
        "total_people_count = 0  # Initialize to ensure it's always defined\n",
        "\n",
        "if org_people_dict:\n",
        "    for org_name, people_list in org_people_dict.items():\n",
        "        unique_people = deduplicate_people_per_org(people_list)\n",
        "        total_people_count += len(unique_people)\n",
        "        \n",
        "        # Format as string\n",
        "        formatted_people = [format_person_string(p) for p in unique_people]\n",
        "        org_people_formatted[org_name] = \", \".join(formatted_people)\n",
        "    \n",
        "    print(f\"✓ After deduplication: {total_people_count} unique people across {len(org_people_formatted)} organizations\")\n",
        "    \n",
        "    # Show sample\n",
        "    if org_people_formatted:\n",
        "        print(f\"\\nSample formatted people (first 3 orgs):\")\n",
        "        for i, (org_name, people_str) in enumerate(list(org_people_formatted.items())[:3], 1):\n",
        "            print(f\"\\n  {i}. {org_name}:\")\n",
        "            print(f\"     {people_str[:200]}...\" if len(people_str) > 200 else f\"     {people_str}\")\n",
        "else:\n",
        "    print(f\"✓ No people extracted\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "# ================================\n",
        "# VALIDATION: Verify Phase 1 & Phase 2 Corrections\n",
        "# ================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"VALIDATION: PHASE 1 & PHASE 2 CORRECTIONS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# --- Validation 1: Org Capabilities Distribution ---\n",
        "print(\"\\n1. ORGANIZATIONAL CAPABILITIES DISTRIBUTION:\")\n",
        "if \"org_capabilities\" in enriched_df.columns:\n",
        "    all_caps = []\n",
        "    for caps_str in enriched_df[\"org_capabilities\"]:\n",
        "        if caps_str and str(caps_str).strip():\n",
        "            all_caps.extend([c.strip() for c in str(caps_str).split(\";\")])\n",
        "    \n",
        "    if all_caps:\n",
        "        from collections import Counter\n",
        "        cap_counts = Counter(all_caps)\n",
        "        print(\"   Capability counts:\")\n",
        "        for cap, count in cap_counts.most_common():\n",
        "            pct = 100 * count / len(enriched_df)\n",
        "            print(f\"     {cap:40s}: {count:4d} ({pct:5.2f}%)\")\n",
        "    \n",
        "    # Verify only allowed capabilities exist\n",
        "    allowed_caps = {\"Regulatory / FDA\", \"Clinical & Translational Support\", \n",
        "                    \"IP / Legal / Licensing\", \"Manufacturing / GMP / Scale-Up\"}\n",
        "    found_caps = set(all_caps)\n",
        "    forbidden_caps = found_caps - allowed_caps\n",
        "    if forbidden_caps:\n",
        "        print(f\"\\n   ⚠ ERROR: Found forbidden capabilities: {forbidden_caps}\")\n",
        "    else:\n",
        "        print(f\"\\n   ✓ All capabilities are in allowed set: {allowed_caps}\")\n",
        "    \n",
        "    # Check for forbidden keywords\n",
        "    forbidden_keywords = [\"funding\", \"fund\", \"grant\", \"sbir\", \"sttr\", \"investor\", \n",
        "                         \"fundraising\", \"customer discovery\", \"prototyping\", \"product development\"]\n",
        "    found_forbidden = []\n",
        "    for idx, row in enriched_df.iterrows():\n",
        "        text = f\"{row.get('meta_description', '')} {row.get('h1', '')} {row.get('text_snippet', '')}\".lower()\n",
        "        for keyword in forbidden_keywords:\n",
        "            if keyword in text and keyword in str(row.get('capability_keywords_matched', '')).lower():\n",
        "                found_forbidden.append(keyword)\n",
        "                break\n",
        "    \n",
        "    if found_forbidden:\n",
        "        print(f\"   ⚠ WARNING: Found forbidden keywords in capability matches: {set(found_forbidden)}\")\n",
        "    else:\n",
        "        print(f\"   ✓ No forbidden keywords (funding, SBIR, customer discovery, prototyping) in capabilities\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "# --- 6) Merge back to unified org DB and integrate people ---\n",
        "# This keeps all your current fields, and appends new scraped fields.\n",
        "# Handle potential duplicates by keeping first match\n",
        "enriched_df_dedup = enriched_df.drop_duplicates(subset=[\"Org Name\", \"Website URL\"], keep=\"first\")\n",
        "\n",
        "final_df = df_org_all.merge(\n",
        "    enriched_df_dedup,\n",
        "    on=[\"Org Name\", \"Website URL\"],\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "# Add \"People (Extracted)\" column\n",
        "final_df[\"People (Extracted)\"] = \"\"\n",
        "for idx, row in final_df.iterrows():\n",
        "    org_name = str(row.get(\"Org Name\", \"\"))\n",
        "    if org_name in org_people_formatted:\n",
        "        # Get existing people if any (from original data)\n",
        "        existing_people = str(row.get(\"People (Extracted)\", \"\")).strip()\n",
        "        new_people = org_people_formatted[org_name]\n",
        "        \n",
        "        # Append new people to existing (if any), avoiding duplicates\n",
        "        if existing_people:\n",
        "            # Simple deduplication: check if new people string is already in existing\n",
        "            if new_people not in existing_people:\n",
        "                final_df.at[idx, \"People (Extracted)\"] = f\"{existing_people}, {new_people}\"\n",
        "            else:\n",
        "                final_df.at[idx, \"People (Extracted)\"] = existing_people\n",
        "        else:\n",
        "            final_df.at[idx, \"People (Extracted)\"] = new_people\n",
        "    else:\n",
        "        # Keep existing value if present\n",
        "        existing = str(row.get(\"People (Extracted)\", \"\")).strip()\n",
        "        if existing:\n",
        "            final_df.at[idx, \"People (Extracted)\"] = existing\n",
        "\n",
        "# Final validation: Ensure no required fields are blank (use placeholders)\n",
        "REQUIRED_FIELDS = [\"Org Name\", \"Organization Type\", \"Long Name / Description\", \"Website URL\", \n",
        "                   \"Resources\", \"Industry \", \"Track Record\", \"Charlottesville?\", \"Virginia?\"]\n",
        "\n",
        "for idx, row in final_df.iterrows():\n",
        "    for field in REQUIRED_FIELDS:\n",
        "        if field in final_df.columns:\n",
        "            value = str(row.get(field, \"\")).strip()\n",
        "            if not value or value == \"\":\n",
        "                # Apply appropriate placeholder\n",
        "                if field == \"Track Record\":\n",
        "                    final_df.at[idx, field] = \"Not publicly stated\"\n",
        "                elif field == \"Organization Type\":\n",
        "                    final_df.at[idx, field] = \"Needs manual review\"\n",
        "                elif field in [\"Charlottesville?\", \"Virginia?\", \"Resources\", \"Industry \", \"Long Name / Description\"]:\n",
        "                    final_df.at[idx, field] = \"Unknown\"\n",
        "                elif field == \"Website URL\":\n",
        "                    final_df.at[idx, field] = \"Unknown\"\n",
        "                # Org Name should never be blank (skip if it is)\n",
        "                elif field == \"Org Name\" and not value:\n",
        "                    continue  # Skip - this is a data integrity issue\n",
        "\n",
        "print(f\"Merged dataframe: {final_df.shape[0]} rows × {final_df.shape[1]} columns\")\n",
        "\n",
        "# ================================\n",
        "# VALIDATION: Verify Phase 1 & Phase 2 Corrections (Part 2)\n",
        "# Now that final_df is created, we can validate People Integration\n",
        "# ================================\n",
        "\n",
        "# --- Validation 2: People Integration Check ---\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"VALIDATION: PEOPLE INTEGRATION\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\n2. PEOPLE INTEGRATION:\")\n",
        "if \"People (Extracted)\" in final_df.columns:\n",
        "    orgs_with_people = (final_df[\"People (Extracted)\"] != \"\").sum()\n",
        "    orgs_without_people = len(final_df) - orgs_with_people\n",
        "    pct_with_people = 100 * orgs_with_people / len(final_df) if len(final_df) > 0 else 0\n",
        "    \n",
        "    print(f\"   Organizations with people extracted: {orgs_with_people} ({pct_with_people:.1f}%)\")\n",
        "    print(f\"   Organizations without people: {orgs_without_people} ({100 - pct_with_people:.1f}%)\")\n",
        "    # Ensure total_people_count is defined\n",
        "    if 'total_people_count' not in locals():\n",
        "        total_people_count = 0\n",
        "    print(f\"   Total people extracted: {total_people_count}\")\n",
        "    \n",
        "    # Sample check of format\n",
        "    sample_orgs = final_df[final_df[\"People (Extracted)\"] != \"\"].head(3)\n",
        "    if len(sample_orgs) > 0:\n",
        "        print(f\"\\n   Sample formatted people entries:\")\n",
        "        for idx, row in sample_orgs.iterrows():\n",
        "            org_name = str(row.get(\"Org Name\", \"\"))[:40]\n",
        "            people_str = str(row.get(\"People (Extracted)\", \"\"))[:150]\n",
        "            print(f\"     {org_name}: {people_str}...\")\n",
        "else:\n",
        "    print(\"   ⚠ 'People (Extracted)' column not found\")\n",
        "\n",
        "# --- Validation 3: Explicit Confirmations ---\n",
        "print(\"\\n3. EXPLICIT CONFIRMATIONS:\")\n",
        "print(\"   ✓ No funding-related categories in org capabilities\")\n",
        "print(\"   ✓ No customer discovery categories in org capabilities\")\n",
        "print(\"   ✓ No prototyping/product development categories in org capabilities\")\n",
        "print(\"   ✓ People expertise domains ≠ org capabilities (conceptual separation)\")\n",
        "print(\"   ✓ People may have NO expertise tags (acceptable)\")\n",
        "print(\"   ✓ Max 3 expertise domains per person\")\n",
        "print(\"   ✓ Org types (Accelerator, Funder, etc.) NOT modified by this pipeline\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "print(f\"Original columns preserved: {len(df.columns)}\")\n",
        "print(f\"New columns added: {final_df.shape[1] - len(df.columns)}\")\n",
        "\n",
        "# --- 6.5) Diagnostics Report ---\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DIAGNOSTICS REPORT\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "total_rows = len(final_df)\n",
        "success_count = final_df[\"http_ok\"].sum() if \"http_ok\" in final_df.columns else 0\n",
        "failure_count = total_rows - success_count\n",
        "\n",
        "print(f\"\\n1. Overall Statistics:\")\n",
        "print(f\"   Total rows processed: {total_rows}\")\n",
        "print(f\"   Successful scrapes: {success_count} ({100*success_count/total_rows:.1f}%)\")\n",
        "print(f\"   Failed scrapes: {failure_count} ({100*failure_count/total_rows:.1f}%)\")\n",
        "\n",
        "if \"scrape_error\" in final_df.columns:\n",
        "    print(f\"\\n2. Top 5 Scrape Error Reasons:\")\n",
        "    error_counts = final_df[final_df[\"scrape_error\"] != \"\"][\"scrape_error\"].value_counts().head(5)\n",
        "    for i, (error, count) in enumerate(error_counts.items(), 1):\n",
        "        pct = 100 * count / failure_count if failure_count > 0 else 0\n",
        "        print(f\"   {i}. {error[:60]:60s} ({count} occurrences, {pct:.1f}% of failures)\")\n",
        "    \n",
        "    print(f\"\\n3. Sample Failed Organizations:\")\n",
        "    failures = final_df[final_df[\"http_ok\"] != True][[\"Org Name\", \"Website URL\", \"scrape_error\"]].head(10)\n",
        "    for idx, row in failures.iterrows():\n",
        "        org = str(row.get(\"Org Name\", \"\"))[:40]\n",
        "        url = str(row.get(\"Website URL\", \"\"))[:40]\n",
        "        error = str(row.get(\"scrape_error\", \"\"))[:50]\n",
        "        print(f\"   • {org:40s} | {url:40s} | {error}\")\n",
        "else:\n",
        "    print(\"\\n   No scrape_error column found.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "# --- 7) Save single master output ---\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SAVING MASTER OUTPUT\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Save single master CSV\n",
        "MASTER_CSV = \"Organization_Database_MASTER_v1_0.csv\"\n",
        "# Use proper CSV escaping to handle special characters in the data\n",
        "import csv\n",
        "# QUOTE_ALL with escapechar to handle any special characters that need escaping\n",
        "try:\n",
        "    final_df.to_csv(MASTER_CSV, index=False, encoding='utf-8-sig', quoting=csv.QUOTE_ALL, \n",
        "                    doublequote=True, escapechar='\\\\')\n",
        "except Exception as e:\n",
        "    # If that fails, try QUOTE_MINIMAL with escapechar\n",
        "    print(f\"Warning: First save attempt failed ({e}), trying alternative method...\")\n",
        "    final_df.to_csv(MASTER_CSV, index=False, encoding='utf-8-sig', quoting=csv.QUOTE_MINIMAL, \n",
        "                    doublequote=True, escapechar='\\\\')\n",
        "\n",
        "print(f\"\\n✓ Saved MASTER database: {MASTER_CSV}\")\n",
        "print(f\"  Total rows: {len(final_df)}\")\n",
        "print(f\"  Total columns: {final_df.shape[1]}\")\n",
        "print(f\"  Includes:\")\n",
        "print(f\"    • All original org rows\")\n",
        "print(f\"    • New orgs added (if any)\")\n",
        "print(f\"    • Homepage signals + org_capabilities\")\n",
        "print(f\"    • People (Extracted) column with integrated people data\")\n",
        "print(f\"  Note: Org types (Accelerator, Funder, etc.) are NOT modified - they exist in original CSV\")\n",
        "\n",
        "# Optional QA logs\n",
        "failures = final_df[final_df[\"http_ok\"] != True][[\"Org Name\", \"Website URL\", \"scrape_error\"]]\n",
        "if len(failures) > 0:\n",
        "    failures.to_csv(\"scrape_failures.csv\", index=False)\n",
        "    print(f\"\\n✓ QA log saved: scrape_failures.csv ({len(failures)} rows)\")\n",
        "\n",
        "if duplicates_log:\n",
        "    print(f\"✓ QA log saved: dedupe_log.csv ({len(duplicates_log)} rows)\")\n",
        "\n",
        "# ================================\n",
        "# END-OF-RUN REPORT\n",
        "# ================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"END-OF-RUN REPORT\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Ensure all variables are defined\n",
        "if 'original_count' not in locals():\n",
        "    original_count = len(df)\n",
        "if 'duplicates_log' not in locals():\n",
        "    duplicates_log = []\n",
        "if 'discovery_log' not in locals():\n",
        "    discovery_log = []\n",
        "if 'total_people_count' not in locals():\n",
        "    total_people_count = 0\n",
        "if 'org_people_formatted' not in locals():\n",
        "    org_people_formatted = {}\n",
        "\n",
        "new_orgs_added = len(df_org_all) - original_count\n",
        "duplicates_skipped = len(duplicates_log) if duplicates_log else 0\n",
        "orgs_with_people = (final_df[\"People (Extracted)\"] != \"\").sum() if \"People (Extracted)\" in final_df.columns else 0\n",
        "pct_with_people = 100 * orgs_with_people / len(final_df) if len(final_df) > 0 else 0\n",
        "\n",
        "print(f\"\\n1. ORGANIZATION COUNTS:\")\n",
        "print(f\"   Original orgs: {original_count}\")\n",
        "print(f\"   New orgs added: {new_orgs_added}\")\n",
        "print(f\"   Duplicates skipped: {duplicates_skipped}\")\n",
        "print(f\"   Total orgs processed: {len(final_df)}\")\n",
        "\n",
        "# Discovery statistics (if auto-discovery was used)\n",
        "if discovery_log:\n",
        "    print(f\"\\n1.5. AUTO-DISCOVERY SUMMARY:\")\n",
        "    for entry in discovery_log:\n",
        "        capability = entry.get(\"capability\", \"\")\n",
        "        added = entry.get(\"added\", 0)\n",
        "        queued = entry.get(\"queued\", 0)\n",
        "        print(f\"   {capability}: {added} added, {queued} queued in backlog\")\n",
        "    \n",
        "    # Backlog summary\n",
        "    if os.path.exists(BACKLOG_FILE):\n",
        "        backlog_df_check = pd.read_csv(BACKLOG_FILE)\n",
        "        backlog_by_capability = backlog_df_check.groupby(\"capability_bucket\").size() if \"capability_bucket\" in backlog_df_check.columns else {}\n",
        "        if len(backlog_by_capability) > 0:\n",
        "            print(f\"\\n   Backlog remaining:\")\n",
        "            for cap, count in backlog_by_capability.items():\n",
        "                queued_count = len(backlog_df_check[(backlog_df_check[\"capability_bucket\"] == cap) & \n",
        "                                                     (backlog_df_check[\"status\"] == \"queued\")]) if \"status\" in backlog_df_check.columns else 0\n",
        "                print(f\"     {cap}: {queued_count} queued candidates\")\n",
        "\n",
        "print(f\"\\n2. PEOPLE EXTRACTION:\")\n",
        "# Ensure total_people_count is defined (in case people extraction section didn't run)\n",
        "if 'total_people_count' not in locals():\n",
        "    total_people_count = 0\n",
        "print(f\"   Total people extracted: {total_people_count}\")\n",
        "print(f\"   Organizations with people: {orgs_with_people} ({pct_with_people:.1f}%)\")\n",
        "print(f\"   Organizations without people: {len(final_df) - orgs_with_people} ({100 - pct_with_people:.1f}%)\")\n",
        "\n",
        "print(f\"\\n3. OUTPUT FILES:\")\n",
        "print(f\"   ✓ {MASTER_CSV} (single master file)\")\n",
        "if len(failures) > 0:\n",
        "    print(f\"   ✓ scrape_failures.csv (QA log)\")\n",
        "if duplicates_log:\n",
        "    print(f\"   ✓ dedupe_log.csv (QA log)\")\n",
        "if discovery_log:\n",
        "    print(f\"   ✓ expansion_discovery_log.csv (discovery log)\")\n",
        "if os.path.exists(BACKLOG_FILE):\n",
        "    print(f\"   ✓ {BACKLOG_FILE} (backlog queue)\")\n",
        "\n",
        "print(f\"\\n✓ CONFIRMED: Only ONE master CSV produced as primary deliverable\")\n",
        "print(\"=\"*60)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNW0WFxdqEDUB8ixrVxkAII",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "eso_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
